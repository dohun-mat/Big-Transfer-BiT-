{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecd1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin  # pylint: disable=g-importing-member\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import bit_pytorch.fewshot as fs\n",
    "import bit_pytorch.lbtoolbox as lb\n",
    "import bit_pytorch.models as models\n",
    "\n",
    "import bit_common\n",
    "import bit_hyperrule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebd65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(output, target, ks=(1,)):\n",
    "    \"\"\"Returns one boolean vector for each k, whether the target is within the output's top-k.\"\"\"\n",
    "    _, pred = output.topk(max(ks), 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [correct[:k].max(0)[0] for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edd0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycle(iterable):\n",
    "    \"\"\"Variant of itertools.cycle that does not save iterates.\"\"\"\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2632c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mktrainval(dataset_name):\n",
    "    \"\"\"Returns train and validation datasets.\"\"\"\n",
    "    precrop, crop = bit_hyperrule.get_resolution_from_dataset(dataset_name)\n",
    "    train_tx = tv.transforms.Compose([\n",
    "      tv.transforms.Resize((precrop, precrop)),\n",
    "      tv.transforms.RandomCrop((crop, crop)),\n",
    "      tv.transforms.RandomHorizontalFlip(),\n",
    "      tv.transforms.ToTensor(),\n",
    "      tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    val_tx = tv.transforms.Compose([\n",
    "      tv.transforms.Resize((crop, crop)),\n",
    "      tv.transforms.ToTensor(),\n",
    "      tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_set = tv.datasets.CIFAR10(args.datadir, transform=train_tx, train=True, download=True)\n",
    "        valid_set = tv.datasets.CIFAR10(args.datadir, transform=val_tx, train=False, download=True)\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        train_set = tv.datasets.CIFAR100('C:/Users/dhkim/Desktop/directory/cifar100_data', transform=train_tx, train=True, download=True)\n",
    "        valid_set = tv.datasets.CIFAR100('C:/Users/dhkim/Desktop/directory/cifar100_data', transform=val_tx, train=False, download=True)\n",
    "    elif dataset_name == \"imagenet2012\":\n",
    "        train_set = tv.datasets.ImageFolder(pjoin(args.datadir, \"train\"), train_tx)\n",
    "        valid_set = tv.datasets.ImageFolder(pjoin(args.datadir, \"val\"), val_tx)\n",
    "    \n",
    "    micro_batch_size = 16\n",
    "        \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_set, batch_size=micro_batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=micro_batch_size, num_workers=0, pin_memory=True,\n",
    "        sampler=torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=micro_batch_size))\n",
    "        \n",
    "\n",
    "    return train_set, valid_set, train_loader, valid_loader   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2662971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, data_loader, device, chrono, step):\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "\n",
    "        print(\"Running validation...\")\n",
    "        \n",
    "        all_c, all_top1, all_top5 = [], [], []\n",
    "        end = time.time()\n",
    "        for b, (x, y) in enumerate(data_loader):\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "                \n",
    "                # measure data loading time\n",
    "                chrono._done(\"eval load\", time.time() - end)\n",
    "                \n",
    "                # compute output, measure accuracy and record loss.\n",
    "                with chrono.measure(\"eval fprop\"):\n",
    "                    logits = model(x)\n",
    "                    c = torch.nn.CrossEntropyLoss(reduction='none')(logits, y)\n",
    "                    top1, top5 = topk(logits, y, ks=(1, 5))\n",
    "                    all_c.extend(c.cpu())  # Also ensures a sync point.\n",
    "                    all_top1.extend(top1.cpu())\n",
    "                    all_top5.extend(top5.cpu())\n",
    "                    \n",
    "                 # measure elapsed time\n",
    "                end = time.time()\n",
    "                \n",
    "            model.train()\n",
    "            print(f\"Validation@{step} loss {np.mean(all_c):.5f}, \"\n",
    "                      f\"top1 {np.mean(all_top1):.2%}, \"\n",
    "                      f\"top5 {np.mean(all_top5):.2%}\")\n",
    "            \n",
    "            return all_c, all_top1, all_top5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "329986ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, l):\n",
    "        \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "        indices = torch.randperm(x.shape[0]).to(x.device)\n",
    "\n",
    "        mixed_x = l * x + (1 - l) * x[indices]\n",
    "        y_a, y_b = y, y[indices]\n",
    "        return mixed_x, y_a, y_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91788180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, l):\n",
    "    return l * criterion(pred, y_a) + (1 - l) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2722183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhkim\\\\Desktop\\\\directory\\\\big_transfer-master'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "993ddad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     logger = bit_common.setup_logger(args)\n",
    "    \n",
    "    # Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "    # Only good if sizes stay the same within the main loop!\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     logger.info(f\"Going to train on {device}\")\n",
    "    \n",
    "    \n",
    "    train_set, valid_set, train_loader, valid_loader = mktrainval('cifar100')\n",
    "    \n",
    "    \n",
    "    model = models.KNOWN_MODELS['BiT-M-R50x1'](head_size=len(valid_set.classes), zero_head=True)\n",
    "    model.load_from(np.load(f\"BiT-M-R50x1.npz\"))\n",
    "    \n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # Optionally resume from a checkpoint.\n",
    "    # Load it to CPU first as we'll move the model to GPU later.\n",
    "    # This way, we save a little bit of GPU memory when loading.\n",
    "    step = 0\n",
    "    \n",
    "    # Note: no weight-decay!\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "    \n",
    "    # Rne-tuning if esume fiwe find a saved model.\n",
    "    savename = pjoin('C:\\\\Users\\\\dhkim\\\\Desktop\\\\directory\\\\big_transfer-master', \"bit.pth.tar\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Model will be saved in '{savename}'\")\n",
    "        checkpoint = torch.load(savename, map_location=\"cpu\")\n",
    "        print(f\"Found saved model to resume from at '{savename}'\")\n",
    "\n",
    "        step = checkpoint[\"step\"]\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optim.load_state_dict(checkpoint[\"optim\"])\n",
    "        print(f\"Resumed at step {step}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Fine-tuning from BiT\")\n",
    "    \n",
    "    \n",
    "    model = model.to(device)\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    model.train()\n",
    "    mixup = bit_hyperrule.get_mixup(len(train_set))\n",
    "    cri = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    chrono = lb.Chrono()\n",
    "    accum_steps = 0\n",
    "    mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n",
    "    end = time.time()\n",
    "    \n",
    "    with lb.Uninterrupt() as u:\n",
    "        for x, y in recycle(train_loader):\n",
    "            # measure data loading time, which is spent in the `for` statement.\n",
    "            chrono._done(\"load\", time.time() - end)\n",
    "            \n",
    "            if u.interrupted:\n",
    "                break\n",
    "                \n",
    "            # Schedule sending to GPU(s)\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            \n",
    "            # Update learning-rate, including stop training if over.\n",
    "            lr = bit_hyperrule.get_lr(step, len(train_set), 0.003)\n",
    "            if lr is None:\n",
    "                break\n",
    "            for param_group in optim.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "                \n",
    "            if mixup > 0.0:\n",
    "                x, y_a, y_b = mixup_data(x, y, mixup_l)\n",
    "                \n",
    "             # compute output\n",
    "            with chrono.measure(\"fprop\"):\n",
    "                logits = model(x)\n",
    "                if mixup > 0.0:\n",
    "                    c = mixup_criterion(cri, logits, y_a, y_b, mixup_l)\n",
    "                else:\n",
    "                    c = cri(logits, y)\n",
    "                c_num = float(c.data.cpu().numpy())  # Also ensures a sync point.\n",
    "            \n",
    "            # Accumulate grads\n",
    "            with chrono.measure(\"grads\"):\n",
    "                (c / 1).backward()\n",
    "                accum_steps += 1\n",
    "                \n",
    "            accstep = f\" ({accum_steps}/{1})\"\n",
    "            print(f\"[step {step}/accstep : {accstep}]: loss={c_num:.5f} (lr={lr:.1e})\")  # pylint: disable=logging-format-interpolation\n",
    "            \n",
    "            \n",
    "             # Update params\n",
    "            if accum_steps == 1:\n",
    "                with chrono.measure(\"update\"):\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "                step += 1\n",
    "                accum_steps = 0\n",
    "                # Sample new mixup ratio for next batch\n",
    "                mixup_l = np.random.beta(mixup, mixup) if mixup > 0 else 1\n",
    "                \n",
    "#                 print(\"!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                # Run evaluation and save the model.\n",
    "                if 0 and step % 0 == 0:\n",
    "                    run_eval(model, valid_loader, device, chrono, logger, step)\n",
    "                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                    if args.save:\n",
    "                        torch.save({\n",
    "                            \"step\": step,\n",
    "                            \"model\": model.state_dict(),\n",
    "                            \"optim\" : optim.state_dict(),\n",
    "                        }, savename)\n",
    "            end = time.time()\n",
    "            if(step == 2500):\n",
    "                break\n",
    "            \n",
    "         # Final eval at end of training.\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        run_eval(model, valid_loader, device, chrono, step='end')\n",
    "\n",
    "    print(f\"Timings:\\n{chrono}\")\n",
    "    \n",
    "    \n",
    " \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc85dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model will be saved in 'C:\\Users\\dhkim\\Desktop\\directory\\big_transfer-master\\bit.pth.tar'\n",
      "Fine-tuning from BiT\n",
      "[step 0/accstep :  (1/1)]: loss=4.60517 (lr=0.0e+00)\n",
      "[step 1/accstep :  (1/1)]: loss=4.60517 (lr=6.0e-06)\n",
      "[step 2/accstep :  (1/1)]: loss=4.60461 (lr=1.2e-05)\n",
      "[step 3/accstep :  (1/1)]: loss=4.60279 (lr=1.8e-05)\n",
      "[step 4/accstep :  (1/1)]: loss=4.60334 (lr=2.4e-05)\n",
      "[step 5/accstep :  (1/1)]: loss=4.59243 (lr=3.0e-05)\n",
      "[step 6/accstep :  (1/1)]: loss=4.60260 (lr=3.6e-05)\n",
      "[step 7/accstep :  (1/1)]: loss=4.60124 (lr=4.2e-05)\n",
      "[step 8/accstep :  (1/1)]: loss=4.60037 (lr=4.8e-05)\n",
      "[step 9/accstep :  (1/1)]: loss=4.56253 (lr=5.4e-05)\n",
      "[step 10/accstep :  (1/1)]: loss=4.57058 (lr=6.0e-05)\n",
      "[step 11/accstep :  (1/1)]: loss=4.53340 (lr=6.6e-05)\n",
      "[step 12/accstep :  (1/1)]: loss=4.54870 (lr=7.2e-05)\n",
      "[step 13/accstep :  (1/1)]: loss=4.58302 (lr=7.8e-05)\n",
      "[step 14/accstep :  (1/1)]: loss=4.52834 (lr=8.4e-05)\n",
      "[step 15/accstep :  (1/1)]: loss=4.60881 (lr=9.0e-05)\n",
      "[step 16/accstep :  (1/1)]: loss=4.55643 (lr=9.6e-05)\n",
      "[step 17/accstep :  (1/1)]: loss=4.48203 (lr=1.0e-04)\n",
      "[step 18/accstep :  (1/1)]: loss=4.48828 (lr=1.1e-04)\n",
      "[step 19/accstep :  (1/1)]: loss=4.37834 (lr=1.1e-04)\n",
      "[step 20/accstep :  (1/1)]: loss=4.57662 (lr=1.2e-04)\n",
      "[step 21/accstep :  (1/1)]: loss=4.59978 (lr=1.3e-04)\n",
      "[step 22/accstep :  (1/1)]: loss=4.47520 (lr=1.3e-04)\n",
      "[step 23/accstep :  (1/1)]: loss=4.44379 (lr=1.4e-04)\n",
      "[step 24/accstep :  (1/1)]: loss=4.53936 (lr=1.4e-04)\n",
      "[step 25/accstep :  (1/1)]: loss=4.13972 (lr=1.5e-04)\n",
      "[step 26/accstep :  (1/1)]: loss=4.47999 (lr=1.6e-04)\n",
      "[step 27/accstep :  (1/1)]: loss=4.47186 (lr=1.6e-04)\n",
      "[step 28/accstep :  (1/1)]: loss=4.34825 (lr=1.7e-04)\n",
      "[step 29/accstep :  (1/1)]: loss=4.22588 (lr=1.7e-04)\n",
      "[step 30/accstep :  (1/1)]: loss=4.04594 (lr=1.8e-04)\n",
      "[step 31/accstep :  (1/1)]: loss=4.06970 (lr=1.9e-04)\n",
      "[step 32/accstep :  (1/1)]: loss=4.08944 (lr=1.9e-04)\n",
      "[step 33/accstep :  (1/1)]: loss=4.36106 (lr=2.0e-04)\n",
      "[step 34/accstep :  (1/1)]: loss=4.56072 (lr=2.0e-04)\n",
      "[step 35/accstep :  (1/1)]: loss=4.15763 (lr=2.1e-04)\n",
      "[step 36/accstep :  (1/1)]: loss=4.02221 (lr=2.2e-04)\n",
      "[step 37/accstep :  (1/1)]: loss=3.95045 (lr=2.2e-04)\n",
      "[step 38/accstep :  (1/1)]: loss=3.88387 (lr=2.3e-04)\n",
      "[step 39/accstep :  (1/1)]: loss=3.25852 (lr=2.3e-04)\n",
      "[step 40/accstep :  (1/1)]: loss=3.82845 (lr=2.4e-04)\n",
      "[step 41/accstep :  (1/1)]: loss=3.86050 (lr=2.5e-04)\n",
      "[step 42/accstep :  (1/1)]: loss=4.11859 (lr=2.5e-04)\n",
      "[step 43/accstep :  (1/1)]: loss=4.09418 (lr=2.6e-04)\n",
      "[step 44/accstep :  (1/1)]: loss=3.98981 (lr=2.6e-04)\n",
      "[step 45/accstep :  (1/1)]: loss=3.55706 (lr=2.7e-04)\n",
      "[step 46/accstep :  (1/1)]: loss=4.17517 (lr=2.8e-04)\n",
      "[step 47/accstep :  (1/1)]: loss=3.86006 (lr=2.8e-04)\n",
      "[step 48/accstep :  (1/1)]: loss=3.25486 (lr=2.9e-04)\n",
      "[step 49/accstep :  (1/1)]: loss=3.70551 (lr=2.9e-04)\n",
      "[step 50/accstep :  (1/1)]: loss=3.81824 (lr=3.0e-04)\n",
      "[step 51/accstep :  (1/1)]: loss=3.53527 (lr=3.1e-04)\n",
      "[step 52/accstep :  (1/1)]: loss=3.58825 (lr=3.1e-04)\n",
      "[step 53/accstep :  (1/1)]: loss=3.27908 (lr=3.2e-04)\n",
      "[step 54/accstep :  (1/1)]: loss=3.62946 (lr=3.2e-04)\n",
      "[step 55/accstep :  (1/1)]: loss=3.00988 (lr=3.3e-04)\n",
      "[step 56/accstep :  (1/1)]: loss=2.93179 (lr=3.4e-04)\n",
      "[step 57/accstep :  (1/1)]: loss=2.51945 (lr=3.4e-04)\n",
      "[step 58/accstep :  (1/1)]: loss=2.73526 (lr=3.5e-04)\n",
      "[step 59/accstep :  (1/1)]: loss=3.30045 (lr=3.5e-04)\n",
      "[step 60/accstep :  (1/1)]: loss=2.54733 (lr=3.6e-04)\n",
      "[step 61/accstep :  (1/1)]: loss=2.70681 (lr=3.7e-04)\n",
      "[step 62/accstep :  (1/1)]: loss=3.20757 (lr=3.7e-04)\n",
      "[step 63/accstep :  (1/1)]: loss=2.27500 (lr=3.8e-04)\n",
      "[step 64/accstep :  (1/1)]: loss=2.24867 (lr=3.8e-04)\n",
      "[step 65/accstep :  (1/1)]: loss=2.82193 (lr=3.9e-04)\n",
      "[step 66/accstep :  (1/1)]: loss=3.56597 (lr=4.0e-04)\n",
      "[step 67/accstep :  (1/1)]: loss=2.74354 (lr=4.0e-04)\n",
      "[step 68/accstep :  (1/1)]: loss=1.49423 (lr=4.1e-04)\n",
      "[step 69/accstep :  (1/1)]: loss=2.04081 (lr=4.1e-04)\n",
      "[step 70/accstep :  (1/1)]: loss=2.85366 (lr=4.2e-04)\n",
      "[step 71/accstep :  (1/1)]: loss=2.77631 (lr=4.3e-04)\n",
      "[step 72/accstep :  (1/1)]: loss=2.34805 (lr=4.3e-04)\n",
      "[step 73/accstep :  (1/1)]: loss=2.67337 (lr=4.4e-04)\n",
      "[step 74/accstep :  (1/1)]: loss=2.10506 (lr=4.4e-04)\n",
      "[step 75/accstep :  (1/1)]: loss=2.95436 (lr=4.5e-04)\n",
      "[step 76/accstep :  (1/1)]: loss=2.87810 (lr=4.6e-04)\n",
      "[step 77/accstep :  (1/1)]: loss=2.29759 (lr=4.6e-04)\n",
      "[step 78/accstep :  (1/1)]: loss=1.92337 (lr=4.7e-04)\n",
      "[step 79/accstep :  (1/1)]: loss=1.83378 (lr=4.7e-04)\n",
      "[step 80/accstep :  (1/1)]: loss=1.98148 (lr=4.8e-04)\n",
      "[step 81/accstep :  (1/1)]: loss=3.31446 (lr=4.9e-04)\n",
      "[step 82/accstep :  (1/1)]: loss=2.74934 (lr=4.9e-04)\n",
      "[step 83/accstep :  (1/1)]: loss=1.87619 (lr=5.0e-04)\n",
      "[step 84/accstep :  (1/1)]: loss=1.81827 (lr=5.0e-04)\n",
      "[step 85/accstep :  (1/1)]: loss=3.58038 (lr=5.1e-04)\n",
      "[step 86/accstep :  (1/1)]: loss=1.99939 (lr=5.2e-04)\n",
      "[step 87/accstep :  (1/1)]: loss=1.49412 (lr=5.2e-04)\n",
      "[step 88/accstep :  (1/1)]: loss=1.19702 (lr=5.3e-04)\n",
      "[step 89/accstep :  (1/1)]: loss=3.25481 (lr=5.3e-04)\n",
      "[step 90/accstep :  (1/1)]: loss=2.01537 (lr=5.4e-04)\n",
      "[step 91/accstep :  (1/1)]: loss=2.09501 (lr=5.5e-04)\n",
      "[step 92/accstep :  (1/1)]: loss=1.26536 (lr=5.5e-04)\n",
      "[step 93/accstep :  (1/1)]: loss=2.77726 (lr=5.6e-04)\n",
      "[step 94/accstep :  (1/1)]: loss=1.70904 (lr=5.6e-04)\n",
      "[step 95/accstep :  (1/1)]: loss=1.61030 (lr=5.7e-04)\n",
      "[step 96/accstep :  (1/1)]: loss=1.28096 (lr=5.8e-04)\n",
      "[step 97/accstep :  (1/1)]: loss=3.46596 (lr=5.8e-04)\n",
      "[step 98/accstep :  (1/1)]: loss=1.24298 (lr=5.9e-04)\n",
      "[step 99/accstep :  (1/1)]: loss=2.52750 (lr=5.9e-04)\n",
      "[step 100/accstep :  (1/1)]: loss=3.04442 (lr=6.0e-04)\n",
      "[step 101/accstep :  (1/1)]: loss=1.19226 (lr=6.1e-04)\n",
      "[step 102/accstep :  (1/1)]: loss=1.97156 (lr=6.1e-04)\n",
      "[step 103/accstep :  (1/1)]: loss=2.57536 (lr=6.2e-04)\n",
      "[step 104/accstep :  (1/1)]: loss=1.09624 (lr=6.2e-04)\n",
      "[step 105/accstep :  (1/1)]: loss=4.62532 (lr=6.3e-04)\n",
      "[step 106/accstep :  (1/1)]: loss=3.11029 (lr=6.4e-04)\n",
      "[step 107/accstep :  (1/1)]: loss=1.13739 (lr=6.4e-04)\n",
      "[step 108/accstep :  (1/1)]: loss=2.17334 (lr=6.5e-04)\n",
      "[step 109/accstep :  (1/1)]: loss=1.66193 (lr=6.5e-04)\n",
      "[step 110/accstep :  (1/1)]: loss=2.29771 (lr=6.6e-04)\n",
      "[step 111/accstep :  (1/1)]: loss=1.95769 (lr=6.7e-04)\n",
      "[step 112/accstep :  (1/1)]: loss=1.39685 (lr=6.7e-04)\n",
      "[step 113/accstep :  (1/1)]: loss=2.29655 (lr=6.8e-04)\n",
      "[step 114/accstep :  (1/1)]: loss=1.60079 (lr=6.8e-04)\n",
      "[step 115/accstep :  (1/1)]: loss=3.18185 (lr=6.9e-04)\n",
      "[step 116/accstep :  (1/1)]: loss=2.05576 (lr=7.0e-04)\n",
      "[step 117/accstep :  (1/1)]: loss=2.05132 (lr=7.0e-04)\n",
      "[step 118/accstep :  (1/1)]: loss=2.03611 (lr=7.1e-04)\n",
      "[step 119/accstep :  (1/1)]: loss=2.26771 (lr=7.1e-04)\n",
      "[step 120/accstep :  (1/1)]: loss=1.81320 (lr=7.2e-04)\n",
      "[step 121/accstep :  (1/1)]: loss=2.36138 (lr=7.3e-04)\n",
      "[step 122/accstep :  (1/1)]: loss=1.90856 (lr=7.3e-04)\n",
      "[step 123/accstep :  (1/1)]: loss=2.09300 (lr=7.4e-04)\n",
      "[step 124/accstep :  (1/1)]: loss=2.17117 (lr=7.4e-04)\n",
      "[step 125/accstep :  (1/1)]: loss=1.83218 (lr=7.5e-04)\n",
      "[step 126/accstep :  (1/1)]: loss=1.64927 (lr=7.6e-04)\n",
      "[step 127/accstep :  (1/1)]: loss=2.04360 (lr=7.6e-04)\n",
      "[step 128/accstep :  (1/1)]: loss=2.00624 (lr=7.7e-04)\n",
      "[step 129/accstep :  (1/1)]: loss=3.43995 (lr=7.7e-04)\n",
      "[step 130/accstep :  (1/1)]: loss=2.11109 (lr=7.8e-04)\n",
      "[step 131/accstep :  (1/1)]: loss=2.37969 (lr=7.9e-04)\n",
      "[step 132/accstep :  (1/1)]: loss=2.28470 (lr=7.9e-04)\n",
      "[step 133/accstep :  (1/1)]: loss=2.74478 (lr=8.0e-04)\n",
      "[step 134/accstep :  (1/1)]: loss=2.50655 (lr=8.0e-04)\n",
      "[step 135/accstep :  (1/1)]: loss=1.65719 (lr=8.1e-04)\n",
      "[step 136/accstep :  (1/1)]: loss=1.92873 (lr=8.2e-04)\n",
      "[step 137/accstep :  (1/1)]: loss=3.02936 (lr=8.2e-04)\n",
      "[step 138/accstep :  (1/1)]: loss=1.17690 (lr=8.3e-04)\n",
      "[step 139/accstep :  (1/1)]: loss=2.14798 (lr=8.3e-04)\n",
      "[step 140/accstep :  (1/1)]: loss=2.84235 (lr=8.4e-04)\n",
      "[step 141/accstep :  (1/1)]: loss=4.13104 (lr=8.5e-04)\n",
      "[step 142/accstep :  (1/1)]: loss=2.53645 (lr=8.5e-04)\n",
      "[step 143/accstep :  (1/1)]: loss=3.20362 (lr=8.6e-04)\n",
      "[step 144/accstep :  (1/1)]: loss=3.45171 (lr=8.6e-04)\n",
      "[step 145/accstep :  (1/1)]: loss=2.58317 (lr=8.7e-04)\n",
      "[step 146/accstep :  (1/1)]: loss=2.69547 (lr=8.8e-04)\n",
      "[step 147/accstep :  (1/1)]: loss=2.62380 (lr=8.8e-04)\n",
      "[step 148/accstep :  (1/1)]: loss=1.82089 (lr=8.9e-04)\n",
      "[step 149/accstep :  (1/1)]: loss=2.51425 (lr=8.9e-04)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 150/accstep :  (1/1)]: loss=3.62720 (lr=9.0e-04)\n",
      "[step 151/accstep :  (1/1)]: loss=2.52214 (lr=9.1e-04)\n",
      "[step 152/accstep :  (1/1)]: loss=2.46938 (lr=9.1e-04)\n",
      "[step 153/accstep :  (1/1)]: loss=3.16761 (lr=9.2e-04)\n",
      "[step 154/accstep :  (1/1)]: loss=2.15886 (lr=9.2e-04)\n",
      "[step 155/accstep :  (1/1)]: loss=2.71255 (lr=9.3e-04)\n",
      "[step 156/accstep :  (1/1)]: loss=1.78272 (lr=9.4e-04)\n",
      "[step 157/accstep :  (1/1)]: loss=2.17160 (lr=9.4e-04)\n",
      "[step 158/accstep :  (1/1)]: loss=2.52218 (lr=9.5e-04)\n",
      "[step 159/accstep :  (1/1)]: loss=2.67304 (lr=9.5e-04)\n",
      "[step 160/accstep :  (1/1)]: loss=1.48138 (lr=9.6e-04)\n",
      "[step 161/accstep :  (1/1)]: loss=2.07914 (lr=9.7e-04)\n",
      "[step 162/accstep :  (1/1)]: loss=3.44594 (lr=9.7e-04)\n",
      "[step 163/accstep :  (1/1)]: loss=2.71082 (lr=9.8e-04)\n",
      "[step 164/accstep :  (1/1)]: loss=4.63071 (lr=9.8e-04)\n",
      "[step 165/accstep :  (1/1)]: loss=1.88943 (lr=9.9e-04)\n",
      "[step 166/accstep :  (1/1)]: loss=2.68162 (lr=1.0e-03)\n",
      "[step 167/accstep :  (1/1)]: loss=2.83301 (lr=1.0e-03)\n",
      "[step 168/accstep :  (1/1)]: loss=2.02621 (lr=1.0e-03)\n",
      "[step 169/accstep :  (1/1)]: loss=3.22217 (lr=1.0e-03)\n",
      "[step 170/accstep :  (1/1)]: loss=2.77794 (lr=1.0e-03)\n",
      "[step 171/accstep :  (1/1)]: loss=2.55841 (lr=1.0e-03)\n",
      "[step 172/accstep :  (1/1)]: loss=2.93589 (lr=1.0e-03)\n",
      "[step 173/accstep :  (1/1)]: loss=2.24691 (lr=1.0e-03)\n",
      "[step 174/accstep :  (1/1)]: loss=2.87024 (lr=1.0e-03)\n",
      "[step 175/accstep :  (1/1)]: loss=3.09020 (lr=1.1e-03)\n",
      "[step 176/accstep :  (1/1)]: loss=3.06025 (lr=1.1e-03)\n",
      "[step 177/accstep :  (1/1)]: loss=3.80255 (lr=1.1e-03)\n",
      "[step 178/accstep :  (1/1)]: loss=2.36792 (lr=1.1e-03)\n",
      "[step 179/accstep :  (1/1)]: loss=3.01743 (lr=1.1e-03)\n",
      "[step 180/accstep :  (1/1)]: loss=3.24806 (lr=1.1e-03)\n",
      "[step 181/accstep :  (1/1)]: loss=2.79165 (lr=1.1e-03)\n",
      "[step 182/accstep :  (1/1)]: loss=1.70314 (lr=1.1e-03)\n",
      "[step 183/accstep :  (1/1)]: loss=1.92737 (lr=1.1e-03)\n",
      "[step 184/accstep :  (1/1)]: loss=2.61854 (lr=1.1e-03)\n",
      "[step 185/accstep :  (1/1)]: loss=2.82419 (lr=1.1e-03)\n",
      "[step 186/accstep :  (1/1)]: loss=2.08446 (lr=1.1e-03)\n",
      "[step 187/accstep :  (1/1)]: loss=3.09539 (lr=1.1e-03)\n",
      "[step 188/accstep :  (1/1)]: loss=5.19047 (lr=1.1e-03)\n",
      "[step 189/accstep :  (1/1)]: loss=2.42486 (lr=1.1e-03)\n",
      "[step 190/accstep :  (1/1)]: loss=2.33512 (lr=1.1e-03)\n",
      "[step 191/accstep :  (1/1)]: loss=2.43702 (lr=1.1e-03)\n",
      "[step 192/accstep :  (1/1)]: loss=1.64026 (lr=1.2e-03)\n",
      "[step 193/accstep :  (1/1)]: loss=4.71347 (lr=1.2e-03)\n",
      "[step 194/accstep :  (1/1)]: loss=2.71386 (lr=1.2e-03)\n",
      "[step 195/accstep :  (1/1)]: loss=1.82716 (lr=1.2e-03)\n",
      "[step 196/accstep :  (1/1)]: loss=2.15333 (lr=1.2e-03)\n",
      "[step 197/accstep :  (1/1)]: loss=2.81319 (lr=1.2e-03)\n",
      "[step 198/accstep :  (1/1)]: loss=3.76682 (lr=1.2e-03)\n",
      "[step 199/accstep :  (1/1)]: loss=2.91840 (lr=1.2e-03)\n",
      "[step 200/accstep :  (1/1)]: loss=2.66317 (lr=1.2e-03)\n",
      "[step 201/accstep :  (1/1)]: loss=2.69867 (lr=1.2e-03)\n",
      "[step 202/accstep :  (1/1)]: loss=2.40480 (lr=1.2e-03)\n",
      "[step 203/accstep :  (1/1)]: loss=3.84472 (lr=1.2e-03)\n",
      "[step 204/accstep :  (1/1)]: loss=3.27021 (lr=1.2e-03)\n",
      "[step 205/accstep :  (1/1)]: loss=3.83313 (lr=1.2e-03)\n",
      "[step 206/accstep :  (1/1)]: loss=2.42977 (lr=1.2e-03)\n",
      "[step 207/accstep :  (1/1)]: loss=3.01781 (lr=1.2e-03)\n",
      "[step 208/accstep :  (1/1)]: loss=2.92874 (lr=1.2e-03)\n",
      "[step 209/accstep :  (1/1)]: loss=1.53430 (lr=1.3e-03)\n",
      "[step 210/accstep :  (1/1)]: loss=2.56573 (lr=1.3e-03)\n",
      "[step 211/accstep :  (1/1)]: loss=2.11859 (lr=1.3e-03)\n",
      "[step 212/accstep :  (1/1)]: loss=1.56531 (lr=1.3e-03)\n",
      "[step 213/accstep :  (1/1)]: loss=2.84496 (lr=1.3e-03)\n",
      "[step 214/accstep :  (1/1)]: loss=1.83476 (lr=1.3e-03)\n",
      "[step 215/accstep :  (1/1)]: loss=3.55620 (lr=1.3e-03)\n",
      "[step 216/accstep :  (1/1)]: loss=1.52294 (lr=1.3e-03)\n",
      "[step 217/accstep :  (1/1)]: loss=2.97728 (lr=1.3e-03)\n",
      "[step 218/accstep :  (1/1)]: loss=4.22463 (lr=1.3e-03)\n",
      "[step 219/accstep :  (1/1)]: loss=2.29980 (lr=1.3e-03)\n",
      "[step 220/accstep :  (1/1)]: loss=4.45746 (lr=1.3e-03)\n",
      "[step 221/accstep :  (1/1)]: loss=3.01719 (lr=1.3e-03)\n",
      "[step 222/accstep :  (1/1)]: loss=2.61396 (lr=1.3e-03)\n",
      "[step 223/accstep :  (1/1)]: loss=2.87843 (lr=1.3e-03)\n",
      "[step 224/accstep :  (1/1)]: loss=1.61303 (lr=1.3e-03)\n",
      "[step 225/accstep :  (1/1)]: loss=1.77260 (lr=1.4e-03)\n",
      "[step 226/accstep :  (1/1)]: loss=4.68527 (lr=1.4e-03)\n",
      "[step 227/accstep :  (1/1)]: loss=2.42091 (lr=1.4e-03)\n",
      "[step 228/accstep :  (1/1)]: loss=2.60100 (lr=1.4e-03)\n",
      "[step 229/accstep :  (1/1)]: loss=2.68817 (lr=1.4e-03)\n",
      "[step 230/accstep :  (1/1)]: loss=2.92687 (lr=1.4e-03)\n",
      "[step 231/accstep :  (1/1)]: loss=3.09293 (lr=1.4e-03)\n",
      "[step 232/accstep :  (1/1)]: loss=2.77833 (lr=1.4e-03)\n",
      "[step 233/accstep :  (1/1)]: loss=2.35842 (lr=1.4e-03)\n",
      "[step 234/accstep :  (1/1)]: loss=3.35381 (lr=1.4e-03)\n",
      "[step 235/accstep :  (1/1)]: loss=3.14293 (lr=1.4e-03)\n",
      "[step 236/accstep :  (1/1)]: loss=4.20820 (lr=1.4e-03)\n",
      "[step 237/accstep :  (1/1)]: loss=3.27788 (lr=1.4e-03)\n",
      "[step 238/accstep :  (1/1)]: loss=3.07153 (lr=1.4e-03)\n",
      "[step 239/accstep :  (1/1)]: loss=3.21149 (lr=1.4e-03)\n",
      "[step 240/accstep :  (1/1)]: loss=4.13535 (lr=1.4e-03)\n",
      "[step 241/accstep :  (1/1)]: loss=2.81454 (lr=1.4e-03)\n",
      "[step 242/accstep :  (1/1)]: loss=2.99073 (lr=1.5e-03)\n",
      "[step 243/accstep :  (1/1)]: loss=2.12429 (lr=1.5e-03)\n",
      "[step 244/accstep :  (1/1)]: loss=3.32764 (lr=1.5e-03)\n",
      "[step 245/accstep :  (1/1)]: loss=2.17335 (lr=1.5e-03)\n",
      "[step 246/accstep :  (1/1)]: loss=2.21939 (lr=1.5e-03)\n",
      "[step 247/accstep :  (1/1)]: loss=2.96054 (lr=1.5e-03)\n",
      "[step 248/accstep :  (1/1)]: loss=2.52579 (lr=1.5e-03)\n",
      "[step 249/accstep :  (1/1)]: loss=2.84408 (lr=1.5e-03)\n",
      "[step 250/accstep :  (1/1)]: loss=2.20926 (lr=1.5e-03)\n",
      "[step 251/accstep :  (1/1)]: loss=2.89632 (lr=1.5e-03)\n",
      "[step 252/accstep :  (1/1)]: loss=2.72268 (lr=1.5e-03)\n",
      "[step 253/accstep :  (1/1)]: loss=2.04085 (lr=1.5e-03)\n",
      "[step 254/accstep :  (1/1)]: loss=1.99919 (lr=1.5e-03)\n",
      "[step 255/accstep :  (1/1)]: loss=4.60109 (lr=1.5e-03)\n",
      "[step 256/accstep :  (1/1)]: loss=3.82154 (lr=1.5e-03)\n",
      "[step 257/accstep :  (1/1)]: loss=2.27267 (lr=1.5e-03)\n",
      "[step 258/accstep :  (1/1)]: loss=2.23417 (lr=1.5e-03)\n",
      "[step 259/accstep :  (1/1)]: loss=2.45048 (lr=1.6e-03)\n",
      "[step 260/accstep :  (1/1)]: loss=1.74397 (lr=1.6e-03)\n",
      "[step 261/accstep :  (1/1)]: loss=2.02040 (lr=1.6e-03)\n",
      "[step 262/accstep :  (1/1)]: loss=1.89767 (lr=1.6e-03)\n",
      "[step 263/accstep :  (1/1)]: loss=2.10817 (lr=1.6e-03)\n",
      "[step 264/accstep :  (1/1)]: loss=2.65830 (lr=1.6e-03)\n",
      "[step 265/accstep :  (1/1)]: loss=3.80600 (lr=1.6e-03)\n",
      "[step 266/accstep :  (1/1)]: loss=3.37174 (lr=1.6e-03)\n",
      "[step 267/accstep :  (1/1)]: loss=2.48997 (lr=1.6e-03)\n",
      "[step 268/accstep :  (1/1)]: loss=2.70735 (lr=1.6e-03)\n",
      "[step 269/accstep :  (1/1)]: loss=3.38954 (lr=1.6e-03)\n",
      "[step 270/accstep :  (1/1)]: loss=2.69276 (lr=1.6e-03)\n",
      "[step 271/accstep :  (1/1)]: loss=3.58639 (lr=1.6e-03)\n",
      "[step 272/accstep :  (1/1)]: loss=3.92998 (lr=1.6e-03)\n",
      "[step 273/accstep :  (1/1)]: loss=2.41801 (lr=1.6e-03)\n",
      "[step 274/accstep :  (1/1)]: loss=1.84288 (lr=1.6e-03)\n",
      "[step 275/accstep :  (1/1)]: loss=2.50806 (lr=1.7e-03)\n",
      "[step 276/accstep :  (1/1)]: loss=2.95289 (lr=1.7e-03)\n",
      "[step 277/accstep :  (1/1)]: loss=2.60977 (lr=1.7e-03)\n",
      "[step 278/accstep :  (1/1)]: loss=3.31144 (lr=1.7e-03)\n",
      "[step 279/accstep :  (1/1)]: loss=3.84838 (lr=1.7e-03)\n",
      "[step 280/accstep :  (1/1)]: loss=2.10078 (lr=1.7e-03)\n",
      "[step 281/accstep :  (1/1)]: loss=2.29852 (lr=1.7e-03)\n",
      "[step 282/accstep :  (1/1)]: loss=4.04390 (lr=1.7e-03)\n",
      "[step 283/accstep :  (1/1)]: loss=2.13529 (lr=1.7e-03)\n",
      "[step 284/accstep :  (1/1)]: loss=2.59439 (lr=1.7e-03)\n",
      "[step 285/accstep :  (1/1)]: loss=3.59854 (lr=1.7e-03)\n",
      "[step 286/accstep :  (1/1)]: loss=2.85099 (lr=1.7e-03)\n",
      "[step 287/accstep :  (1/1)]: loss=2.33183 (lr=1.7e-03)\n",
      "[step 288/accstep :  (1/1)]: loss=2.33853 (lr=1.7e-03)\n",
      "[step 289/accstep :  (1/1)]: loss=3.09163 (lr=1.7e-03)\n",
      "[step 290/accstep :  (1/1)]: loss=3.68622 (lr=1.7e-03)\n",
      "[step 291/accstep :  (1/1)]: loss=2.61517 (lr=1.7e-03)\n",
      "[step 292/accstep :  (1/1)]: loss=2.44549 (lr=1.8e-03)\n",
      "[step 293/accstep :  (1/1)]: loss=2.69568 (lr=1.8e-03)\n",
      "[step 294/accstep :  (1/1)]: loss=4.39163 (lr=1.8e-03)\n",
      "[step 295/accstep :  (1/1)]: loss=2.13909 (lr=1.8e-03)\n",
      "[step 296/accstep :  (1/1)]: loss=2.46247 (lr=1.8e-03)\n",
      "[step 297/accstep :  (1/1)]: loss=2.31249 (lr=1.8e-03)\n",
      "[step 298/accstep :  (1/1)]: loss=2.31098 (lr=1.8e-03)\n",
      "[step 299/accstep :  (1/1)]: loss=2.60774 (lr=1.8e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 300/accstep :  (1/1)]: loss=1.78845 (lr=1.8e-03)\n",
      "[step 301/accstep :  (1/1)]: loss=4.29195 (lr=1.8e-03)\n",
      "[step 302/accstep :  (1/1)]: loss=3.07781 (lr=1.8e-03)\n",
      "[step 303/accstep :  (1/1)]: loss=4.36802 (lr=1.8e-03)\n",
      "[step 304/accstep :  (1/1)]: loss=2.24074 (lr=1.8e-03)\n",
      "[step 305/accstep :  (1/1)]: loss=1.68109 (lr=1.8e-03)\n",
      "[step 306/accstep :  (1/1)]: loss=2.10942 (lr=1.8e-03)\n",
      "[step 307/accstep :  (1/1)]: loss=2.49856 (lr=1.8e-03)\n",
      "[step 308/accstep :  (1/1)]: loss=2.15299 (lr=1.8e-03)\n",
      "[step 309/accstep :  (1/1)]: loss=4.70872 (lr=1.9e-03)\n",
      "[step 310/accstep :  (1/1)]: loss=2.63390 (lr=1.9e-03)\n",
      "[step 311/accstep :  (1/1)]: loss=2.57980 (lr=1.9e-03)\n",
      "[step 312/accstep :  (1/1)]: loss=1.96624 (lr=1.9e-03)\n",
      "[step 313/accstep :  (1/1)]: loss=2.88015 (lr=1.9e-03)\n",
      "[step 314/accstep :  (1/1)]: loss=2.28235 (lr=1.9e-03)\n",
      "[step 315/accstep :  (1/1)]: loss=2.25302 (lr=1.9e-03)\n",
      "[step 316/accstep :  (1/1)]: loss=2.94083 (lr=1.9e-03)\n",
      "[step 317/accstep :  (1/1)]: loss=2.29136 (lr=1.9e-03)\n",
      "[step 318/accstep :  (1/1)]: loss=3.28910 (lr=1.9e-03)\n",
      "[step 319/accstep :  (1/1)]: loss=2.49395 (lr=1.9e-03)\n",
      "[step 320/accstep :  (1/1)]: loss=2.09238 (lr=1.9e-03)\n",
      "[step 321/accstep :  (1/1)]: loss=2.48128 (lr=1.9e-03)\n",
      "[step 322/accstep :  (1/1)]: loss=1.49075 (lr=1.9e-03)\n",
      "[step 323/accstep :  (1/1)]: loss=3.02447 (lr=1.9e-03)\n",
      "[step 324/accstep :  (1/1)]: loss=3.04112 (lr=1.9e-03)\n",
      "[step 325/accstep :  (1/1)]: loss=2.12965 (lr=1.9e-03)\n",
      "[step 326/accstep :  (1/1)]: loss=2.46881 (lr=2.0e-03)\n",
      "[step 327/accstep :  (1/1)]: loss=2.50679 (lr=2.0e-03)\n",
      "[step 328/accstep :  (1/1)]: loss=4.17033 (lr=2.0e-03)\n",
      "[step 329/accstep :  (1/1)]: loss=3.82376 (lr=2.0e-03)\n",
      "[step 330/accstep :  (1/1)]: loss=3.35137 (lr=2.0e-03)\n",
      "[step 331/accstep :  (1/1)]: loss=2.26114 (lr=2.0e-03)\n",
      "[step 332/accstep :  (1/1)]: loss=3.39487 (lr=2.0e-03)\n",
      "[step 333/accstep :  (1/1)]: loss=3.05510 (lr=2.0e-03)\n",
      "[step 334/accstep :  (1/1)]: loss=2.85005 (lr=2.0e-03)\n",
      "[step 335/accstep :  (1/1)]: loss=2.59143 (lr=2.0e-03)\n",
      "[step 336/accstep :  (1/1)]: loss=2.84903 (lr=2.0e-03)\n",
      "[step 337/accstep :  (1/1)]: loss=2.75921 (lr=2.0e-03)\n",
      "[step 338/accstep :  (1/1)]: loss=4.07435 (lr=2.0e-03)\n",
      "[step 339/accstep :  (1/1)]: loss=2.64295 (lr=2.0e-03)\n",
      "[step 340/accstep :  (1/1)]: loss=4.20346 (lr=2.0e-03)\n",
      "[step 341/accstep :  (1/1)]: loss=1.87073 (lr=2.0e-03)\n",
      "[step 342/accstep :  (1/1)]: loss=2.53489 (lr=2.1e-03)\n",
      "[step 343/accstep :  (1/1)]: loss=2.03733 (lr=2.1e-03)\n",
      "[step 344/accstep :  (1/1)]: loss=2.75557 (lr=2.1e-03)\n",
      "[step 345/accstep :  (1/1)]: loss=1.96141 (lr=2.1e-03)\n",
      "[step 346/accstep :  (1/1)]: loss=2.40289 (lr=2.1e-03)\n",
      "[step 347/accstep :  (1/1)]: loss=2.36506 (lr=2.1e-03)\n",
      "[step 348/accstep :  (1/1)]: loss=2.09111 (lr=2.1e-03)\n",
      "[step 349/accstep :  (1/1)]: loss=2.14985 (lr=2.1e-03)\n",
      "[step 350/accstep :  (1/1)]: loss=2.15478 (lr=2.1e-03)\n",
      "[step 351/accstep :  (1/1)]: loss=2.36296 (lr=2.1e-03)\n",
      "[step 352/accstep :  (1/1)]: loss=2.75835 (lr=2.1e-03)\n",
      "[step 353/accstep :  (1/1)]: loss=2.87954 (lr=2.1e-03)\n",
      "[step 354/accstep :  (1/1)]: loss=2.16479 (lr=2.1e-03)\n",
      "[step 355/accstep :  (1/1)]: loss=2.36334 (lr=2.1e-03)\n",
      "[step 356/accstep :  (1/1)]: loss=2.16692 (lr=2.1e-03)\n",
      "[step 357/accstep :  (1/1)]: loss=2.70546 (lr=2.1e-03)\n",
      "[step 358/accstep :  (1/1)]: loss=3.64684 (lr=2.1e-03)\n",
      "[step 359/accstep :  (1/1)]: loss=2.96280 (lr=2.2e-03)\n",
      "[step 360/accstep :  (1/1)]: loss=2.48641 (lr=2.2e-03)\n",
      "[step 361/accstep :  (1/1)]: loss=2.71607 (lr=2.2e-03)\n",
      "[step 362/accstep :  (1/1)]: loss=3.12245 (lr=2.2e-03)\n",
      "[step 363/accstep :  (1/1)]: loss=2.74077 (lr=2.2e-03)\n",
      "[step 364/accstep :  (1/1)]: loss=3.20278 (lr=2.2e-03)\n",
      "[step 365/accstep :  (1/1)]: loss=3.13528 (lr=2.2e-03)\n",
      "[step 366/accstep :  (1/1)]: loss=3.92786 (lr=2.2e-03)\n",
      "[step 367/accstep :  (1/1)]: loss=4.20069 (lr=2.2e-03)\n",
      "[step 368/accstep :  (1/1)]: loss=3.21577 (lr=2.2e-03)\n",
      "[step 369/accstep :  (1/1)]: loss=3.14872 (lr=2.2e-03)\n",
      "[step 370/accstep :  (1/1)]: loss=2.89691 (lr=2.2e-03)\n",
      "[step 371/accstep :  (1/1)]: loss=2.86453 (lr=2.2e-03)\n",
      "[step 372/accstep :  (1/1)]: loss=2.72864 (lr=2.2e-03)\n",
      "[step 373/accstep :  (1/1)]: loss=2.35159 (lr=2.2e-03)\n",
      "[step 374/accstep :  (1/1)]: loss=2.96107 (lr=2.2e-03)\n",
      "[step 375/accstep :  (1/1)]: loss=2.95612 (lr=2.2e-03)\n",
      "[step 376/accstep :  (1/1)]: loss=3.02045 (lr=2.3e-03)\n",
      "[step 377/accstep :  (1/1)]: loss=2.40495 (lr=2.3e-03)\n",
      "[step 378/accstep :  (1/1)]: loss=2.30743 (lr=2.3e-03)\n",
      "[step 379/accstep :  (1/1)]: loss=1.82003 (lr=2.3e-03)\n",
      "[step 380/accstep :  (1/1)]: loss=2.90827 (lr=2.3e-03)\n",
      "[step 381/accstep :  (1/1)]: loss=2.10964 (lr=2.3e-03)\n",
      "[step 382/accstep :  (1/1)]: loss=4.94800 (lr=2.3e-03)\n",
      "[step 383/accstep :  (1/1)]: loss=2.60477 (lr=2.3e-03)\n",
      "[step 384/accstep :  (1/1)]: loss=1.65242 (lr=2.3e-03)\n",
      "[step 385/accstep :  (1/1)]: loss=2.93125 (lr=2.3e-03)\n",
      "[step 386/accstep :  (1/1)]: loss=2.54842 (lr=2.3e-03)\n",
      "[step 387/accstep :  (1/1)]: loss=2.46233 (lr=2.3e-03)\n",
      "[step 388/accstep :  (1/1)]: loss=3.59564 (lr=2.3e-03)\n",
      "[step 389/accstep :  (1/1)]: loss=3.23428 (lr=2.3e-03)\n",
      "[step 390/accstep :  (1/1)]: loss=2.19048 (lr=2.3e-03)\n",
      "[step 391/accstep :  (1/1)]: loss=3.29866 (lr=2.3e-03)\n",
      "[step 392/accstep :  (1/1)]: loss=2.45870 (lr=2.4e-03)\n",
      "[step 393/accstep :  (1/1)]: loss=2.48118 (lr=2.4e-03)\n",
      "[step 394/accstep :  (1/1)]: loss=3.63296 (lr=2.4e-03)\n",
      "[step 395/accstep :  (1/1)]: loss=2.33411 (lr=2.4e-03)\n",
      "[step 396/accstep :  (1/1)]: loss=2.63561 (lr=2.4e-03)\n",
      "[step 397/accstep :  (1/1)]: loss=2.14943 (lr=2.4e-03)\n",
      "[step 398/accstep :  (1/1)]: loss=2.08492 (lr=2.4e-03)\n",
      "[step 399/accstep :  (1/1)]: loss=2.24329 (lr=2.4e-03)\n",
      "[step 400/accstep :  (1/1)]: loss=4.47360 (lr=2.4e-03)\n",
      "[step 401/accstep :  (1/1)]: loss=1.95767 (lr=2.4e-03)\n",
      "[step 402/accstep :  (1/1)]: loss=3.51644 (lr=2.4e-03)\n",
      "[step 403/accstep :  (1/1)]: loss=2.14566 (lr=2.4e-03)\n",
      "[step 404/accstep :  (1/1)]: loss=2.79749 (lr=2.4e-03)\n",
      "[step 405/accstep :  (1/1)]: loss=3.30791 (lr=2.4e-03)\n",
      "[step 406/accstep :  (1/1)]: loss=3.70746 (lr=2.4e-03)\n",
      "[step 407/accstep :  (1/1)]: loss=3.01216 (lr=2.4e-03)\n",
      "[step 408/accstep :  (1/1)]: loss=3.01905 (lr=2.4e-03)\n",
      "[step 409/accstep :  (1/1)]: loss=2.24900 (lr=2.5e-03)\n",
      "[step 410/accstep :  (1/1)]: loss=1.83398 (lr=2.5e-03)\n",
      "[step 411/accstep :  (1/1)]: loss=2.80266 (lr=2.5e-03)\n",
      "[step 412/accstep :  (1/1)]: loss=2.28391 (lr=2.5e-03)\n",
      "[step 413/accstep :  (1/1)]: loss=3.53943 (lr=2.5e-03)\n",
      "[step 414/accstep :  (1/1)]: loss=1.82796 (lr=2.5e-03)\n",
      "[step 415/accstep :  (1/1)]: loss=3.08422 (lr=2.5e-03)\n",
      "[step 416/accstep :  (1/1)]: loss=1.88869 (lr=2.5e-03)\n",
      "[step 417/accstep :  (1/1)]: loss=3.31488 (lr=2.5e-03)\n",
      "[step 418/accstep :  (1/1)]: loss=2.14312 (lr=2.5e-03)\n",
      "[step 419/accstep :  (1/1)]: loss=2.46835 (lr=2.5e-03)\n",
      "[step 420/accstep :  (1/1)]: loss=2.30548 (lr=2.5e-03)\n",
      "[step 421/accstep :  (1/1)]: loss=2.91354 (lr=2.5e-03)\n",
      "[step 422/accstep :  (1/1)]: loss=1.69985 (lr=2.5e-03)\n",
      "[step 423/accstep :  (1/1)]: loss=2.27745 (lr=2.5e-03)\n",
      "[step 424/accstep :  (1/1)]: loss=2.26238 (lr=2.5e-03)\n",
      "[step 425/accstep :  (1/1)]: loss=3.46644 (lr=2.6e-03)\n",
      "[step 426/accstep :  (1/1)]: loss=2.29523 (lr=2.6e-03)\n",
      "[step 427/accstep :  (1/1)]: loss=2.46428 (lr=2.6e-03)\n",
      "[step 428/accstep :  (1/1)]: loss=1.45029 (lr=2.6e-03)\n",
      "[step 429/accstep :  (1/1)]: loss=4.43282 (lr=2.6e-03)\n",
      "[step 430/accstep :  (1/1)]: loss=4.34003 (lr=2.6e-03)\n",
      "[step 431/accstep :  (1/1)]: loss=2.72677 (lr=2.6e-03)\n",
      "[step 432/accstep :  (1/1)]: loss=3.20689 (lr=2.6e-03)\n",
      "[step 433/accstep :  (1/1)]: loss=2.94970 (lr=2.6e-03)\n",
      "[step 434/accstep :  (1/1)]: loss=2.69989 (lr=2.6e-03)\n",
      "[step 435/accstep :  (1/1)]: loss=4.18067 (lr=2.6e-03)\n",
      "[step 436/accstep :  (1/1)]: loss=1.98702 (lr=2.6e-03)\n",
      "[step 437/accstep :  (1/1)]: loss=2.34239 (lr=2.6e-03)\n",
      "[step 438/accstep :  (1/1)]: loss=3.06075 (lr=2.6e-03)\n",
      "[step 439/accstep :  (1/1)]: loss=2.16198 (lr=2.6e-03)\n",
      "[step 440/accstep :  (1/1)]: loss=2.79745 (lr=2.6e-03)\n",
      "[step 441/accstep :  (1/1)]: loss=2.91178 (lr=2.6e-03)\n",
      "[step 442/accstep :  (1/1)]: loss=2.65584 (lr=2.7e-03)\n",
      "[step 443/accstep :  (1/1)]: loss=2.34733 (lr=2.7e-03)\n",
      "[step 444/accstep :  (1/1)]: loss=2.59126 (lr=2.7e-03)\n",
      "[step 445/accstep :  (1/1)]: loss=4.18680 (lr=2.7e-03)\n",
      "[step 446/accstep :  (1/1)]: loss=2.28113 (lr=2.7e-03)\n",
      "[step 447/accstep :  (1/1)]: loss=1.67299 (lr=2.7e-03)\n",
      "[step 448/accstep :  (1/1)]: loss=2.40523 (lr=2.7e-03)\n",
      "[step 449/accstep :  (1/1)]: loss=2.58052 (lr=2.7e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 450/accstep :  (1/1)]: loss=2.39375 (lr=2.7e-03)\n",
      "[step 451/accstep :  (1/1)]: loss=2.49055 (lr=2.7e-03)\n",
      "[step 452/accstep :  (1/1)]: loss=1.66521 (lr=2.7e-03)\n",
      "[step 453/accstep :  (1/1)]: loss=2.10436 (lr=2.7e-03)\n",
      "[step 454/accstep :  (1/1)]: loss=2.90610 (lr=2.7e-03)\n",
      "[step 455/accstep :  (1/1)]: loss=2.22133 (lr=2.7e-03)\n",
      "[step 456/accstep :  (1/1)]: loss=1.93229 (lr=2.7e-03)\n",
      "[step 457/accstep :  (1/1)]: loss=2.86369 (lr=2.7e-03)\n",
      "[step 458/accstep :  (1/1)]: loss=1.81945 (lr=2.7e-03)\n",
      "[step 459/accstep :  (1/1)]: loss=1.41834 (lr=2.8e-03)\n",
      "[step 460/accstep :  (1/1)]: loss=2.59692 (lr=2.8e-03)\n",
      "[step 461/accstep :  (1/1)]: loss=1.41431 (lr=2.8e-03)\n",
      "[step 462/accstep :  (1/1)]: loss=1.36542 (lr=2.8e-03)\n",
      "[step 463/accstep :  (1/1)]: loss=1.92526 (lr=2.8e-03)\n",
      "[step 464/accstep :  (1/1)]: loss=2.59565 (lr=2.8e-03)\n",
      "[step 465/accstep :  (1/1)]: loss=4.05447 (lr=2.8e-03)\n",
      "[step 466/accstep :  (1/1)]: loss=2.47030 (lr=2.8e-03)\n",
      "[step 467/accstep :  (1/1)]: loss=2.69814 (lr=2.8e-03)\n",
      "[step 468/accstep :  (1/1)]: loss=3.52713 (lr=2.8e-03)\n",
      "[step 469/accstep :  (1/1)]: loss=4.13427 (lr=2.8e-03)\n",
      "[step 470/accstep :  (1/1)]: loss=2.69416 (lr=2.8e-03)\n",
      "[step 471/accstep :  (1/1)]: loss=2.25079 (lr=2.8e-03)\n",
      "[step 472/accstep :  (1/1)]: loss=2.11602 (lr=2.8e-03)\n",
      "[step 473/accstep :  (1/1)]: loss=1.90836 (lr=2.8e-03)\n",
      "[step 474/accstep :  (1/1)]: loss=2.93821 (lr=2.8e-03)\n",
      "[step 475/accstep :  (1/1)]: loss=2.62374 (lr=2.9e-03)\n",
      "[step 476/accstep :  (1/1)]: loss=2.65868 (lr=2.9e-03)\n",
      "[step 477/accstep :  (1/1)]: loss=2.86667 (lr=2.9e-03)\n",
      "[step 478/accstep :  (1/1)]: loss=2.40621 (lr=2.9e-03)\n",
      "[step 479/accstep :  (1/1)]: loss=2.07488 (lr=2.9e-03)\n",
      "[step 480/accstep :  (1/1)]: loss=3.53786 (lr=2.9e-03)\n",
      "[step 481/accstep :  (1/1)]: loss=2.92639 (lr=2.9e-03)\n",
      "[step 482/accstep :  (1/1)]: loss=2.87760 (lr=2.9e-03)\n",
      "[step 483/accstep :  (1/1)]: loss=2.68982 (lr=2.9e-03)\n",
      "[step 484/accstep :  (1/1)]: loss=2.89768 (lr=2.9e-03)\n",
      "[step 485/accstep :  (1/1)]: loss=3.48267 (lr=2.9e-03)\n",
      "[step 486/accstep :  (1/1)]: loss=4.01696 (lr=2.9e-03)\n",
      "[step 487/accstep :  (1/1)]: loss=2.00268 (lr=2.9e-03)\n",
      "[step 488/accstep :  (1/1)]: loss=3.33043 (lr=2.9e-03)\n",
      "[step 489/accstep :  (1/1)]: loss=2.75934 (lr=2.9e-03)\n",
      "[step 490/accstep :  (1/1)]: loss=2.73042 (lr=2.9e-03)\n",
      "[step 491/accstep :  (1/1)]: loss=2.90638 (lr=2.9e-03)\n",
      "[step 492/accstep :  (1/1)]: loss=2.77525 (lr=3.0e-03)\n",
      "[step 493/accstep :  (1/1)]: loss=2.67325 (lr=3.0e-03)\n",
      "[step 494/accstep :  (1/1)]: loss=2.05973 (lr=3.0e-03)\n",
      "[step 495/accstep :  (1/1)]: loss=1.64464 (lr=3.0e-03)\n",
      "[step 496/accstep :  (1/1)]: loss=2.47742 (lr=3.0e-03)\n",
      "[step 497/accstep :  (1/1)]: loss=3.64720 (lr=3.0e-03)\n",
      "[step 498/accstep :  (1/1)]: loss=3.03171 (lr=3.0e-03)\n",
      "[step 499/accstep :  (1/1)]: loss=2.18368 (lr=3.0e-03)\n",
      "[step 500/accstep :  (1/1)]: loss=2.54924 (lr=3.0e-03)\n",
      "[step 501/accstep :  (1/1)]: loss=2.80061 (lr=3.0e-03)\n",
      "[step 502/accstep :  (1/1)]: loss=2.23780 (lr=3.0e-03)\n",
      "[step 503/accstep :  (1/1)]: loss=2.30067 (lr=3.0e-03)\n",
      "[step 504/accstep :  (1/1)]: loss=1.67310 (lr=3.0e-03)\n",
      "[step 505/accstep :  (1/1)]: loss=1.11672 (lr=3.0e-03)\n",
      "[step 506/accstep :  (1/1)]: loss=2.17241 (lr=3.0e-03)\n",
      "[step 507/accstep :  (1/1)]: loss=3.02639 (lr=3.0e-03)\n",
      "[step 508/accstep :  (1/1)]: loss=1.76612 (lr=3.0e-03)\n",
      "[step 509/accstep :  (1/1)]: loss=3.20957 (lr=3.0e-03)\n",
      "[step 510/accstep :  (1/1)]: loss=2.15875 (lr=3.0e-03)\n",
      "[step 511/accstep :  (1/1)]: loss=2.40623 (lr=3.0e-03)\n",
      "[step 512/accstep :  (1/1)]: loss=2.76624 (lr=3.0e-03)\n",
      "[step 513/accstep :  (1/1)]: loss=4.73195 (lr=3.0e-03)\n",
      "[step 514/accstep :  (1/1)]: loss=2.24834 (lr=3.0e-03)\n",
      "[step 515/accstep :  (1/1)]: loss=2.93608 (lr=3.0e-03)\n",
      "[step 516/accstep :  (1/1)]: loss=2.52017 (lr=3.0e-03)\n",
      "[step 517/accstep :  (1/1)]: loss=2.40907 (lr=3.0e-03)\n",
      "[step 518/accstep :  (1/1)]: loss=2.60891 (lr=3.0e-03)\n",
      "[step 519/accstep :  (1/1)]: loss=1.85455 (lr=3.0e-03)\n",
      "[step 520/accstep :  (1/1)]: loss=3.22609 (lr=3.0e-03)\n",
      "[step 521/accstep :  (1/1)]: loss=2.52457 (lr=3.0e-03)\n",
      "[step 522/accstep :  (1/1)]: loss=2.51057 (lr=3.0e-03)\n",
      "[step 523/accstep :  (1/1)]: loss=2.92864 (lr=3.0e-03)\n",
      "[step 524/accstep :  (1/1)]: loss=2.70225 (lr=3.0e-03)\n",
      "[step 525/accstep :  (1/1)]: loss=2.75520 (lr=3.0e-03)\n",
      "[step 526/accstep :  (1/1)]: loss=2.33225 (lr=3.0e-03)\n",
      "[step 527/accstep :  (1/1)]: loss=2.07187 (lr=3.0e-03)\n",
      "[step 528/accstep :  (1/1)]: loss=2.84937 (lr=3.0e-03)\n",
      "[step 529/accstep :  (1/1)]: loss=2.48257 (lr=3.0e-03)\n",
      "[step 530/accstep :  (1/1)]: loss=3.19910 (lr=3.0e-03)\n",
      "[step 531/accstep :  (1/1)]: loss=2.68153 (lr=3.0e-03)\n",
      "[step 532/accstep :  (1/1)]: loss=2.14354 (lr=3.0e-03)\n",
      "[step 533/accstep :  (1/1)]: loss=2.56051 (lr=3.0e-03)\n",
      "[step 534/accstep :  (1/1)]: loss=1.84703 (lr=3.0e-03)\n",
      "[step 535/accstep :  (1/1)]: loss=2.98604 (lr=3.0e-03)\n",
      "[step 536/accstep :  (1/1)]: loss=2.37780 (lr=3.0e-03)\n",
      "[step 537/accstep :  (1/1)]: loss=2.49643 (lr=3.0e-03)\n",
      "[step 538/accstep :  (1/1)]: loss=2.92567 (lr=3.0e-03)\n",
      "[step 539/accstep :  (1/1)]: loss=2.92372 (lr=3.0e-03)\n",
      "[step 540/accstep :  (1/1)]: loss=1.50977 (lr=3.0e-03)\n",
      "[step 541/accstep :  (1/1)]: loss=3.88172 (lr=3.0e-03)\n",
      "[step 542/accstep :  (1/1)]: loss=3.85688 (lr=3.0e-03)\n",
      "[step 543/accstep :  (1/1)]: loss=2.50904 (lr=3.0e-03)\n",
      "[step 544/accstep :  (1/1)]: loss=1.87825 (lr=3.0e-03)\n",
      "[step 545/accstep :  (1/1)]: loss=2.50553 (lr=3.0e-03)\n",
      "[step 546/accstep :  (1/1)]: loss=2.69465 (lr=3.0e-03)\n",
      "[step 547/accstep :  (1/1)]: loss=3.11877 (lr=3.0e-03)\n",
      "[step 548/accstep :  (1/1)]: loss=2.35041 (lr=3.0e-03)\n",
      "[step 549/accstep :  (1/1)]: loss=3.93374 (lr=3.0e-03)\n",
      "[step 550/accstep :  (1/1)]: loss=2.70637 (lr=3.0e-03)\n",
      "[step 551/accstep :  (1/1)]: loss=2.43334 (lr=3.0e-03)\n",
      "[step 552/accstep :  (1/1)]: loss=2.22561 (lr=3.0e-03)\n",
      "[step 553/accstep :  (1/1)]: loss=3.14984 (lr=3.0e-03)\n",
      "[step 554/accstep :  (1/1)]: loss=2.23960 (lr=3.0e-03)\n",
      "[step 555/accstep :  (1/1)]: loss=3.22833 (lr=3.0e-03)\n",
      "[step 556/accstep :  (1/1)]: loss=2.84125 (lr=3.0e-03)\n",
      "[step 557/accstep :  (1/1)]: loss=2.27449 (lr=3.0e-03)\n",
      "[step 558/accstep :  (1/1)]: loss=3.44438 (lr=3.0e-03)\n",
      "[step 559/accstep :  (1/1)]: loss=2.85041 (lr=3.0e-03)\n",
      "[step 560/accstep :  (1/1)]: loss=2.39758 (lr=3.0e-03)\n",
      "[step 561/accstep :  (1/1)]: loss=2.27495 (lr=3.0e-03)\n",
      "[step 562/accstep :  (1/1)]: loss=2.33621 (lr=3.0e-03)\n",
      "[step 563/accstep :  (1/1)]: loss=2.90008 (lr=3.0e-03)\n",
      "[step 564/accstep :  (1/1)]: loss=2.02254 (lr=3.0e-03)\n",
      "[step 565/accstep :  (1/1)]: loss=2.80483 (lr=3.0e-03)\n",
      "[step 566/accstep :  (1/1)]: loss=3.23849 (lr=3.0e-03)\n",
      "[step 567/accstep :  (1/1)]: loss=3.61843 (lr=3.0e-03)\n",
      "[step 568/accstep :  (1/1)]: loss=1.73162 (lr=3.0e-03)\n",
      "[step 569/accstep :  (1/1)]: loss=4.73994 (lr=3.0e-03)\n",
      "[step 570/accstep :  (1/1)]: loss=2.43048 (lr=3.0e-03)\n",
      "[step 571/accstep :  (1/1)]: loss=2.83122 (lr=3.0e-03)\n",
      "[step 572/accstep :  (1/1)]: loss=2.13156 (lr=3.0e-03)\n",
      "[step 573/accstep :  (1/1)]: loss=2.26733 (lr=3.0e-03)\n",
      "[step 574/accstep :  (1/1)]: loss=2.35759 (lr=3.0e-03)\n",
      "[step 575/accstep :  (1/1)]: loss=2.59731 (lr=3.0e-03)\n",
      "[step 576/accstep :  (1/1)]: loss=2.14572 (lr=3.0e-03)\n",
      "[step 577/accstep :  (1/1)]: loss=2.12013 (lr=3.0e-03)\n",
      "[step 578/accstep :  (1/1)]: loss=2.06078 (lr=3.0e-03)\n",
      "[step 579/accstep :  (1/1)]: loss=2.89868 (lr=3.0e-03)\n",
      "[step 580/accstep :  (1/1)]: loss=3.27827 (lr=3.0e-03)\n",
      "[step 581/accstep :  (1/1)]: loss=2.34393 (lr=3.0e-03)\n",
      "[step 582/accstep :  (1/1)]: loss=2.25615 (lr=3.0e-03)\n",
      "[step 583/accstep :  (1/1)]: loss=1.74581 (lr=3.0e-03)\n",
      "[step 584/accstep :  (1/1)]: loss=3.76110 (lr=3.0e-03)\n",
      "[step 585/accstep :  (1/1)]: loss=1.85100 (lr=3.0e-03)\n",
      "[step 586/accstep :  (1/1)]: loss=1.82186 (lr=3.0e-03)\n",
      "[step 587/accstep :  (1/1)]: loss=3.09731 (lr=3.0e-03)\n",
      "[step 588/accstep :  (1/1)]: loss=2.29021 (lr=3.0e-03)\n",
      "[step 589/accstep :  (1/1)]: loss=1.57465 (lr=3.0e-03)\n",
      "[step 590/accstep :  (1/1)]: loss=2.28801 (lr=3.0e-03)\n",
      "[step 591/accstep :  (1/1)]: loss=2.01057 (lr=3.0e-03)\n",
      "[step 592/accstep :  (1/1)]: loss=2.51912 (lr=3.0e-03)\n",
      "[step 593/accstep :  (1/1)]: loss=2.05169 (lr=3.0e-03)\n",
      "[step 594/accstep :  (1/1)]: loss=2.06105 (lr=3.0e-03)\n",
      "[step 595/accstep :  (1/1)]: loss=3.05984 (lr=3.0e-03)\n",
      "[step 596/accstep :  (1/1)]: loss=2.14572 (lr=3.0e-03)\n",
      "[step 597/accstep :  (1/1)]: loss=2.01880 (lr=3.0e-03)\n",
      "[step 598/accstep :  (1/1)]: loss=2.41127 (lr=3.0e-03)\n",
      "[step 599/accstep :  (1/1)]: loss=1.28879 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 600/accstep :  (1/1)]: loss=1.58262 (lr=3.0e-03)\n",
      "[step 601/accstep :  (1/1)]: loss=2.89970 (lr=3.0e-03)\n",
      "[step 602/accstep :  (1/1)]: loss=1.82691 (lr=3.0e-03)\n",
      "[step 603/accstep :  (1/1)]: loss=2.94096 (lr=3.0e-03)\n",
      "[step 604/accstep :  (1/1)]: loss=2.23150 (lr=3.0e-03)\n",
      "[step 605/accstep :  (1/1)]: loss=3.53608 (lr=3.0e-03)\n",
      "[step 606/accstep :  (1/1)]: loss=2.06506 (lr=3.0e-03)\n",
      "[step 607/accstep :  (1/1)]: loss=2.26299 (lr=3.0e-03)\n",
      "[step 608/accstep :  (1/1)]: loss=4.03053 (lr=3.0e-03)\n",
      "[step 609/accstep :  (1/1)]: loss=2.58543 (lr=3.0e-03)\n",
      "[step 610/accstep :  (1/1)]: loss=2.27389 (lr=3.0e-03)\n",
      "[step 611/accstep :  (1/1)]: loss=1.71170 (lr=3.0e-03)\n",
      "[step 612/accstep :  (1/1)]: loss=2.74217 (lr=3.0e-03)\n",
      "[step 613/accstep :  (1/1)]: loss=2.32069 (lr=3.0e-03)\n",
      "[step 614/accstep :  (1/1)]: loss=3.65269 (lr=3.0e-03)\n",
      "[step 615/accstep :  (1/1)]: loss=2.41734 (lr=3.0e-03)\n",
      "[step 616/accstep :  (1/1)]: loss=1.93204 (lr=3.0e-03)\n",
      "[step 617/accstep :  (1/1)]: loss=1.37257 (lr=3.0e-03)\n",
      "[step 618/accstep :  (1/1)]: loss=1.82818 (lr=3.0e-03)\n",
      "[step 619/accstep :  (1/1)]: loss=1.90824 (lr=3.0e-03)\n",
      "[step 620/accstep :  (1/1)]: loss=2.09578 (lr=3.0e-03)\n",
      "[step 621/accstep :  (1/1)]: loss=2.80146 (lr=3.0e-03)\n",
      "[step 622/accstep :  (1/1)]: loss=2.45261 (lr=3.0e-03)\n",
      "[step 623/accstep :  (1/1)]: loss=2.34361 (lr=3.0e-03)\n",
      "[step 624/accstep :  (1/1)]: loss=1.83064 (lr=3.0e-03)\n",
      "[step 625/accstep :  (1/1)]: loss=1.90368 (lr=3.0e-03)\n",
      "[step 626/accstep :  (1/1)]: loss=1.57095 (lr=3.0e-03)\n",
      "[step 627/accstep :  (1/1)]: loss=2.44165 (lr=3.0e-03)\n",
      "[step 628/accstep :  (1/1)]: loss=1.12859 (lr=3.0e-03)\n",
      "[step 629/accstep :  (1/1)]: loss=1.30641 (lr=3.0e-03)\n",
      "[step 630/accstep :  (1/1)]: loss=2.22773 (lr=3.0e-03)\n",
      "[step 631/accstep :  (1/1)]: loss=1.27574 (lr=3.0e-03)\n",
      "[step 632/accstep :  (1/1)]: loss=1.55631 (lr=3.0e-03)\n",
      "[step 633/accstep :  (1/1)]: loss=3.65994 (lr=3.0e-03)\n",
      "[step 634/accstep :  (1/1)]: loss=1.97778 (lr=3.0e-03)\n",
      "[step 635/accstep :  (1/1)]: loss=1.17539 (lr=3.0e-03)\n",
      "[step 636/accstep :  (1/1)]: loss=3.07998 (lr=3.0e-03)\n",
      "[step 637/accstep :  (1/1)]: loss=2.02353 (lr=3.0e-03)\n",
      "[step 638/accstep :  (1/1)]: loss=2.20854 (lr=3.0e-03)\n",
      "[step 639/accstep :  (1/1)]: loss=1.96085 (lr=3.0e-03)\n",
      "[step 640/accstep :  (1/1)]: loss=2.99873 (lr=3.0e-03)\n",
      "[step 641/accstep :  (1/1)]: loss=1.89664 (lr=3.0e-03)\n",
      "[step 642/accstep :  (1/1)]: loss=3.31580 (lr=3.0e-03)\n",
      "[step 643/accstep :  (1/1)]: loss=1.71144 (lr=3.0e-03)\n",
      "[step 644/accstep :  (1/1)]: loss=3.85241 (lr=3.0e-03)\n",
      "[step 645/accstep :  (1/1)]: loss=1.91560 (lr=3.0e-03)\n",
      "[step 646/accstep :  (1/1)]: loss=2.30330 (lr=3.0e-03)\n",
      "[step 647/accstep :  (1/1)]: loss=3.27590 (lr=3.0e-03)\n",
      "[step 648/accstep :  (1/1)]: loss=2.70601 (lr=3.0e-03)\n",
      "[step 649/accstep :  (1/1)]: loss=2.83640 (lr=3.0e-03)\n",
      "[step 650/accstep :  (1/1)]: loss=4.30192 (lr=3.0e-03)\n",
      "[step 651/accstep :  (1/1)]: loss=2.40451 (lr=3.0e-03)\n",
      "[step 652/accstep :  (1/1)]: loss=3.44203 (lr=3.0e-03)\n",
      "[step 653/accstep :  (1/1)]: loss=2.77958 (lr=3.0e-03)\n",
      "[step 654/accstep :  (1/1)]: loss=3.05382 (lr=3.0e-03)\n",
      "[step 655/accstep :  (1/1)]: loss=2.21900 (lr=3.0e-03)\n",
      "[step 656/accstep :  (1/1)]: loss=2.73082 (lr=3.0e-03)\n",
      "[step 657/accstep :  (1/1)]: loss=2.87275 (lr=3.0e-03)\n",
      "[step 658/accstep :  (1/1)]: loss=1.99839 (lr=3.0e-03)\n",
      "[step 659/accstep :  (1/1)]: loss=2.81007 (lr=3.0e-03)\n",
      "[step 660/accstep :  (1/1)]: loss=3.72549 (lr=3.0e-03)\n",
      "[step 661/accstep :  (1/1)]: loss=4.21436 (lr=3.0e-03)\n",
      "[step 662/accstep :  (1/1)]: loss=3.41381 (lr=3.0e-03)\n",
      "[step 663/accstep :  (1/1)]: loss=2.50861 (lr=3.0e-03)\n",
      "[step 664/accstep :  (1/1)]: loss=2.30616 (lr=3.0e-03)\n",
      "[step 665/accstep :  (1/1)]: loss=2.11904 (lr=3.0e-03)\n",
      "[step 666/accstep :  (1/1)]: loss=2.10596 (lr=3.0e-03)\n",
      "[step 667/accstep :  (1/1)]: loss=2.86425 (lr=3.0e-03)\n",
      "[step 668/accstep :  (1/1)]: loss=3.09788 (lr=3.0e-03)\n",
      "[step 669/accstep :  (1/1)]: loss=2.02818 (lr=3.0e-03)\n",
      "[step 670/accstep :  (1/1)]: loss=2.58172 (lr=3.0e-03)\n",
      "[step 671/accstep :  (1/1)]: loss=2.11580 (lr=3.0e-03)\n",
      "[step 672/accstep :  (1/1)]: loss=1.67421 (lr=3.0e-03)\n",
      "[step 673/accstep :  (1/1)]: loss=1.79419 (lr=3.0e-03)\n",
      "[step 674/accstep :  (1/1)]: loss=2.44151 (lr=3.0e-03)\n",
      "[step 675/accstep :  (1/1)]: loss=1.58709 (lr=3.0e-03)\n",
      "[step 676/accstep :  (1/1)]: loss=1.93587 (lr=3.0e-03)\n",
      "[step 677/accstep :  (1/1)]: loss=2.56106 (lr=3.0e-03)\n",
      "[step 678/accstep :  (1/1)]: loss=1.28603 (lr=3.0e-03)\n",
      "[step 679/accstep :  (1/1)]: loss=3.14044 (lr=3.0e-03)\n",
      "[step 680/accstep :  (1/1)]: loss=1.28861 (lr=3.0e-03)\n",
      "[step 681/accstep :  (1/1)]: loss=1.61450 (lr=3.0e-03)\n",
      "[step 682/accstep :  (1/1)]: loss=4.43741 (lr=3.0e-03)\n",
      "[step 683/accstep :  (1/1)]: loss=1.82766 (lr=3.0e-03)\n",
      "[step 684/accstep :  (1/1)]: loss=2.63773 (lr=3.0e-03)\n",
      "[step 685/accstep :  (1/1)]: loss=1.84693 (lr=3.0e-03)\n",
      "[step 686/accstep :  (1/1)]: loss=4.67532 (lr=3.0e-03)\n",
      "[step 687/accstep :  (1/1)]: loss=4.03071 (lr=3.0e-03)\n",
      "[step 688/accstep :  (1/1)]: loss=2.68973 (lr=3.0e-03)\n",
      "[step 689/accstep :  (1/1)]: loss=1.53855 (lr=3.0e-03)\n",
      "[step 690/accstep :  (1/1)]: loss=3.31529 (lr=3.0e-03)\n",
      "[step 691/accstep :  (1/1)]: loss=2.34569 (lr=3.0e-03)\n",
      "[step 692/accstep :  (1/1)]: loss=2.56879 (lr=3.0e-03)\n",
      "[step 693/accstep :  (1/1)]: loss=3.87300 (lr=3.0e-03)\n",
      "[step 694/accstep :  (1/1)]: loss=2.22679 (lr=3.0e-03)\n",
      "[step 695/accstep :  (1/1)]: loss=2.70791 (lr=3.0e-03)\n",
      "[step 696/accstep :  (1/1)]: loss=2.12510 (lr=3.0e-03)\n",
      "[step 697/accstep :  (1/1)]: loss=1.76905 (lr=3.0e-03)\n",
      "[step 698/accstep :  (1/1)]: loss=2.04098 (lr=3.0e-03)\n",
      "[step 699/accstep :  (1/1)]: loss=2.30926 (lr=3.0e-03)\n",
      "[step 700/accstep :  (1/1)]: loss=1.71501 (lr=3.0e-03)\n",
      "[step 701/accstep :  (1/1)]: loss=1.93155 (lr=3.0e-03)\n",
      "[step 702/accstep :  (1/1)]: loss=1.85336 (lr=3.0e-03)\n",
      "[step 703/accstep :  (1/1)]: loss=2.97452 (lr=3.0e-03)\n",
      "[step 704/accstep :  (1/1)]: loss=2.86377 (lr=3.0e-03)\n",
      "[step 705/accstep :  (1/1)]: loss=2.49241 (lr=3.0e-03)\n",
      "[step 706/accstep :  (1/1)]: loss=1.79435 (lr=3.0e-03)\n",
      "[step 707/accstep :  (1/1)]: loss=2.81347 (lr=3.0e-03)\n",
      "[step 708/accstep :  (1/1)]: loss=2.29707 (lr=3.0e-03)\n",
      "[step 709/accstep :  (1/1)]: loss=1.75837 (lr=3.0e-03)\n",
      "[step 710/accstep :  (1/1)]: loss=1.86542 (lr=3.0e-03)\n",
      "[step 711/accstep :  (1/1)]: loss=1.52914 (lr=3.0e-03)\n",
      "[step 712/accstep :  (1/1)]: loss=2.30003 (lr=3.0e-03)\n",
      "[step 713/accstep :  (1/1)]: loss=1.20257 (lr=3.0e-03)\n",
      "[step 714/accstep :  (1/1)]: loss=2.73039 (lr=3.0e-03)\n",
      "[step 715/accstep :  (1/1)]: loss=2.38667 (lr=3.0e-03)\n",
      "[step 716/accstep :  (1/1)]: loss=2.35790 (lr=3.0e-03)\n",
      "[step 717/accstep :  (1/1)]: loss=2.12526 (lr=3.0e-03)\n",
      "[step 718/accstep :  (1/1)]: loss=2.31187 (lr=3.0e-03)\n",
      "[step 719/accstep :  (1/1)]: loss=2.89920 (lr=3.0e-03)\n",
      "[step 720/accstep :  (1/1)]: loss=1.53864 (lr=3.0e-03)\n",
      "[step 721/accstep :  (1/1)]: loss=1.26902 (lr=3.0e-03)\n",
      "[step 722/accstep :  (1/1)]: loss=2.21133 (lr=3.0e-03)\n",
      "[step 723/accstep :  (1/1)]: loss=2.75707 (lr=3.0e-03)\n",
      "[step 724/accstep :  (1/1)]: loss=3.21533 (lr=3.0e-03)\n",
      "[step 725/accstep :  (1/1)]: loss=1.47387 (lr=3.0e-03)\n",
      "[step 726/accstep :  (1/1)]: loss=3.97409 (lr=3.0e-03)\n",
      "[step 727/accstep :  (1/1)]: loss=2.06711 (lr=3.0e-03)\n",
      "[step 728/accstep :  (1/1)]: loss=1.78935 (lr=3.0e-03)\n",
      "[step 729/accstep :  (1/1)]: loss=4.05383 (lr=3.0e-03)\n",
      "[step 730/accstep :  (1/1)]: loss=2.87007 (lr=3.0e-03)\n",
      "[step 731/accstep :  (1/1)]: loss=1.68937 (lr=3.0e-03)\n",
      "[step 732/accstep :  (1/1)]: loss=2.16677 (lr=3.0e-03)\n",
      "[step 733/accstep :  (1/1)]: loss=2.62538 (lr=3.0e-03)\n",
      "[step 734/accstep :  (1/1)]: loss=2.52753 (lr=3.0e-03)\n",
      "[step 735/accstep :  (1/1)]: loss=3.59020 (lr=3.0e-03)\n",
      "[step 736/accstep :  (1/1)]: loss=2.59757 (lr=3.0e-03)\n",
      "[step 737/accstep :  (1/1)]: loss=1.66011 (lr=3.0e-03)\n",
      "[step 738/accstep :  (1/1)]: loss=2.19422 (lr=3.0e-03)\n",
      "[step 739/accstep :  (1/1)]: loss=1.88674 (lr=3.0e-03)\n",
      "[step 740/accstep :  (1/1)]: loss=1.64510 (lr=3.0e-03)\n",
      "[step 741/accstep :  (1/1)]: loss=2.21394 (lr=3.0e-03)\n",
      "[step 742/accstep :  (1/1)]: loss=2.49058 (lr=3.0e-03)\n",
      "[step 743/accstep :  (1/1)]: loss=2.18979 (lr=3.0e-03)\n",
      "[step 744/accstep :  (1/1)]: loss=2.09524 (lr=3.0e-03)\n",
      "[step 745/accstep :  (1/1)]: loss=1.80706 (lr=3.0e-03)\n",
      "[step 746/accstep :  (1/1)]: loss=1.77118 (lr=3.0e-03)\n",
      "[step 747/accstep :  (1/1)]: loss=3.35564 (lr=3.0e-03)\n",
      "[step 748/accstep :  (1/1)]: loss=1.80331 (lr=3.0e-03)\n",
      "[step 749/accstep :  (1/1)]: loss=2.11852 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 750/accstep :  (1/1)]: loss=2.71218 (lr=3.0e-03)\n",
      "[step 751/accstep :  (1/1)]: loss=1.28430 (lr=3.0e-03)\n",
      "[step 752/accstep :  (1/1)]: loss=1.70269 (lr=3.0e-03)\n",
      "[step 753/accstep :  (1/1)]: loss=1.94346 (lr=3.0e-03)\n",
      "[step 754/accstep :  (1/1)]: loss=2.10947 (lr=3.0e-03)\n",
      "[step 755/accstep :  (1/1)]: loss=3.51571 (lr=3.0e-03)\n",
      "[step 756/accstep :  (1/1)]: loss=1.65331 (lr=3.0e-03)\n",
      "[step 757/accstep :  (1/1)]: loss=1.62953 (lr=3.0e-03)\n",
      "[step 758/accstep :  (1/1)]: loss=1.10810 (lr=3.0e-03)\n",
      "[step 759/accstep :  (1/1)]: loss=1.75237 (lr=3.0e-03)\n",
      "[step 760/accstep :  (1/1)]: loss=4.33605 (lr=3.0e-03)\n",
      "[step 761/accstep :  (1/1)]: loss=4.12247 (lr=3.0e-03)\n",
      "[step 762/accstep :  (1/1)]: loss=2.74499 (lr=3.0e-03)\n",
      "[step 763/accstep :  (1/1)]: loss=2.17716 (lr=3.0e-03)\n",
      "[step 764/accstep :  (1/1)]: loss=2.26265 (lr=3.0e-03)\n",
      "[step 765/accstep :  (1/1)]: loss=2.28349 (lr=3.0e-03)\n",
      "[step 766/accstep :  (1/1)]: loss=1.99484 (lr=3.0e-03)\n",
      "[step 767/accstep :  (1/1)]: loss=2.39501 (lr=3.0e-03)\n",
      "[step 768/accstep :  (1/1)]: loss=2.15219 (lr=3.0e-03)\n",
      "[step 769/accstep :  (1/1)]: loss=1.29615 (lr=3.0e-03)\n",
      "[step 770/accstep :  (1/1)]: loss=1.88161 (lr=3.0e-03)\n",
      "[step 771/accstep :  (1/1)]: loss=1.82104 (lr=3.0e-03)\n",
      "[step 772/accstep :  (1/1)]: loss=1.01392 (lr=3.0e-03)\n",
      "[step 773/accstep :  (1/1)]: loss=2.05997 (lr=3.0e-03)\n",
      "[step 774/accstep :  (1/1)]: loss=2.89969 (lr=3.0e-03)\n",
      "[step 775/accstep :  (1/1)]: loss=2.25115 (lr=3.0e-03)\n",
      "[step 776/accstep :  (1/1)]: loss=3.51559 (lr=3.0e-03)\n",
      "[step 777/accstep :  (1/1)]: loss=2.39909 (lr=3.0e-03)\n",
      "[step 778/accstep :  (1/1)]: loss=3.63440 (lr=3.0e-03)\n",
      "[step 779/accstep :  (1/1)]: loss=3.19481 (lr=3.0e-03)\n",
      "[step 780/accstep :  (1/1)]: loss=1.93193 (lr=3.0e-03)\n",
      "[step 781/accstep :  (1/1)]: loss=3.63805 (lr=3.0e-03)\n",
      "[step 782/accstep :  (1/1)]: loss=2.05372 (lr=3.0e-03)\n",
      "[step 783/accstep :  (1/1)]: loss=2.23423 (lr=3.0e-03)\n",
      "[step 784/accstep :  (1/1)]: loss=3.32001 (lr=3.0e-03)\n",
      "[step 785/accstep :  (1/1)]: loss=2.24581 (lr=3.0e-03)\n",
      "[step 786/accstep :  (1/1)]: loss=2.44484 (lr=3.0e-03)\n",
      "[step 787/accstep :  (1/1)]: loss=2.67894 (lr=3.0e-03)\n",
      "[step 788/accstep :  (1/1)]: loss=2.03623 (lr=3.0e-03)\n",
      "[step 789/accstep :  (1/1)]: loss=2.39487 (lr=3.0e-03)\n",
      "[step 790/accstep :  (1/1)]: loss=1.75721 (lr=3.0e-03)\n",
      "[step 791/accstep :  (1/1)]: loss=2.04816 (lr=3.0e-03)\n",
      "[step 792/accstep :  (1/1)]: loss=2.38887 (lr=3.0e-03)\n",
      "[step 793/accstep :  (1/1)]: loss=1.81026 (lr=3.0e-03)\n",
      "[step 794/accstep :  (1/1)]: loss=2.28665 (lr=3.0e-03)\n",
      "[step 795/accstep :  (1/1)]: loss=2.39251 (lr=3.0e-03)\n",
      "[step 796/accstep :  (1/1)]: loss=2.19272 (lr=3.0e-03)\n",
      "[step 797/accstep :  (1/1)]: loss=1.89301 (lr=3.0e-03)\n",
      "[step 798/accstep :  (1/1)]: loss=2.33968 (lr=3.0e-03)\n",
      "[step 799/accstep :  (1/1)]: loss=1.09359 (lr=3.0e-03)\n",
      "[step 800/accstep :  (1/1)]: loss=2.01487 (lr=3.0e-03)\n",
      "[step 801/accstep :  (1/1)]: loss=1.64213 (lr=3.0e-03)\n",
      "[step 802/accstep :  (1/1)]: loss=1.77645 (lr=3.0e-03)\n",
      "[step 803/accstep :  (1/1)]: loss=3.98593 (lr=3.0e-03)\n",
      "[step 804/accstep :  (1/1)]: loss=2.10927 (lr=3.0e-03)\n",
      "[step 805/accstep :  (1/1)]: loss=2.34051 (lr=3.0e-03)\n",
      "[step 806/accstep :  (1/1)]: loss=2.80605 (lr=3.0e-03)\n",
      "[step 807/accstep :  (1/1)]: loss=2.95139 (lr=3.0e-03)\n",
      "[step 808/accstep :  (1/1)]: loss=1.96873 (lr=3.0e-03)\n",
      "[step 809/accstep :  (1/1)]: loss=1.83101 (lr=3.0e-03)\n",
      "[step 810/accstep :  (1/1)]: loss=4.17048 (lr=3.0e-03)\n",
      "[step 811/accstep :  (1/1)]: loss=2.28869 (lr=3.0e-03)\n",
      "[step 812/accstep :  (1/1)]: loss=2.48979 (lr=3.0e-03)\n",
      "[step 813/accstep :  (1/1)]: loss=2.11641 (lr=3.0e-03)\n",
      "[step 814/accstep :  (1/1)]: loss=2.06134 (lr=3.0e-03)\n",
      "[step 815/accstep :  (1/1)]: loss=3.17624 (lr=3.0e-03)\n",
      "[step 816/accstep :  (1/1)]: loss=2.17886 (lr=3.0e-03)\n",
      "[step 817/accstep :  (1/1)]: loss=3.65405 (lr=3.0e-03)\n",
      "[step 818/accstep :  (1/1)]: loss=1.89978 (lr=3.0e-03)\n",
      "[step 819/accstep :  (1/1)]: loss=1.41499 (lr=3.0e-03)\n",
      "[step 820/accstep :  (1/1)]: loss=2.05004 (lr=3.0e-03)\n",
      "[step 821/accstep :  (1/1)]: loss=1.75659 (lr=3.0e-03)\n",
      "[step 822/accstep :  (1/1)]: loss=2.23814 (lr=3.0e-03)\n",
      "[step 823/accstep :  (1/1)]: loss=1.83499 (lr=3.0e-03)\n",
      "[step 824/accstep :  (1/1)]: loss=1.47414 (lr=3.0e-03)\n",
      "[step 825/accstep :  (1/1)]: loss=2.59948 (lr=3.0e-03)\n",
      "[step 826/accstep :  (1/1)]: loss=2.32650 (lr=3.0e-03)\n",
      "[step 827/accstep :  (1/1)]: loss=2.48817 (lr=3.0e-03)\n",
      "[step 828/accstep :  (1/1)]: loss=3.06574 (lr=3.0e-03)\n",
      "[step 829/accstep :  (1/1)]: loss=1.54649 (lr=3.0e-03)\n",
      "[step 830/accstep :  (1/1)]: loss=2.13877 (lr=3.0e-03)\n",
      "[step 831/accstep :  (1/1)]: loss=4.86828 (lr=3.0e-03)\n",
      "[step 832/accstep :  (1/1)]: loss=2.69911 (lr=3.0e-03)\n",
      "[step 833/accstep :  (1/1)]: loss=2.23040 (lr=3.0e-03)\n",
      "[step 834/accstep :  (1/1)]: loss=2.18421 (lr=3.0e-03)\n",
      "[step 835/accstep :  (1/1)]: loss=2.92808 (lr=3.0e-03)\n",
      "[step 836/accstep :  (1/1)]: loss=2.20814 (lr=3.0e-03)\n",
      "[step 837/accstep :  (1/1)]: loss=2.18546 (lr=3.0e-03)\n",
      "[step 838/accstep :  (1/1)]: loss=3.31657 (lr=3.0e-03)\n",
      "[step 839/accstep :  (1/1)]: loss=2.08731 (lr=3.0e-03)\n",
      "[step 840/accstep :  (1/1)]: loss=1.93215 (lr=3.0e-03)\n",
      "[step 841/accstep :  (1/1)]: loss=1.85642 (lr=3.0e-03)\n",
      "[step 842/accstep :  (1/1)]: loss=4.02252 (lr=3.0e-03)\n",
      "[step 843/accstep :  (1/1)]: loss=2.89565 (lr=3.0e-03)\n",
      "[step 844/accstep :  (1/1)]: loss=2.01889 (lr=3.0e-03)\n",
      "[step 845/accstep :  (1/1)]: loss=1.88564 (lr=3.0e-03)\n",
      "[step 846/accstep :  (1/1)]: loss=3.93821 (lr=3.0e-03)\n",
      "[step 847/accstep :  (1/1)]: loss=1.65748 (lr=3.0e-03)\n",
      "[step 848/accstep :  (1/1)]: loss=2.55249 (lr=3.0e-03)\n",
      "[step 849/accstep :  (1/1)]: loss=1.77390 (lr=3.0e-03)\n",
      "[step 850/accstep :  (1/1)]: loss=3.53401 (lr=3.0e-03)\n",
      "[step 851/accstep :  (1/1)]: loss=1.64447 (lr=3.0e-03)\n",
      "[step 852/accstep :  (1/1)]: loss=2.83132 (lr=3.0e-03)\n",
      "[step 853/accstep :  (1/1)]: loss=2.91001 (lr=3.0e-03)\n",
      "[step 854/accstep :  (1/1)]: loss=2.39116 (lr=3.0e-03)\n",
      "[step 855/accstep :  (1/1)]: loss=2.41162 (lr=3.0e-03)\n",
      "[step 856/accstep :  (1/1)]: loss=1.47823 (lr=3.0e-03)\n",
      "[step 857/accstep :  (1/1)]: loss=2.06003 (lr=3.0e-03)\n",
      "[step 858/accstep :  (1/1)]: loss=2.66738 (lr=3.0e-03)\n",
      "[step 859/accstep :  (1/1)]: loss=3.39213 (lr=3.0e-03)\n",
      "[step 860/accstep :  (1/1)]: loss=2.49700 (lr=3.0e-03)\n",
      "[step 861/accstep :  (1/1)]: loss=1.54531 (lr=3.0e-03)\n",
      "[step 862/accstep :  (1/1)]: loss=1.45022 (lr=3.0e-03)\n",
      "[step 863/accstep :  (1/1)]: loss=2.02264 (lr=3.0e-03)\n",
      "[step 864/accstep :  (1/1)]: loss=1.58319 (lr=3.0e-03)\n",
      "[step 865/accstep :  (1/1)]: loss=1.40897 (lr=3.0e-03)\n",
      "[step 866/accstep :  (1/1)]: loss=1.79582 (lr=3.0e-03)\n",
      "[step 867/accstep :  (1/1)]: loss=1.87755 (lr=3.0e-03)\n",
      "[step 868/accstep :  (1/1)]: loss=1.36334 (lr=3.0e-03)\n",
      "[step 869/accstep :  (1/1)]: loss=1.86508 (lr=3.0e-03)\n",
      "[step 870/accstep :  (1/1)]: loss=2.60798 (lr=3.0e-03)\n",
      "[step 871/accstep :  (1/1)]: loss=1.74607 (lr=3.0e-03)\n",
      "[step 872/accstep :  (1/1)]: loss=1.72790 (lr=3.0e-03)\n",
      "[step 873/accstep :  (1/1)]: loss=1.10261 (lr=3.0e-03)\n",
      "[step 874/accstep :  (1/1)]: loss=2.23143 (lr=3.0e-03)\n",
      "[step 875/accstep :  (1/1)]: loss=2.24241 (lr=3.0e-03)\n",
      "[step 876/accstep :  (1/1)]: loss=1.72799 (lr=3.0e-03)\n",
      "[step 877/accstep :  (1/1)]: loss=1.87110 (lr=3.0e-03)\n",
      "[step 878/accstep :  (1/1)]: loss=2.64932 (lr=3.0e-03)\n",
      "[step 879/accstep :  (1/1)]: loss=4.06961 (lr=3.0e-03)\n",
      "[step 880/accstep :  (1/1)]: loss=2.51655 (lr=3.0e-03)\n",
      "[step 881/accstep :  (1/1)]: loss=1.26478 (lr=3.0e-03)\n",
      "[step 882/accstep :  (1/1)]: loss=2.89004 (lr=3.0e-03)\n",
      "[step 883/accstep :  (1/1)]: loss=2.11349 (lr=3.0e-03)\n",
      "[step 884/accstep :  (1/1)]: loss=2.27267 (lr=3.0e-03)\n",
      "[step 885/accstep :  (1/1)]: loss=1.68656 (lr=3.0e-03)\n",
      "[step 886/accstep :  (1/1)]: loss=2.98157 (lr=3.0e-03)\n",
      "[step 887/accstep :  (1/1)]: loss=1.89414 (lr=3.0e-03)\n",
      "[step 888/accstep :  (1/1)]: loss=1.87853 (lr=3.0e-03)\n",
      "[step 889/accstep :  (1/1)]: loss=2.73462 (lr=3.0e-03)\n",
      "[step 890/accstep :  (1/1)]: loss=1.40693 (lr=3.0e-03)\n",
      "[step 891/accstep :  (1/1)]: loss=3.40140 (lr=3.0e-03)\n",
      "[step 892/accstep :  (1/1)]: loss=2.43031 (lr=3.0e-03)\n",
      "[step 893/accstep :  (1/1)]: loss=1.30560 (lr=3.0e-03)\n",
      "[step 894/accstep :  (1/1)]: loss=4.13253 (lr=3.0e-03)\n",
      "[step 895/accstep :  (1/1)]: loss=1.22808 (lr=3.0e-03)\n",
      "[step 896/accstep :  (1/1)]: loss=1.54665 (lr=3.0e-03)\n",
      "[step 897/accstep :  (1/1)]: loss=1.55979 (lr=3.0e-03)\n",
      "[step 898/accstep :  (1/1)]: loss=1.72278 (lr=3.0e-03)\n",
      "[step 899/accstep :  (1/1)]: loss=1.40471 (lr=3.0e-03)\n",
      "[step 900/accstep :  (1/1)]: loss=1.49679 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 901/accstep :  (1/1)]: loss=2.08505 (lr=3.0e-03)\n",
      "[step 902/accstep :  (1/1)]: loss=2.14092 (lr=3.0e-03)\n",
      "[step 903/accstep :  (1/1)]: loss=1.60528 (lr=3.0e-03)\n",
      "[step 904/accstep :  (1/1)]: loss=2.04431 (lr=3.0e-03)\n",
      "[step 905/accstep :  (1/1)]: loss=1.97718 (lr=3.0e-03)\n",
      "[step 906/accstep :  (1/1)]: loss=1.70460 (lr=3.0e-03)\n",
      "[step 907/accstep :  (1/1)]: loss=2.15047 (lr=3.0e-03)\n",
      "[step 908/accstep :  (1/1)]: loss=1.49729 (lr=3.0e-03)\n",
      "[step 909/accstep :  (1/1)]: loss=2.31881 (lr=3.0e-03)\n",
      "[step 910/accstep :  (1/1)]: loss=1.24831 (lr=3.0e-03)\n",
      "[step 911/accstep :  (1/1)]: loss=2.59057 (lr=3.0e-03)\n",
      "[step 912/accstep :  (1/1)]: loss=1.68223 (lr=3.0e-03)\n",
      "[step 913/accstep :  (1/1)]: loss=2.01802 (lr=3.0e-03)\n",
      "[step 914/accstep :  (1/1)]: loss=2.25340 (lr=3.0e-03)\n",
      "[step 915/accstep :  (1/1)]: loss=1.84202 (lr=3.0e-03)\n",
      "[step 916/accstep :  (1/1)]: loss=2.09899 (lr=3.0e-03)\n",
      "[step 917/accstep :  (1/1)]: loss=2.12248 (lr=3.0e-03)\n",
      "[step 918/accstep :  (1/1)]: loss=1.71871 (lr=3.0e-03)\n",
      "[step 919/accstep :  (1/1)]: loss=1.54058 (lr=3.0e-03)\n",
      "[step 920/accstep :  (1/1)]: loss=1.38498 (lr=3.0e-03)\n",
      "[step 921/accstep :  (1/1)]: loss=1.97841 (lr=3.0e-03)\n",
      "[step 922/accstep :  (1/1)]: loss=1.98757 (lr=3.0e-03)\n",
      "[step 923/accstep :  (1/1)]: loss=2.15823 (lr=3.0e-03)\n",
      "[step 924/accstep :  (1/1)]: loss=1.85745 (lr=3.0e-03)\n",
      "[step 925/accstep :  (1/1)]: loss=1.89537 (lr=3.0e-03)\n",
      "[step 926/accstep :  (1/1)]: loss=1.80246 (lr=3.0e-03)\n",
      "[step 927/accstep :  (1/1)]: loss=2.61992 (lr=3.0e-03)\n",
      "[step 928/accstep :  (1/1)]: loss=3.20585 (lr=3.0e-03)\n",
      "[step 929/accstep :  (1/1)]: loss=2.35521 (lr=3.0e-03)\n",
      "[step 930/accstep :  (1/1)]: loss=2.18120 (lr=3.0e-03)\n",
      "[step 931/accstep :  (1/1)]: loss=1.96118 (lr=3.0e-03)\n",
      "[step 932/accstep :  (1/1)]: loss=2.51468 (lr=3.0e-03)\n",
      "[step 933/accstep :  (1/1)]: loss=1.74787 (lr=3.0e-03)\n",
      "[step 934/accstep :  (1/1)]: loss=2.11525 (lr=3.0e-03)\n",
      "[step 935/accstep :  (1/1)]: loss=1.86296 (lr=3.0e-03)\n",
      "[step 936/accstep :  (1/1)]: loss=2.69992 (lr=3.0e-03)\n",
      "[step 937/accstep :  (1/1)]: loss=2.76525 (lr=3.0e-03)\n",
      "[step 938/accstep :  (1/1)]: loss=2.13368 (lr=3.0e-03)\n",
      "[step 939/accstep :  (1/1)]: loss=2.81822 (lr=3.0e-03)\n",
      "[step 940/accstep :  (1/1)]: loss=2.84921 (lr=3.0e-03)\n",
      "[step 941/accstep :  (1/1)]: loss=3.91732 (lr=3.0e-03)\n",
      "[step 942/accstep :  (1/1)]: loss=1.45695 (lr=3.0e-03)\n",
      "[step 943/accstep :  (1/1)]: loss=2.04427 (lr=3.0e-03)\n",
      "[step 944/accstep :  (1/1)]: loss=1.70994 (lr=3.0e-03)\n",
      "[step 945/accstep :  (1/1)]: loss=2.24689 (lr=3.0e-03)\n",
      "[step 946/accstep :  (1/1)]: loss=1.57163 (lr=3.0e-03)\n",
      "[step 947/accstep :  (1/1)]: loss=2.11264 (lr=3.0e-03)\n",
      "[step 948/accstep :  (1/1)]: loss=2.69622 (lr=3.0e-03)\n",
      "[step 949/accstep :  (1/1)]: loss=1.75068 (lr=3.0e-03)\n",
      "[step 950/accstep :  (1/1)]: loss=1.98122 (lr=3.0e-03)\n",
      "[step 951/accstep :  (1/1)]: loss=1.24435 (lr=3.0e-03)\n",
      "[step 952/accstep :  (1/1)]: loss=1.61407 (lr=3.0e-03)\n",
      "[step 953/accstep :  (1/1)]: loss=1.80509 (lr=3.0e-03)\n",
      "[step 954/accstep :  (1/1)]: loss=2.73452 (lr=3.0e-03)\n",
      "[step 955/accstep :  (1/1)]: loss=2.02321 (lr=3.0e-03)\n",
      "[step 956/accstep :  (1/1)]: loss=3.25210 (lr=3.0e-03)\n",
      "[step 957/accstep :  (1/1)]: loss=1.50143 (lr=3.0e-03)\n",
      "[step 958/accstep :  (1/1)]: loss=1.46678 (lr=3.0e-03)\n",
      "[step 959/accstep :  (1/1)]: loss=2.94078 (lr=3.0e-03)\n",
      "[step 960/accstep :  (1/1)]: loss=1.45873 (lr=3.0e-03)\n",
      "[step 961/accstep :  (1/1)]: loss=2.04449 (lr=3.0e-03)\n",
      "[step 962/accstep :  (1/1)]: loss=2.65042 (lr=3.0e-03)\n",
      "[step 963/accstep :  (1/1)]: loss=2.40863 (lr=3.0e-03)\n",
      "[step 964/accstep :  (1/1)]: loss=1.87407 (lr=3.0e-03)\n",
      "[step 965/accstep :  (1/1)]: loss=0.67250 (lr=3.0e-03)\n",
      "[step 966/accstep :  (1/1)]: loss=1.72787 (lr=3.0e-03)\n",
      "[step 967/accstep :  (1/1)]: loss=2.12926 (lr=3.0e-03)\n",
      "[step 968/accstep :  (1/1)]: loss=1.80085 (lr=3.0e-03)\n",
      "[step 969/accstep :  (1/1)]: loss=1.40178 (lr=3.0e-03)\n",
      "[step 970/accstep :  (1/1)]: loss=2.23633 (lr=3.0e-03)\n",
      "[step 971/accstep :  (1/1)]: loss=2.83999 (lr=3.0e-03)\n",
      "[step 972/accstep :  (1/1)]: loss=2.37842 (lr=3.0e-03)\n",
      "[step 973/accstep :  (1/1)]: loss=1.90784 (lr=3.0e-03)\n",
      "[step 974/accstep :  (1/1)]: loss=2.44761 (lr=3.0e-03)\n",
      "[step 975/accstep :  (1/1)]: loss=2.20086 (lr=3.0e-03)\n",
      "[step 976/accstep :  (1/1)]: loss=2.12729 (lr=3.0e-03)\n",
      "[step 977/accstep :  (1/1)]: loss=3.91881 (lr=3.0e-03)\n",
      "[step 978/accstep :  (1/1)]: loss=1.70940 (lr=3.0e-03)\n",
      "[step 979/accstep :  (1/1)]: loss=1.26861 (lr=3.0e-03)\n",
      "[step 980/accstep :  (1/1)]: loss=2.23310 (lr=3.0e-03)\n",
      "[step 981/accstep :  (1/1)]: loss=1.58528 (lr=3.0e-03)\n",
      "[step 982/accstep :  (1/1)]: loss=2.71095 (lr=3.0e-03)\n",
      "[step 983/accstep :  (1/1)]: loss=1.13019 (lr=3.0e-03)\n",
      "[step 984/accstep :  (1/1)]: loss=1.62900 (lr=3.0e-03)\n",
      "[step 985/accstep :  (1/1)]: loss=2.24602 (lr=3.0e-03)\n",
      "[step 986/accstep :  (1/1)]: loss=0.98963 (lr=3.0e-03)\n",
      "[step 987/accstep :  (1/1)]: loss=1.93794 (lr=3.0e-03)\n",
      "[step 988/accstep :  (1/1)]: loss=1.51105 (lr=3.0e-03)\n",
      "[step 989/accstep :  (1/1)]: loss=2.71354 (lr=3.0e-03)\n",
      "[step 990/accstep :  (1/1)]: loss=2.43978 (lr=3.0e-03)\n",
      "[step 991/accstep :  (1/1)]: loss=1.82736 (lr=3.0e-03)\n",
      "[step 992/accstep :  (1/1)]: loss=2.05045 (lr=3.0e-03)\n",
      "[step 993/accstep :  (1/1)]: loss=1.76743 (lr=3.0e-03)\n",
      "[step 994/accstep :  (1/1)]: loss=4.02512 (lr=3.0e-03)\n",
      "[step 995/accstep :  (1/1)]: loss=1.64784 (lr=3.0e-03)\n",
      "[step 996/accstep :  (1/1)]: loss=0.99816 (lr=3.0e-03)\n",
      "[step 997/accstep :  (1/1)]: loss=1.47758 (lr=3.0e-03)\n",
      "[step 998/accstep :  (1/1)]: loss=2.45249 (lr=3.0e-03)\n",
      "[step 999/accstep :  (1/1)]: loss=1.40225 (lr=3.0e-03)\n",
      "[step 1000/accstep :  (1/1)]: loss=1.56710 (lr=3.0e-03)\n",
      "[step 1001/accstep :  (1/1)]: loss=1.94222 (lr=3.0e-03)\n",
      "[step 1002/accstep :  (1/1)]: loss=2.34399 (lr=3.0e-03)\n",
      "[step 1003/accstep :  (1/1)]: loss=2.54727 (lr=3.0e-03)\n",
      "[step 1004/accstep :  (1/1)]: loss=2.71374 (lr=3.0e-03)\n",
      "[step 1005/accstep :  (1/1)]: loss=1.91022 (lr=3.0e-03)\n",
      "[step 1006/accstep :  (1/1)]: loss=2.91837 (lr=3.0e-03)\n",
      "[step 1007/accstep :  (1/1)]: loss=1.67683 (lr=3.0e-03)\n",
      "[step 1008/accstep :  (1/1)]: loss=2.35012 (lr=3.0e-03)\n",
      "[step 1009/accstep :  (1/1)]: loss=1.72810 (lr=3.0e-03)\n",
      "[step 1010/accstep :  (1/1)]: loss=1.15990 (lr=3.0e-03)\n",
      "[step 1011/accstep :  (1/1)]: loss=1.06408 (lr=3.0e-03)\n",
      "[step 1012/accstep :  (1/1)]: loss=1.07547 (lr=3.0e-03)\n",
      "[step 1013/accstep :  (1/1)]: loss=1.72818 (lr=3.0e-03)\n",
      "[step 1014/accstep :  (1/1)]: loss=2.77064 (lr=3.0e-03)\n",
      "[step 1015/accstep :  (1/1)]: loss=2.17582 (lr=3.0e-03)\n",
      "[step 1016/accstep :  (1/1)]: loss=2.01744 (lr=3.0e-03)\n",
      "[step 1017/accstep :  (1/1)]: loss=1.17621 (lr=3.0e-03)\n",
      "[step 1018/accstep :  (1/1)]: loss=1.81868 (lr=3.0e-03)\n",
      "[step 1019/accstep :  (1/1)]: loss=1.28608 (lr=3.0e-03)\n",
      "[step 1020/accstep :  (1/1)]: loss=1.46654 (lr=3.0e-03)\n",
      "[step 1021/accstep :  (1/1)]: loss=1.93422 (lr=3.0e-03)\n",
      "[step 1022/accstep :  (1/1)]: loss=2.63968 (lr=3.0e-03)\n",
      "[step 1023/accstep :  (1/1)]: loss=1.54120 (lr=3.0e-03)\n",
      "[step 1024/accstep :  (1/1)]: loss=1.22176 (lr=3.0e-03)\n",
      "[step 1025/accstep :  (1/1)]: loss=1.89386 (lr=3.0e-03)\n",
      "[step 1026/accstep :  (1/1)]: loss=1.74497 (lr=3.0e-03)\n",
      "[step 1027/accstep :  (1/1)]: loss=1.64237 (lr=3.0e-03)\n",
      "[step 1028/accstep :  (1/1)]: loss=1.59634 (lr=3.0e-03)\n",
      "[step 1029/accstep :  (1/1)]: loss=1.62632 (lr=3.0e-03)\n",
      "[step 1030/accstep :  (1/1)]: loss=1.75345 (lr=3.0e-03)\n",
      "[step 1031/accstep :  (1/1)]: loss=1.12110 (lr=3.0e-03)\n",
      "[step 1032/accstep :  (1/1)]: loss=4.44132 (lr=3.0e-03)\n",
      "[step 1033/accstep :  (1/1)]: loss=3.45684 (lr=3.0e-03)\n",
      "[step 1034/accstep :  (1/1)]: loss=1.51506 (lr=3.0e-03)\n",
      "[step 1035/accstep :  (1/1)]: loss=1.65395 (lr=3.0e-03)\n",
      "[step 1036/accstep :  (1/1)]: loss=1.39609 (lr=3.0e-03)\n",
      "[step 1037/accstep :  (1/1)]: loss=2.24639 (lr=3.0e-03)\n",
      "[step 1038/accstep :  (1/1)]: loss=2.21692 (lr=3.0e-03)\n",
      "[step 1039/accstep :  (1/1)]: loss=1.98733 (lr=3.0e-03)\n",
      "[step 1040/accstep :  (1/1)]: loss=1.90355 (lr=3.0e-03)\n",
      "[step 1041/accstep :  (1/1)]: loss=2.68494 (lr=3.0e-03)\n",
      "[step 1042/accstep :  (1/1)]: loss=3.06734 (lr=3.0e-03)\n",
      "[step 1043/accstep :  (1/1)]: loss=1.84236 (lr=3.0e-03)\n",
      "[step 1044/accstep :  (1/1)]: loss=1.93337 (lr=3.0e-03)\n",
      "[step 1045/accstep :  (1/1)]: loss=4.27689 (lr=3.0e-03)\n",
      "[step 1046/accstep :  (1/1)]: loss=1.74739 (lr=3.0e-03)\n",
      "[step 1047/accstep :  (1/1)]: loss=1.94591 (lr=3.0e-03)\n",
      "[step 1048/accstep :  (1/1)]: loss=2.00890 (lr=3.0e-03)\n",
      "[step 1049/accstep :  (1/1)]: loss=2.36752 (lr=3.0e-03)\n",
      "[step 1050/accstep :  (1/1)]: loss=1.84597 (lr=3.0e-03)\n",
      "[step 1051/accstep :  (1/1)]: loss=1.70558 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1052/accstep :  (1/1)]: loss=1.37616 (lr=3.0e-03)\n",
      "[step 1053/accstep :  (1/1)]: loss=2.02758 (lr=3.0e-03)\n",
      "[step 1054/accstep :  (1/1)]: loss=1.87461 (lr=3.0e-03)\n",
      "[step 1055/accstep :  (1/1)]: loss=2.52253 (lr=3.0e-03)\n",
      "[step 1056/accstep :  (1/1)]: loss=1.62294 (lr=3.0e-03)\n",
      "[step 1057/accstep :  (1/1)]: loss=3.11374 (lr=3.0e-03)\n",
      "[step 1058/accstep :  (1/1)]: loss=1.86312 (lr=3.0e-03)\n",
      "[step 1059/accstep :  (1/1)]: loss=2.23010 (lr=3.0e-03)\n",
      "[step 1060/accstep :  (1/1)]: loss=0.80216 (lr=3.0e-03)\n",
      "[step 1061/accstep :  (1/1)]: loss=1.57412 (lr=3.0e-03)\n",
      "[step 1062/accstep :  (1/1)]: loss=3.84842 (lr=3.0e-03)\n",
      "[step 1063/accstep :  (1/1)]: loss=2.07588 (lr=3.0e-03)\n",
      "[step 1064/accstep :  (1/1)]: loss=2.34979 (lr=3.0e-03)\n",
      "[step 1065/accstep :  (1/1)]: loss=1.75390 (lr=3.0e-03)\n",
      "[step 1066/accstep :  (1/1)]: loss=2.71894 (lr=3.0e-03)\n",
      "[step 1067/accstep :  (1/1)]: loss=1.93121 (lr=3.0e-03)\n",
      "[step 1068/accstep :  (1/1)]: loss=2.39649 (lr=3.0e-03)\n",
      "[step 1069/accstep :  (1/1)]: loss=2.17597 (lr=3.0e-03)\n",
      "[step 1070/accstep :  (1/1)]: loss=2.03151 (lr=3.0e-03)\n",
      "[step 1071/accstep :  (1/1)]: loss=1.89068 (lr=3.0e-03)\n",
      "[step 1072/accstep :  (1/1)]: loss=1.54931 (lr=3.0e-03)\n",
      "[step 1073/accstep :  (1/1)]: loss=2.13364 (lr=3.0e-03)\n",
      "[step 1074/accstep :  (1/1)]: loss=0.94971 (lr=3.0e-03)\n",
      "[step 1075/accstep :  (1/1)]: loss=1.48712 (lr=3.0e-03)\n",
      "[step 1076/accstep :  (1/1)]: loss=1.39182 (lr=3.0e-03)\n",
      "[step 1077/accstep :  (1/1)]: loss=1.96806 (lr=3.0e-03)\n",
      "[step 1078/accstep :  (1/1)]: loss=2.46864 (lr=3.0e-03)\n",
      "[step 1079/accstep :  (1/1)]: loss=1.91211 (lr=3.0e-03)\n",
      "[step 1080/accstep :  (1/1)]: loss=1.55547 (lr=3.0e-03)\n",
      "[step 1081/accstep :  (1/1)]: loss=2.55172 (lr=3.0e-03)\n",
      "[step 1082/accstep :  (1/1)]: loss=2.01028 (lr=3.0e-03)\n",
      "[step 1083/accstep :  (1/1)]: loss=1.17719 (lr=3.0e-03)\n",
      "[step 1084/accstep :  (1/1)]: loss=1.47726 (lr=3.0e-03)\n",
      "[step 1085/accstep :  (1/1)]: loss=1.46855 (lr=3.0e-03)\n",
      "[step 1086/accstep :  (1/1)]: loss=1.11221 (lr=3.0e-03)\n",
      "[step 1087/accstep :  (1/1)]: loss=2.21490 (lr=3.0e-03)\n",
      "[step 1088/accstep :  (1/1)]: loss=1.58139 (lr=3.0e-03)\n",
      "[step 1089/accstep :  (1/1)]: loss=2.38217 (lr=3.0e-03)\n",
      "[step 1090/accstep :  (1/1)]: loss=2.32032 (lr=3.0e-03)\n",
      "[step 1091/accstep :  (1/1)]: loss=2.10064 (lr=3.0e-03)\n",
      "[step 1092/accstep :  (1/1)]: loss=1.33823 (lr=3.0e-03)\n",
      "[step 1093/accstep :  (1/1)]: loss=1.54916 (lr=3.0e-03)\n",
      "[step 1094/accstep :  (1/1)]: loss=1.47800 (lr=3.0e-03)\n",
      "[step 1095/accstep :  (1/1)]: loss=1.89563 (lr=3.0e-03)\n",
      "[step 1096/accstep :  (1/1)]: loss=2.11661 (lr=3.0e-03)\n",
      "[step 1097/accstep :  (1/1)]: loss=3.42696 (lr=3.0e-03)\n",
      "[step 1098/accstep :  (1/1)]: loss=4.23984 (lr=3.0e-03)\n",
      "[step 1099/accstep :  (1/1)]: loss=2.12358 (lr=3.0e-03)\n",
      "[step 1100/accstep :  (1/1)]: loss=2.14553 (lr=3.0e-03)\n",
      "[step 1101/accstep :  (1/1)]: loss=3.49896 (lr=3.0e-03)\n",
      "[step 1102/accstep :  (1/1)]: loss=2.09865 (lr=3.0e-03)\n",
      "[step 1103/accstep :  (1/1)]: loss=1.99750 (lr=3.0e-03)\n",
      "[step 1104/accstep :  (1/1)]: loss=1.96609 (lr=3.0e-03)\n",
      "[step 1105/accstep :  (1/1)]: loss=1.28074 (lr=3.0e-03)\n",
      "[step 1106/accstep :  (1/1)]: loss=1.98697 (lr=3.0e-03)\n",
      "[step 1107/accstep :  (1/1)]: loss=0.89134 (lr=3.0e-03)\n",
      "[step 1108/accstep :  (1/1)]: loss=1.44835 (lr=3.0e-03)\n",
      "[step 1109/accstep :  (1/1)]: loss=1.58548 (lr=3.0e-03)\n",
      "[step 1110/accstep :  (1/1)]: loss=2.44383 (lr=3.0e-03)\n",
      "[step 1111/accstep :  (1/1)]: loss=2.34333 (lr=3.0e-03)\n",
      "[step 1112/accstep :  (1/1)]: loss=1.90317 (lr=3.0e-03)\n",
      "[step 1113/accstep :  (1/1)]: loss=1.80319 (lr=3.0e-03)\n",
      "[step 1114/accstep :  (1/1)]: loss=3.07923 (lr=3.0e-03)\n",
      "[step 1115/accstep :  (1/1)]: loss=1.91590 (lr=3.0e-03)\n",
      "[step 1116/accstep :  (1/1)]: loss=3.93534 (lr=3.0e-03)\n",
      "[step 1117/accstep :  (1/1)]: loss=2.19828 (lr=3.0e-03)\n",
      "[step 1118/accstep :  (1/1)]: loss=3.67642 (lr=3.0e-03)\n",
      "[step 1119/accstep :  (1/1)]: loss=1.97268 (lr=3.0e-03)\n",
      "[step 1120/accstep :  (1/1)]: loss=1.63333 (lr=3.0e-03)\n",
      "[step 1121/accstep :  (1/1)]: loss=1.76356 (lr=3.0e-03)\n",
      "[step 1122/accstep :  (1/1)]: loss=1.93037 (lr=3.0e-03)\n",
      "[step 1123/accstep :  (1/1)]: loss=1.43310 (lr=3.0e-03)\n",
      "[step 1124/accstep :  (1/1)]: loss=2.04912 (lr=3.0e-03)\n",
      "[step 1125/accstep :  (1/1)]: loss=0.97447 (lr=3.0e-03)\n",
      "[step 1126/accstep :  (1/1)]: loss=2.14140 (lr=3.0e-03)\n",
      "[step 1127/accstep :  (1/1)]: loss=1.63124 (lr=3.0e-03)\n",
      "[step 1128/accstep :  (1/1)]: loss=2.31795 (lr=3.0e-03)\n",
      "[step 1129/accstep :  (1/1)]: loss=2.64798 (lr=3.0e-03)\n",
      "[step 1130/accstep :  (1/1)]: loss=2.48214 (lr=3.0e-03)\n",
      "[step 1131/accstep :  (1/1)]: loss=2.38151 (lr=3.0e-03)\n",
      "[step 1132/accstep :  (1/1)]: loss=2.07727 (lr=3.0e-03)\n",
      "[step 1133/accstep :  (1/1)]: loss=4.30831 (lr=3.0e-03)\n",
      "[step 1134/accstep :  (1/1)]: loss=1.96054 (lr=3.0e-03)\n",
      "[step 1135/accstep :  (1/1)]: loss=1.68281 (lr=3.0e-03)\n",
      "[step 1136/accstep :  (1/1)]: loss=4.02564 (lr=3.0e-03)\n",
      "[step 1137/accstep :  (1/1)]: loss=4.38892 (lr=3.0e-03)\n",
      "[step 1138/accstep :  (1/1)]: loss=2.58808 (lr=3.0e-03)\n",
      "[step 1139/accstep :  (1/1)]: loss=1.59737 (lr=3.0e-03)\n",
      "[step 1140/accstep :  (1/1)]: loss=1.42160 (lr=3.0e-03)\n",
      "[step 1141/accstep :  (1/1)]: loss=2.97989 (lr=3.0e-03)\n",
      "[step 1142/accstep :  (1/1)]: loss=1.52944 (lr=3.0e-03)\n",
      "[step 1143/accstep :  (1/1)]: loss=1.55084 (lr=3.0e-03)\n",
      "[step 1144/accstep :  (1/1)]: loss=1.54989 (lr=3.0e-03)\n",
      "[step 1145/accstep :  (1/1)]: loss=1.79049 (lr=3.0e-03)\n",
      "[step 1146/accstep :  (1/1)]: loss=1.65353 (lr=3.0e-03)\n",
      "[step 1147/accstep :  (1/1)]: loss=2.47643 (lr=3.0e-03)\n",
      "[step 1148/accstep :  (1/1)]: loss=1.99698 (lr=3.0e-03)\n",
      "[step 1149/accstep :  (1/1)]: loss=2.37083 (lr=3.0e-03)\n",
      "[step 1150/accstep :  (1/1)]: loss=1.49200 (lr=3.0e-03)\n",
      "[step 1151/accstep :  (1/1)]: loss=4.34406 (lr=3.0e-03)\n",
      "[step 1152/accstep :  (1/1)]: loss=1.38634 (lr=3.0e-03)\n",
      "[step 1153/accstep :  (1/1)]: loss=2.20910 (lr=3.0e-03)\n",
      "[step 1154/accstep :  (1/1)]: loss=2.01792 (lr=3.0e-03)\n",
      "[step 1155/accstep :  (1/1)]: loss=2.30663 (lr=3.0e-03)\n",
      "[step 1156/accstep :  (1/1)]: loss=1.43273 (lr=3.0e-03)\n",
      "[step 1157/accstep :  (1/1)]: loss=2.52080 (lr=3.0e-03)\n",
      "[step 1158/accstep :  (1/1)]: loss=1.96891 (lr=3.0e-03)\n",
      "[step 1159/accstep :  (1/1)]: loss=2.25811 (lr=3.0e-03)\n",
      "[step 1160/accstep :  (1/1)]: loss=2.32760 (lr=3.0e-03)\n",
      "[step 1161/accstep :  (1/1)]: loss=1.84004 (lr=3.0e-03)\n",
      "[step 1162/accstep :  (1/1)]: loss=3.02531 (lr=3.0e-03)\n",
      "[step 1163/accstep :  (1/1)]: loss=3.96660 (lr=3.0e-03)\n",
      "[step 1164/accstep :  (1/1)]: loss=2.06713 (lr=3.0e-03)\n",
      "[step 1165/accstep :  (1/1)]: loss=1.62718 (lr=3.0e-03)\n",
      "[step 1166/accstep :  (1/1)]: loss=1.94217 (lr=3.0e-03)\n",
      "[step 1167/accstep :  (1/1)]: loss=1.53014 (lr=3.0e-03)\n",
      "[step 1168/accstep :  (1/1)]: loss=2.46262 (lr=3.0e-03)\n",
      "[step 1169/accstep :  (1/1)]: loss=1.35001 (lr=3.0e-03)\n",
      "[step 1170/accstep :  (1/1)]: loss=1.68526 (lr=3.0e-03)\n",
      "[step 1171/accstep :  (1/1)]: loss=2.07385 (lr=3.0e-03)\n",
      "[step 1172/accstep :  (1/1)]: loss=2.14885 (lr=3.0e-03)\n",
      "[step 1173/accstep :  (1/1)]: loss=1.42942 (lr=3.0e-03)\n",
      "[step 1174/accstep :  (1/1)]: loss=1.48156 (lr=3.0e-03)\n",
      "[step 1175/accstep :  (1/1)]: loss=1.41190 (lr=3.0e-03)\n",
      "[step 1176/accstep :  (1/1)]: loss=3.22631 (lr=3.0e-03)\n",
      "[step 1177/accstep :  (1/1)]: loss=1.70886 (lr=3.0e-03)\n",
      "[step 1178/accstep :  (1/1)]: loss=1.73903 (lr=3.0e-03)\n",
      "[step 1179/accstep :  (1/1)]: loss=2.35396 (lr=3.0e-03)\n",
      "[step 1180/accstep :  (1/1)]: loss=1.47408 (lr=3.0e-03)\n",
      "[step 1181/accstep :  (1/1)]: loss=1.38368 (lr=3.0e-03)\n",
      "[step 1182/accstep :  (1/1)]: loss=2.48094 (lr=3.0e-03)\n",
      "[step 1183/accstep :  (1/1)]: loss=1.72641 (lr=3.0e-03)\n",
      "[step 1184/accstep :  (1/1)]: loss=1.80961 (lr=3.0e-03)\n",
      "[step 1185/accstep :  (1/1)]: loss=2.52978 (lr=3.0e-03)\n",
      "[step 1186/accstep :  (1/1)]: loss=1.89691 (lr=3.0e-03)\n",
      "[step 1187/accstep :  (1/1)]: loss=1.23980 (lr=3.0e-03)\n",
      "[step 1188/accstep :  (1/1)]: loss=1.36967 (lr=3.0e-03)\n",
      "[step 1189/accstep :  (1/1)]: loss=3.32130 (lr=3.0e-03)\n",
      "[step 1190/accstep :  (1/1)]: loss=1.87004 (lr=3.0e-03)\n",
      "[step 1191/accstep :  (1/1)]: loss=1.70142 (lr=3.0e-03)\n",
      "[step 1192/accstep :  (1/1)]: loss=1.72912 (lr=3.0e-03)\n",
      "[step 1193/accstep :  (1/1)]: loss=1.71611 (lr=3.0e-03)\n",
      "[step 1194/accstep :  (1/1)]: loss=1.72314 (lr=3.0e-03)\n",
      "[step 1195/accstep :  (1/1)]: loss=1.73285 (lr=3.0e-03)\n",
      "[step 1196/accstep :  (1/1)]: loss=1.53720 (lr=3.0e-03)\n",
      "[step 1197/accstep :  (1/1)]: loss=1.67713 (lr=3.0e-03)\n",
      "[step 1198/accstep :  (1/1)]: loss=1.55034 (lr=3.0e-03)\n",
      "[step 1199/accstep :  (1/1)]: loss=2.00002 (lr=3.0e-03)\n",
      "[step 1200/accstep :  (1/1)]: loss=1.74127 (lr=3.0e-03)\n",
      "[step 1201/accstep :  (1/1)]: loss=1.80260 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1202/accstep :  (1/1)]: loss=1.09559 (lr=3.0e-03)\n",
      "[step 1203/accstep :  (1/1)]: loss=3.08263 (lr=3.0e-03)\n",
      "[step 1204/accstep :  (1/1)]: loss=2.26099 (lr=3.0e-03)\n",
      "[step 1205/accstep :  (1/1)]: loss=2.41379 (lr=3.0e-03)\n",
      "[step 1206/accstep :  (1/1)]: loss=2.38563 (lr=3.0e-03)\n",
      "[step 1207/accstep :  (1/1)]: loss=0.99725 (lr=3.0e-03)\n",
      "[step 1208/accstep :  (1/1)]: loss=1.83176 (lr=3.0e-03)\n",
      "[step 1209/accstep :  (1/1)]: loss=1.32496 (lr=3.0e-03)\n",
      "[step 1210/accstep :  (1/1)]: loss=1.81284 (lr=3.0e-03)\n",
      "[step 1211/accstep :  (1/1)]: loss=2.11524 (lr=3.0e-03)\n",
      "[step 1212/accstep :  (1/1)]: loss=1.53822 (lr=3.0e-03)\n",
      "[step 1213/accstep :  (1/1)]: loss=2.56862 (lr=3.0e-03)\n",
      "[step 1214/accstep :  (1/1)]: loss=2.27720 (lr=3.0e-03)\n",
      "[step 1215/accstep :  (1/1)]: loss=4.01947 (lr=3.0e-03)\n",
      "[step 1216/accstep :  (1/1)]: loss=2.41846 (lr=3.0e-03)\n",
      "[step 1217/accstep :  (1/1)]: loss=1.26658 (lr=3.0e-03)\n",
      "[step 1218/accstep :  (1/1)]: loss=3.85648 (lr=3.0e-03)\n",
      "[step 1219/accstep :  (1/1)]: loss=1.83635 (lr=3.0e-03)\n",
      "[step 1220/accstep :  (1/1)]: loss=2.65231 (lr=3.0e-03)\n",
      "[step 1221/accstep :  (1/1)]: loss=3.43851 (lr=3.0e-03)\n",
      "[step 1222/accstep :  (1/1)]: loss=1.71246 (lr=3.0e-03)\n",
      "[step 1223/accstep :  (1/1)]: loss=1.81312 (lr=3.0e-03)\n",
      "[step 1224/accstep :  (1/1)]: loss=2.38716 (lr=3.0e-03)\n",
      "[step 1225/accstep :  (1/1)]: loss=1.83511 (lr=3.0e-03)\n",
      "[step 1226/accstep :  (1/1)]: loss=1.46997 (lr=3.0e-03)\n",
      "[step 1227/accstep :  (1/1)]: loss=3.65273 (lr=3.0e-03)\n",
      "[step 1228/accstep :  (1/1)]: loss=1.71885 (lr=3.0e-03)\n",
      "[step 1229/accstep :  (1/1)]: loss=2.71127 (lr=3.0e-03)\n",
      "[step 1230/accstep :  (1/1)]: loss=2.59549 (lr=3.0e-03)\n",
      "[step 1231/accstep :  (1/1)]: loss=1.51458 (lr=3.0e-03)\n",
      "[step 1232/accstep :  (1/1)]: loss=1.89550 (lr=3.0e-03)\n",
      "[step 1233/accstep :  (1/1)]: loss=1.85534 (lr=3.0e-03)\n",
      "[step 1234/accstep :  (1/1)]: loss=1.77560 (lr=3.0e-03)\n",
      "[step 1235/accstep :  (1/1)]: loss=1.69173 (lr=3.0e-03)\n",
      "[step 1236/accstep :  (1/1)]: loss=2.04201 (lr=3.0e-03)\n",
      "[step 1237/accstep :  (1/1)]: loss=3.37721 (lr=3.0e-03)\n",
      "[step 1238/accstep :  (1/1)]: loss=0.72935 (lr=3.0e-03)\n",
      "[step 1239/accstep :  (1/1)]: loss=1.70127 (lr=3.0e-03)\n",
      "[step 1240/accstep :  (1/1)]: loss=1.71255 (lr=3.0e-03)\n",
      "[step 1241/accstep :  (1/1)]: loss=3.54150 (lr=3.0e-03)\n",
      "[step 1242/accstep :  (1/1)]: loss=2.29312 (lr=3.0e-03)\n",
      "[step 1243/accstep :  (1/1)]: loss=2.35934 (lr=3.0e-03)\n",
      "[step 1244/accstep :  (1/1)]: loss=1.40513 (lr=3.0e-03)\n",
      "[step 1245/accstep :  (1/1)]: loss=2.40456 (lr=3.0e-03)\n",
      "[step 1246/accstep :  (1/1)]: loss=3.73723 (lr=3.0e-03)\n",
      "[step 1247/accstep :  (1/1)]: loss=1.45823 (lr=3.0e-03)\n",
      "[step 1248/accstep :  (1/1)]: loss=2.44166 (lr=3.0e-03)\n",
      "[step 1249/accstep :  (1/1)]: loss=2.21800 (lr=3.0e-03)\n",
      "[step 1250/accstep :  (1/1)]: loss=2.08141 (lr=3.0e-03)\n",
      "[step 1251/accstep :  (1/1)]: loss=1.63947 (lr=3.0e-03)\n",
      "[step 1252/accstep :  (1/1)]: loss=1.87527 (lr=3.0e-03)\n",
      "[step 1253/accstep :  (1/1)]: loss=1.80124 (lr=3.0e-03)\n",
      "[step 1254/accstep :  (1/1)]: loss=2.05792 (lr=3.0e-03)\n",
      "[step 1255/accstep :  (1/1)]: loss=1.53706 (lr=3.0e-03)\n",
      "[step 1256/accstep :  (1/1)]: loss=1.20997 (lr=3.0e-03)\n",
      "[step 1257/accstep :  (1/1)]: loss=2.37216 (lr=3.0e-03)\n",
      "[step 1258/accstep :  (1/1)]: loss=1.59398 (lr=3.0e-03)\n",
      "[step 1259/accstep :  (1/1)]: loss=2.34677 (lr=3.0e-03)\n",
      "[step 1260/accstep :  (1/1)]: loss=2.16365 (lr=3.0e-03)\n",
      "[step 1261/accstep :  (1/1)]: loss=1.74899 (lr=3.0e-03)\n",
      "[step 1262/accstep :  (1/1)]: loss=1.60613 (lr=3.0e-03)\n",
      "[step 1263/accstep :  (1/1)]: loss=1.92453 (lr=3.0e-03)\n",
      "[step 1264/accstep :  (1/1)]: loss=1.96962 (lr=3.0e-03)\n",
      "[step 1265/accstep :  (1/1)]: loss=1.78503 (lr=3.0e-03)\n",
      "[step 1266/accstep :  (1/1)]: loss=1.65482 (lr=3.0e-03)\n",
      "[step 1267/accstep :  (1/1)]: loss=1.34978 (lr=3.0e-03)\n",
      "[step 1268/accstep :  (1/1)]: loss=2.28261 (lr=3.0e-03)\n",
      "[step 1269/accstep :  (1/1)]: loss=3.41051 (lr=3.0e-03)\n",
      "[step 1270/accstep :  (1/1)]: loss=3.07338 (lr=3.0e-03)\n",
      "[step 1271/accstep :  (1/1)]: loss=1.66152 (lr=3.0e-03)\n",
      "[step 1272/accstep :  (1/1)]: loss=2.04392 (lr=3.0e-03)\n",
      "[step 1273/accstep :  (1/1)]: loss=1.52035 (lr=3.0e-03)\n",
      "[step 1274/accstep :  (1/1)]: loss=1.12650 (lr=3.0e-03)\n",
      "[step 1275/accstep :  (1/1)]: loss=1.27849 (lr=3.0e-03)\n",
      "[step 1276/accstep :  (1/1)]: loss=1.69096 (lr=3.0e-03)\n",
      "[step 1277/accstep :  (1/1)]: loss=1.58983 (lr=3.0e-03)\n",
      "[step 1278/accstep :  (1/1)]: loss=1.55690 (lr=3.0e-03)\n",
      "[step 1279/accstep :  (1/1)]: loss=2.89266 (lr=3.0e-03)\n",
      "[step 1280/accstep :  (1/1)]: loss=1.45299 (lr=3.0e-03)\n",
      "[step 1281/accstep :  (1/1)]: loss=1.76018 (lr=3.0e-03)\n",
      "[step 1282/accstep :  (1/1)]: loss=2.13152 (lr=3.0e-03)\n",
      "[step 1283/accstep :  (1/1)]: loss=0.94899 (lr=3.0e-03)\n",
      "[step 1284/accstep :  (1/1)]: loss=2.77231 (lr=3.0e-03)\n",
      "[step 1285/accstep :  (1/1)]: loss=3.05590 (lr=3.0e-03)\n",
      "[step 1286/accstep :  (1/1)]: loss=1.84023 (lr=3.0e-03)\n",
      "[step 1287/accstep :  (1/1)]: loss=3.23588 (lr=3.0e-03)\n",
      "[step 1288/accstep :  (1/1)]: loss=1.16032 (lr=3.0e-03)\n",
      "[step 1289/accstep :  (1/1)]: loss=1.30460 (lr=3.0e-03)\n",
      "[step 1290/accstep :  (1/1)]: loss=1.21512 (lr=3.0e-03)\n",
      "[step 1291/accstep :  (1/1)]: loss=4.58278 (lr=3.0e-03)\n",
      "[step 1292/accstep :  (1/1)]: loss=1.75270 (lr=3.0e-03)\n",
      "[step 1293/accstep :  (1/1)]: loss=2.24555 (lr=3.0e-03)\n",
      "[step 1294/accstep :  (1/1)]: loss=1.99779 (lr=3.0e-03)\n",
      "[step 1295/accstep :  (1/1)]: loss=1.83180 (lr=3.0e-03)\n",
      "[step 1296/accstep :  (1/1)]: loss=1.37114 (lr=3.0e-03)\n",
      "[step 1297/accstep :  (1/1)]: loss=2.18266 (lr=3.0e-03)\n",
      "[step 1298/accstep :  (1/1)]: loss=2.11191 (lr=3.0e-03)\n",
      "[step 1299/accstep :  (1/1)]: loss=2.15321 (lr=3.0e-03)\n",
      "[step 1300/accstep :  (1/1)]: loss=1.37567 (lr=3.0e-03)\n",
      "[step 1301/accstep :  (1/1)]: loss=2.21887 (lr=3.0e-03)\n",
      "[step 1302/accstep :  (1/1)]: loss=3.26702 (lr=3.0e-03)\n",
      "[step 1303/accstep :  (1/1)]: loss=1.38833 (lr=3.0e-03)\n",
      "[step 1304/accstep :  (1/1)]: loss=2.76059 (lr=3.0e-03)\n",
      "[step 1305/accstep :  (1/1)]: loss=1.80635 (lr=3.0e-03)\n",
      "[step 1306/accstep :  (1/1)]: loss=3.07406 (lr=3.0e-03)\n",
      "[step 1307/accstep :  (1/1)]: loss=1.56480 (lr=3.0e-03)\n",
      "[step 1308/accstep :  (1/1)]: loss=1.90332 (lr=3.0e-03)\n",
      "[step 1309/accstep :  (1/1)]: loss=1.38238 (lr=3.0e-03)\n",
      "[step 1310/accstep :  (1/1)]: loss=2.07239 (lr=3.0e-03)\n",
      "[step 1311/accstep :  (1/1)]: loss=2.36358 (lr=3.0e-03)\n",
      "[step 1312/accstep :  (1/1)]: loss=1.78938 (lr=3.0e-03)\n",
      "[step 1313/accstep :  (1/1)]: loss=1.30742 (lr=3.0e-03)\n",
      "[step 1314/accstep :  (1/1)]: loss=1.56380 (lr=3.0e-03)\n",
      "[step 1315/accstep :  (1/1)]: loss=1.37111 (lr=3.0e-03)\n",
      "[step 1316/accstep :  (1/1)]: loss=1.77621 (lr=3.0e-03)\n",
      "[step 1317/accstep :  (1/1)]: loss=3.66429 (lr=3.0e-03)\n",
      "[step 1318/accstep :  (1/1)]: loss=1.72190 (lr=3.0e-03)\n",
      "[step 1319/accstep :  (1/1)]: loss=1.74751 (lr=3.0e-03)\n",
      "[step 1320/accstep :  (1/1)]: loss=1.07277 (lr=3.0e-03)\n",
      "[step 1321/accstep :  (1/1)]: loss=1.69122 (lr=3.0e-03)\n",
      "[step 1322/accstep :  (1/1)]: loss=1.66389 (lr=3.0e-03)\n",
      "[step 1323/accstep :  (1/1)]: loss=1.75326 (lr=3.0e-03)\n",
      "[step 1324/accstep :  (1/1)]: loss=0.76181 (lr=3.0e-03)\n",
      "[step 1325/accstep :  (1/1)]: loss=1.26689 (lr=3.0e-03)\n",
      "[step 1326/accstep :  (1/1)]: loss=2.26014 (lr=3.0e-03)\n",
      "[step 1327/accstep :  (1/1)]: loss=2.25698 (lr=3.0e-03)\n",
      "[step 1328/accstep :  (1/1)]: loss=1.44136 (lr=3.0e-03)\n",
      "[step 1329/accstep :  (1/1)]: loss=2.01287 (lr=3.0e-03)\n",
      "[step 1330/accstep :  (1/1)]: loss=1.87201 (lr=3.0e-03)\n",
      "[step 1331/accstep :  (1/1)]: loss=1.70849 (lr=3.0e-03)\n",
      "[step 1332/accstep :  (1/1)]: loss=1.40815 (lr=3.0e-03)\n",
      "[step 1333/accstep :  (1/1)]: loss=2.35920 (lr=3.0e-03)\n",
      "[step 1334/accstep :  (1/1)]: loss=1.10268 (lr=3.0e-03)\n",
      "[step 1335/accstep :  (1/1)]: loss=1.70703 (lr=3.0e-03)\n",
      "[step 1336/accstep :  (1/1)]: loss=2.84728 (lr=3.0e-03)\n",
      "[step 1337/accstep :  (1/1)]: loss=2.25832 (lr=3.0e-03)\n",
      "[step 1338/accstep :  (1/1)]: loss=1.47352 (lr=3.0e-03)\n",
      "[step 1339/accstep :  (1/1)]: loss=2.31320 (lr=3.0e-03)\n",
      "[step 1340/accstep :  (1/1)]: loss=1.16422 (lr=3.0e-03)\n",
      "[step 1341/accstep :  (1/1)]: loss=0.99699 (lr=3.0e-03)\n",
      "[step 1342/accstep :  (1/1)]: loss=1.45388 (lr=3.0e-03)\n",
      "[step 1343/accstep :  (1/1)]: loss=1.46683 (lr=3.0e-03)\n",
      "[step 1344/accstep :  (1/1)]: loss=2.51409 (lr=3.0e-03)\n",
      "[step 1345/accstep :  (1/1)]: loss=1.69226 (lr=3.0e-03)\n",
      "[step 1346/accstep :  (1/1)]: loss=2.09441 (lr=3.0e-03)\n",
      "[step 1347/accstep :  (1/1)]: loss=1.22271 (lr=3.0e-03)\n",
      "[step 1348/accstep :  (1/1)]: loss=2.04704 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1349/accstep :  (1/1)]: loss=1.65755 (lr=3.0e-03)\n",
      "[step 1350/accstep :  (1/1)]: loss=2.31733 (lr=3.0e-03)\n",
      "[step 1351/accstep :  (1/1)]: loss=1.39356 (lr=3.0e-03)\n",
      "[step 1352/accstep :  (1/1)]: loss=1.79053 (lr=3.0e-03)\n",
      "[step 1353/accstep :  (1/1)]: loss=1.37409 (lr=3.0e-03)\n",
      "[step 1354/accstep :  (1/1)]: loss=1.33404 (lr=3.0e-03)\n",
      "[step 1355/accstep :  (1/1)]: loss=2.33832 (lr=3.0e-03)\n",
      "[step 1356/accstep :  (1/1)]: loss=1.59361 (lr=3.0e-03)\n",
      "[step 1357/accstep :  (1/1)]: loss=1.20304 (lr=3.0e-03)\n",
      "[step 1358/accstep :  (1/1)]: loss=1.91167 (lr=3.0e-03)\n",
      "[step 1359/accstep :  (1/1)]: loss=1.67642 (lr=3.0e-03)\n",
      "[step 1360/accstep :  (1/1)]: loss=1.44250 (lr=3.0e-03)\n",
      "[step 1361/accstep :  (1/1)]: loss=1.36555 (lr=3.0e-03)\n",
      "[step 1362/accstep :  (1/1)]: loss=1.47106 (lr=3.0e-03)\n",
      "[step 1363/accstep :  (1/1)]: loss=3.70873 (lr=3.0e-03)\n",
      "[step 1364/accstep :  (1/1)]: loss=3.88177 (lr=3.0e-03)\n",
      "[step 1365/accstep :  (1/1)]: loss=1.12371 (lr=3.0e-03)\n",
      "[step 1366/accstep :  (1/1)]: loss=1.38166 (lr=3.0e-03)\n",
      "[step 1367/accstep :  (1/1)]: loss=3.92359 (lr=3.0e-03)\n",
      "[step 1368/accstep :  (1/1)]: loss=2.10371 (lr=3.0e-03)\n",
      "[step 1369/accstep :  (1/1)]: loss=2.75043 (lr=3.0e-03)\n",
      "[step 1370/accstep :  (1/1)]: loss=1.05932 (lr=3.0e-03)\n",
      "[step 1371/accstep :  (1/1)]: loss=1.97063 (lr=3.0e-03)\n",
      "[step 1372/accstep :  (1/1)]: loss=1.29848 (lr=3.0e-03)\n",
      "[step 1373/accstep :  (1/1)]: loss=1.65785 (lr=3.0e-03)\n",
      "[step 1374/accstep :  (1/1)]: loss=1.38440 (lr=3.0e-03)\n",
      "[step 1375/accstep :  (1/1)]: loss=1.52190 (lr=3.0e-03)\n",
      "[step 1376/accstep :  (1/1)]: loss=1.17237 (lr=3.0e-03)\n",
      "[step 1377/accstep :  (1/1)]: loss=3.91882 (lr=3.0e-03)\n",
      "[step 1378/accstep :  (1/1)]: loss=2.13912 (lr=3.0e-03)\n",
      "[step 1379/accstep :  (1/1)]: loss=1.76690 (lr=3.0e-03)\n",
      "[step 1380/accstep :  (1/1)]: loss=2.62127 (lr=3.0e-03)\n",
      "[step 1381/accstep :  (1/1)]: loss=1.86369 (lr=3.0e-03)\n",
      "[step 1382/accstep :  (1/1)]: loss=1.28120 (lr=3.0e-03)\n",
      "[step 1383/accstep :  (1/1)]: loss=1.50198 (lr=3.0e-03)\n",
      "[step 1384/accstep :  (1/1)]: loss=1.66769 (lr=3.0e-03)\n",
      "[step 1385/accstep :  (1/1)]: loss=3.35638 (lr=3.0e-03)\n",
      "[step 1386/accstep :  (1/1)]: loss=1.22687 (lr=3.0e-03)\n",
      "[step 1387/accstep :  (1/1)]: loss=1.98060 (lr=3.0e-03)\n",
      "[step 1388/accstep :  (1/1)]: loss=1.48260 (lr=3.0e-03)\n",
      "[step 1389/accstep :  (1/1)]: loss=1.76798 (lr=3.0e-03)\n",
      "[step 1390/accstep :  (1/1)]: loss=1.56329 (lr=3.0e-03)\n",
      "[step 1391/accstep :  (1/1)]: loss=1.82454 (lr=3.0e-03)\n",
      "[step 1392/accstep :  (1/1)]: loss=0.83958 (lr=3.0e-03)\n",
      "[step 1393/accstep :  (1/1)]: loss=3.48557 (lr=3.0e-03)\n",
      "[step 1394/accstep :  (1/1)]: loss=1.04748 (lr=3.0e-03)\n",
      "[step 1395/accstep :  (1/1)]: loss=2.70738 (lr=3.0e-03)\n",
      "[step 1396/accstep :  (1/1)]: loss=3.43548 (lr=3.0e-03)\n",
      "[step 1397/accstep :  (1/1)]: loss=1.32001 (lr=3.0e-03)\n",
      "[step 1398/accstep :  (1/1)]: loss=2.83126 (lr=3.0e-03)\n",
      "[step 1399/accstep :  (1/1)]: loss=2.66851 (lr=3.0e-03)\n",
      "[step 1400/accstep :  (1/1)]: loss=1.37522 (lr=3.0e-03)\n",
      "[step 1401/accstep :  (1/1)]: loss=2.18298 (lr=3.0e-03)\n",
      "[step 1402/accstep :  (1/1)]: loss=1.62569 (lr=3.0e-03)\n",
      "[step 1403/accstep :  (1/1)]: loss=0.94742 (lr=3.0e-03)\n",
      "[step 1404/accstep :  (1/1)]: loss=2.18301 (lr=3.0e-03)\n",
      "[step 1405/accstep :  (1/1)]: loss=1.93300 (lr=3.0e-03)\n",
      "[step 1406/accstep :  (1/1)]: loss=1.62586 (lr=3.0e-03)\n",
      "[step 1407/accstep :  (1/1)]: loss=3.42676 (lr=3.0e-03)\n",
      "[step 1408/accstep :  (1/1)]: loss=1.13830 (lr=3.0e-03)\n",
      "[step 1409/accstep :  (1/1)]: loss=1.90567 (lr=3.0e-03)\n",
      "[step 1410/accstep :  (1/1)]: loss=1.65854 (lr=3.0e-03)\n",
      "[step 1411/accstep :  (1/1)]: loss=2.48436 (lr=3.0e-03)\n",
      "[step 1412/accstep :  (1/1)]: loss=2.83961 (lr=3.0e-03)\n",
      "[step 1413/accstep :  (1/1)]: loss=2.07115 (lr=3.0e-03)\n",
      "[step 1414/accstep :  (1/1)]: loss=1.56567 (lr=3.0e-03)\n",
      "[step 1415/accstep :  (1/1)]: loss=1.23108 (lr=3.0e-03)\n",
      "[step 1416/accstep :  (1/1)]: loss=0.50738 (lr=3.0e-03)\n",
      "[step 1417/accstep :  (1/1)]: loss=3.49416 (lr=3.0e-03)\n",
      "[step 1418/accstep :  (1/1)]: loss=1.64969 (lr=3.0e-03)\n",
      "[step 1419/accstep :  (1/1)]: loss=2.38225 (lr=3.0e-03)\n",
      "[step 1420/accstep :  (1/1)]: loss=1.13620 (lr=3.0e-03)\n",
      "[step 1421/accstep :  (1/1)]: loss=1.59180 (lr=3.0e-03)\n",
      "[step 1422/accstep :  (1/1)]: loss=2.62810 (lr=3.0e-03)\n",
      "[step 1423/accstep :  (1/1)]: loss=1.71996 (lr=3.0e-03)\n",
      "[step 1424/accstep :  (1/1)]: loss=1.16003 (lr=3.0e-03)\n",
      "[step 1425/accstep :  (1/1)]: loss=3.00262 (lr=3.0e-03)\n",
      "[step 1426/accstep :  (1/1)]: loss=1.40391 (lr=3.0e-03)\n",
      "[step 1427/accstep :  (1/1)]: loss=1.24398 (lr=3.0e-03)\n",
      "[step 1428/accstep :  (1/1)]: loss=1.22853 (lr=3.0e-03)\n",
      "[step 1429/accstep :  (1/1)]: loss=1.80789 (lr=3.0e-03)\n",
      "[step 1430/accstep :  (1/1)]: loss=1.43404 (lr=3.0e-03)\n",
      "[step 1431/accstep :  (1/1)]: loss=1.00742 (lr=3.0e-03)\n",
      "[step 1432/accstep :  (1/1)]: loss=1.90046 (lr=3.0e-03)\n",
      "[step 1433/accstep :  (1/1)]: loss=1.47746 (lr=3.0e-03)\n",
      "[step 1434/accstep :  (1/1)]: loss=1.50926 (lr=3.0e-03)\n",
      "[step 1435/accstep :  (1/1)]: loss=1.70589 (lr=3.0e-03)\n",
      "[step 1436/accstep :  (1/1)]: loss=1.46175 (lr=3.0e-03)\n",
      "[step 1437/accstep :  (1/1)]: loss=3.26871 (lr=3.0e-03)\n",
      "[step 1438/accstep :  (1/1)]: loss=1.23038 (lr=3.0e-03)\n",
      "[step 1439/accstep :  (1/1)]: loss=1.35136 (lr=3.0e-03)\n",
      "[step 1440/accstep :  (1/1)]: loss=3.22969 (lr=3.0e-03)\n",
      "[step 1441/accstep :  (1/1)]: loss=3.53198 (lr=3.0e-03)\n",
      "[step 1442/accstep :  (1/1)]: loss=0.91259 (lr=3.0e-03)\n",
      "[step 1443/accstep :  (1/1)]: loss=2.77824 (lr=3.0e-03)\n",
      "[step 1444/accstep :  (1/1)]: loss=1.17220 (lr=3.0e-03)\n",
      "[step 1445/accstep :  (1/1)]: loss=1.19396 (lr=3.0e-03)\n",
      "[step 1446/accstep :  (1/1)]: loss=1.94299 (lr=3.0e-03)\n",
      "[step 1447/accstep :  (1/1)]: loss=1.82428 (lr=3.0e-03)\n",
      "[step 1448/accstep :  (1/1)]: loss=1.67202 (lr=3.0e-03)\n",
      "[step 1449/accstep :  (1/1)]: loss=1.69857 (lr=3.0e-03)\n",
      "[step 1450/accstep :  (1/1)]: loss=0.94795 (lr=3.0e-03)\n",
      "[step 1451/accstep :  (1/1)]: loss=2.33848 (lr=3.0e-03)\n",
      "[step 1452/accstep :  (1/1)]: loss=3.64407 (lr=3.0e-03)\n",
      "[step 1453/accstep :  (1/1)]: loss=2.74510 (lr=3.0e-03)\n",
      "[step 1454/accstep :  (1/1)]: loss=2.06253 (lr=3.0e-03)\n",
      "[step 1455/accstep :  (1/1)]: loss=1.18785 (lr=3.0e-03)\n",
      "[step 1456/accstep :  (1/1)]: loss=3.06513 (lr=3.0e-03)\n",
      "[step 1457/accstep :  (1/1)]: loss=1.91877 (lr=3.0e-03)\n",
      "[step 1458/accstep :  (1/1)]: loss=1.35571 (lr=3.0e-03)\n",
      "[step 1459/accstep :  (1/1)]: loss=2.03683 (lr=3.0e-03)\n",
      "[step 1460/accstep :  (1/1)]: loss=1.66496 (lr=3.0e-03)\n",
      "[step 1461/accstep :  (1/1)]: loss=0.59540 (lr=3.0e-03)\n",
      "[step 1462/accstep :  (1/1)]: loss=1.31991 (lr=3.0e-03)\n",
      "[step 1463/accstep :  (1/1)]: loss=2.25494 (lr=3.0e-03)\n",
      "[step 1464/accstep :  (1/1)]: loss=3.83114 (lr=3.0e-03)\n",
      "[step 1465/accstep :  (1/1)]: loss=1.46745 (lr=3.0e-03)\n",
      "[step 1466/accstep :  (1/1)]: loss=1.17308 (lr=3.0e-03)\n",
      "[step 1467/accstep :  (1/1)]: loss=2.33712 (lr=3.0e-03)\n",
      "[step 1468/accstep :  (1/1)]: loss=2.17703 (lr=3.0e-03)\n",
      "[step 1469/accstep :  (1/1)]: loss=3.55983 (lr=3.0e-03)\n",
      "[step 1470/accstep :  (1/1)]: loss=1.91609 (lr=3.0e-03)\n",
      "[step 1471/accstep :  (1/1)]: loss=1.74064 (lr=3.0e-03)\n",
      "[step 1472/accstep :  (1/1)]: loss=1.78149 (lr=3.0e-03)\n",
      "[step 1473/accstep :  (1/1)]: loss=1.64557 (lr=3.0e-03)\n",
      "[step 1474/accstep :  (1/1)]: loss=1.49183 (lr=3.0e-03)\n",
      "[step 1475/accstep :  (1/1)]: loss=1.75989 (lr=3.0e-03)\n",
      "[step 1476/accstep :  (1/1)]: loss=2.13941 (lr=3.0e-03)\n",
      "[step 1477/accstep :  (1/1)]: loss=1.62395 (lr=3.0e-03)\n",
      "[step 1478/accstep :  (1/1)]: loss=1.71138 (lr=3.0e-03)\n",
      "[step 1479/accstep :  (1/1)]: loss=4.25371 (lr=3.0e-03)\n",
      "[step 1480/accstep :  (1/1)]: loss=0.99703 (lr=3.0e-03)\n",
      "[step 1481/accstep :  (1/1)]: loss=1.77597 (lr=3.0e-03)\n",
      "[step 1482/accstep :  (1/1)]: loss=3.82487 (lr=3.0e-03)\n",
      "[step 1483/accstep :  (1/1)]: loss=2.39016 (lr=3.0e-03)\n",
      "[step 1484/accstep :  (1/1)]: loss=1.65010 (lr=3.0e-03)\n",
      "[step 1485/accstep :  (1/1)]: loss=1.91623 (lr=3.0e-03)\n",
      "[step 1486/accstep :  (1/1)]: loss=2.75562 (lr=3.0e-03)\n",
      "[step 1487/accstep :  (1/1)]: loss=1.75522 (lr=3.0e-03)\n",
      "[step 1488/accstep :  (1/1)]: loss=1.88718 (lr=3.0e-03)\n",
      "[step 1489/accstep :  (1/1)]: loss=0.89656 (lr=3.0e-03)\n",
      "[step 1490/accstep :  (1/1)]: loss=1.17840 (lr=3.0e-03)\n",
      "[step 1491/accstep :  (1/1)]: loss=2.05584 (lr=3.0e-03)\n",
      "[step 1492/accstep :  (1/1)]: loss=1.13269 (lr=3.0e-03)\n",
      "[step 1493/accstep :  (1/1)]: loss=1.41821 (lr=3.0e-03)\n",
      "[step 1494/accstep :  (1/1)]: loss=1.68599 (lr=3.0e-03)\n",
      "[step 1495/accstep :  (1/1)]: loss=1.69536 (lr=3.0e-03)\n",
      "[step 1496/accstep :  (1/1)]: loss=0.85780 (lr=3.0e-03)\n",
      "[step 1497/accstep :  (1/1)]: loss=2.20129 (lr=3.0e-03)\n",
      "[step 1498/accstep :  (1/1)]: loss=1.48626 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1499/accstep :  (1/1)]: loss=1.11164 (lr=3.0e-03)\n",
      "[step 1500/accstep :  (1/1)]: loss=1.77868 (lr=3.0e-03)\n",
      "[step 1501/accstep :  (1/1)]: loss=2.29035 (lr=3.0e-03)\n",
      "[step 1502/accstep :  (1/1)]: loss=1.11137 (lr=3.0e-03)\n",
      "[step 1503/accstep :  (1/1)]: loss=1.74306 (lr=3.0e-03)\n",
      "[step 1504/accstep :  (1/1)]: loss=1.62403 (lr=3.0e-03)\n",
      "[step 1505/accstep :  (1/1)]: loss=1.71723 (lr=3.0e-03)\n",
      "[step 1506/accstep :  (1/1)]: loss=2.13056 (lr=3.0e-03)\n",
      "[step 1507/accstep :  (1/1)]: loss=1.63989 (lr=3.0e-03)\n",
      "[step 1508/accstep :  (1/1)]: loss=1.96282 (lr=3.0e-03)\n",
      "[step 1509/accstep :  (1/1)]: loss=1.72810 (lr=3.0e-03)\n",
      "[step 1510/accstep :  (1/1)]: loss=1.78724 (lr=3.0e-03)\n",
      "[step 1511/accstep :  (1/1)]: loss=2.31704 (lr=3.0e-03)\n",
      "[step 1512/accstep :  (1/1)]: loss=2.66922 (lr=3.0e-03)\n",
      "[step 1513/accstep :  (1/1)]: loss=1.70660 (lr=3.0e-03)\n",
      "[step 1514/accstep :  (1/1)]: loss=2.44375 (lr=3.0e-03)\n",
      "[step 1515/accstep :  (1/1)]: loss=0.87973 (lr=3.0e-03)\n",
      "[step 1516/accstep :  (1/1)]: loss=1.28471 (lr=3.0e-03)\n",
      "[step 1517/accstep :  (1/1)]: loss=2.54829 (lr=3.0e-03)\n",
      "[step 1518/accstep :  (1/1)]: loss=3.65879 (lr=3.0e-03)\n",
      "[step 1519/accstep :  (1/1)]: loss=2.00941 (lr=3.0e-03)\n",
      "[step 1520/accstep :  (1/1)]: loss=2.50633 (lr=3.0e-03)\n",
      "[step 1521/accstep :  (1/1)]: loss=1.00504 (lr=3.0e-03)\n",
      "[step 1522/accstep :  (1/1)]: loss=1.66980 (lr=3.0e-03)\n",
      "[step 1523/accstep :  (1/1)]: loss=2.33409 (lr=3.0e-03)\n",
      "[step 1524/accstep :  (1/1)]: loss=1.93459 (lr=3.0e-03)\n",
      "[step 1525/accstep :  (1/1)]: loss=1.47861 (lr=3.0e-03)\n",
      "[step 1526/accstep :  (1/1)]: loss=1.63729 (lr=3.0e-03)\n",
      "[step 1527/accstep :  (1/1)]: loss=1.34080 (lr=3.0e-03)\n",
      "[step 1528/accstep :  (1/1)]: loss=3.87306 (lr=3.0e-03)\n",
      "[step 1529/accstep :  (1/1)]: loss=3.70535 (lr=3.0e-03)\n",
      "[step 1530/accstep :  (1/1)]: loss=1.33877 (lr=3.0e-03)\n",
      "[step 1531/accstep :  (1/1)]: loss=1.30856 (lr=3.0e-03)\n",
      "[step 1532/accstep :  (1/1)]: loss=1.24973 (lr=3.0e-03)\n",
      "[step 1533/accstep :  (1/1)]: loss=1.94797 (lr=3.0e-03)\n",
      "[step 1534/accstep :  (1/1)]: loss=1.97499 (lr=3.0e-03)\n",
      "[step 1535/accstep :  (1/1)]: loss=1.68339 (lr=3.0e-03)\n",
      "[step 1536/accstep :  (1/1)]: loss=1.79401 (lr=3.0e-03)\n",
      "[step 1537/accstep :  (1/1)]: loss=1.69397 (lr=3.0e-03)\n",
      "[step 1538/accstep :  (1/1)]: loss=1.90497 (lr=3.0e-03)\n",
      "[step 1539/accstep :  (1/1)]: loss=2.38171 (lr=3.0e-03)\n",
      "[step 1540/accstep :  (1/1)]: loss=2.39062 (lr=3.0e-03)\n",
      "[step 1541/accstep :  (1/1)]: loss=2.62815 (lr=3.0e-03)\n",
      "[step 1542/accstep :  (1/1)]: loss=1.16988 (lr=3.0e-03)\n",
      "[step 1543/accstep :  (1/1)]: loss=1.64284 (lr=3.0e-03)\n",
      "[step 1544/accstep :  (1/1)]: loss=1.38563 (lr=3.0e-03)\n",
      "[step 1545/accstep :  (1/1)]: loss=1.70062 (lr=3.0e-03)\n",
      "[step 1546/accstep :  (1/1)]: loss=1.83188 (lr=3.0e-03)\n",
      "[step 1547/accstep :  (1/1)]: loss=1.54442 (lr=3.0e-03)\n",
      "[step 1548/accstep :  (1/1)]: loss=1.60394 (lr=3.0e-03)\n",
      "[step 1549/accstep :  (1/1)]: loss=1.55887 (lr=3.0e-03)\n",
      "[step 1550/accstep :  (1/1)]: loss=1.59737 (lr=3.0e-03)\n",
      "[step 1551/accstep :  (1/1)]: loss=1.29107 (lr=3.0e-03)\n",
      "[step 1552/accstep :  (1/1)]: loss=1.12824 (lr=3.0e-03)\n",
      "[step 1553/accstep :  (1/1)]: loss=1.64893 (lr=3.0e-03)\n",
      "[step 1554/accstep :  (1/1)]: loss=0.95209 (lr=3.0e-03)\n",
      "[step 1555/accstep :  (1/1)]: loss=1.28172 (lr=3.0e-03)\n",
      "[step 1556/accstep :  (1/1)]: loss=3.43582 (lr=3.0e-03)\n",
      "[step 1557/accstep :  (1/1)]: loss=2.98571 (lr=3.0e-03)\n",
      "[step 1558/accstep :  (1/1)]: loss=1.44522 (lr=3.0e-03)\n",
      "[step 1559/accstep :  (1/1)]: loss=1.12835 (lr=3.0e-03)\n",
      "[step 1560/accstep :  (1/1)]: loss=1.36876 (lr=3.0e-03)\n",
      "[step 1561/accstep :  (1/1)]: loss=1.17588 (lr=3.0e-03)\n",
      "[step 1562/accstep :  (1/1)]: loss=1.56500 (lr=3.0e-03)\n",
      "[step 1563/accstep :  (1/1)]: loss=1.62178 (lr=3.0e-03)\n",
      "[step 1564/accstep :  (1/1)]: loss=2.06196 (lr=3.0e-03)\n",
      "[step 1565/accstep :  (1/1)]: loss=1.81331 (lr=3.0e-03)\n",
      "[step 1566/accstep :  (1/1)]: loss=1.54970 (lr=3.0e-03)\n",
      "[step 1567/accstep :  (1/1)]: loss=0.85595 (lr=3.0e-03)\n",
      "[step 1568/accstep :  (1/1)]: loss=2.70106 (lr=3.0e-03)\n",
      "[step 1569/accstep :  (1/1)]: loss=1.65674 (lr=3.0e-03)\n",
      "[step 1570/accstep :  (1/1)]: loss=0.90854 (lr=3.0e-03)\n",
      "[step 1571/accstep :  (1/1)]: loss=1.01233 (lr=3.0e-03)\n",
      "[step 1572/accstep :  (1/1)]: loss=1.56286 (lr=3.0e-03)\n",
      "[step 1573/accstep :  (1/1)]: loss=1.88754 (lr=3.0e-03)\n",
      "[step 1574/accstep :  (1/1)]: loss=3.82463 (lr=3.0e-03)\n",
      "[step 1575/accstep :  (1/1)]: loss=2.20985 (lr=3.0e-03)\n",
      "[step 1576/accstep :  (1/1)]: loss=1.73902 (lr=3.0e-03)\n",
      "[step 1577/accstep :  (1/1)]: loss=4.03270 (lr=3.0e-03)\n",
      "[step 1578/accstep :  (1/1)]: loss=1.13049 (lr=3.0e-03)\n",
      "[step 1579/accstep :  (1/1)]: loss=1.63451 (lr=3.0e-03)\n",
      "[step 1580/accstep :  (1/1)]: loss=3.36760 (lr=3.0e-03)\n",
      "[step 1581/accstep :  (1/1)]: loss=1.57339 (lr=3.0e-03)\n",
      "[step 1582/accstep :  (1/1)]: loss=0.96686 (lr=3.0e-03)\n",
      "[step 1583/accstep :  (1/1)]: loss=1.36562 (lr=3.0e-03)\n",
      "[step 1584/accstep :  (1/1)]: loss=1.09827 (lr=3.0e-03)\n",
      "[step 1585/accstep :  (1/1)]: loss=2.00647 (lr=3.0e-03)\n",
      "[step 1586/accstep :  (1/1)]: loss=0.83633 (lr=3.0e-03)\n",
      "[step 1587/accstep :  (1/1)]: loss=1.36999 (lr=3.0e-03)\n",
      "[step 1588/accstep :  (1/1)]: loss=2.14692 (lr=3.0e-03)\n",
      "[step 1589/accstep :  (1/1)]: loss=2.15115 (lr=3.0e-03)\n",
      "[step 1590/accstep :  (1/1)]: loss=0.90347 (lr=3.0e-03)\n",
      "[step 1591/accstep :  (1/1)]: loss=1.19255 (lr=3.0e-03)\n",
      "[step 1592/accstep :  (1/1)]: loss=1.16315 (lr=3.0e-03)\n",
      "[step 1593/accstep :  (1/1)]: loss=1.68118 (lr=3.0e-03)\n",
      "[step 1594/accstep :  (1/1)]: loss=1.02597 (lr=3.0e-03)\n",
      "[step 1595/accstep :  (1/1)]: loss=2.15185 (lr=3.0e-03)\n",
      "[step 1596/accstep :  (1/1)]: loss=2.01620 (lr=3.0e-03)\n",
      "[step 1597/accstep :  (1/1)]: loss=2.49459 (lr=3.0e-03)\n",
      "[step 1598/accstep :  (1/1)]: loss=2.73028 (lr=3.0e-03)\n",
      "[step 1599/accstep :  (1/1)]: loss=2.19309 (lr=3.0e-03)\n",
      "[step 1600/accstep :  (1/1)]: loss=1.50700 (lr=3.0e-03)\n",
      "[step 1601/accstep :  (1/1)]: loss=2.32824 (lr=3.0e-03)\n",
      "[step 1602/accstep :  (1/1)]: loss=1.45456 (lr=3.0e-03)\n",
      "[step 1603/accstep :  (1/1)]: loss=2.78011 (lr=3.0e-03)\n",
      "[step 1604/accstep :  (1/1)]: loss=1.62298 (lr=3.0e-03)\n",
      "[step 1605/accstep :  (1/1)]: loss=1.33233 (lr=3.0e-03)\n",
      "[step 1606/accstep :  (1/1)]: loss=1.62025 (lr=3.0e-03)\n",
      "[step 1607/accstep :  (1/1)]: loss=1.24640 (lr=3.0e-03)\n",
      "[step 1608/accstep :  (1/1)]: loss=1.78209 (lr=3.0e-03)\n",
      "[step 1609/accstep :  (1/1)]: loss=1.95603 (lr=3.0e-03)\n",
      "[step 1610/accstep :  (1/1)]: loss=1.24887 (lr=3.0e-03)\n",
      "[step 1611/accstep :  (1/1)]: loss=0.74630 (lr=3.0e-03)\n",
      "[step 1612/accstep :  (1/1)]: loss=2.84342 (lr=3.0e-03)\n",
      "[step 1613/accstep :  (1/1)]: loss=2.04389 (lr=3.0e-03)\n",
      "[step 1614/accstep :  (1/1)]: loss=1.25626 (lr=3.0e-03)\n",
      "[step 1615/accstep :  (1/1)]: loss=3.69839 (lr=3.0e-03)\n",
      "[step 1616/accstep :  (1/1)]: loss=1.51107 (lr=3.0e-03)\n",
      "[step 1617/accstep :  (1/1)]: loss=1.65414 (lr=3.0e-03)\n",
      "[step 1618/accstep :  (1/1)]: loss=1.00609 (lr=3.0e-03)\n",
      "[step 1619/accstep :  (1/1)]: loss=2.48230 (lr=3.0e-03)\n",
      "[step 1620/accstep :  (1/1)]: loss=1.47132 (lr=3.0e-03)\n",
      "[step 1621/accstep :  (1/1)]: loss=1.55049 (lr=3.0e-03)\n",
      "[step 1622/accstep :  (1/1)]: loss=1.35650 (lr=3.0e-03)\n",
      "[step 1623/accstep :  (1/1)]: loss=1.05135 (lr=3.0e-03)\n",
      "[step 1624/accstep :  (1/1)]: loss=2.18107 (lr=3.0e-03)\n",
      "[step 1625/accstep :  (1/1)]: loss=2.42313 (lr=3.0e-03)\n",
      "[step 1626/accstep :  (1/1)]: loss=1.24326 (lr=3.0e-03)\n",
      "[step 1627/accstep :  (1/1)]: loss=1.47471 (lr=3.0e-03)\n",
      "[step 1628/accstep :  (1/1)]: loss=1.65422 (lr=3.0e-03)\n",
      "[step 1629/accstep :  (1/1)]: loss=1.39200 (lr=3.0e-03)\n",
      "[step 1630/accstep :  (1/1)]: loss=1.21042 (lr=3.0e-03)\n",
      "[step 1631/accstep :  (1/1)]: loss=1.57129 (lr=3.0e-03)\n",
      "[step 1632/accstep :  (1/1)]: loss=1.82964 (lr=3.0e-03)\n",
      "[step 1633/accstep :  (1/1)]: loss=2.38903 (lr=3.0e-03)\n",
      "[step 1634/accstep :  (1/1)]: loss=2.29821 (lr=3.0e-03)\n",
      "[step 1635/accstep :  (1/1)]: loss=1.58195 (lr=3.0e-03)\n",
      "[step 1636/accstep :  (1/1)]: loss=1.27444 (lr=3.0e-03)\n",
      "[step 1637/accstep :  (1/1)]: loss=1.19496 (lr=3.0e-03)\n",
      "[step 1638/accstep :  (1/1)]: loss=1.21725 (lr=3.0e-03)\n",
      "[step 1639/accstep :  (1/1)]: loss=1.00756 (lr=3.0e-03)\n",
      "[step 1640/accstep :  (1/1)]: loss=1.83219 (lr=3.0e-03)\n",
      "[step 1641/accstep :  (1/1)]: loss=1.08767 (lr=3.0e-03)\n",
      "[step 1642/accstep :  (1/1)]: loss=1.23157 (lr=3.0e-03)\n",
      "[step 1643/accstep :  (1/1)]: loss=1.18620 (lr=3.0e-03)\n",
      "[step 1644/accstep :  (1/1)]: loss=2.74284 (lr=3.0e-03)\n",
      "[step 1645/accstep :  (1/1)]: loss=2.33512 (lr=3.0e-03)\n",
      "[step 1646/accstep :  (1/1)]: loss=1.65743 (lr=3.0e-03)\n",
      "[step 1647/accstep :  (1/1)]: loss=2.04873 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1648/accstep :  (1/1)]: loss=1.63666 (lr=3.0e-03)\n",
      "[step 1649/accstep :  (1/1)]: loss=1.62118 (lr=3.0e-03)\n",
      "[step 1650/accstep :  (1/1)]: loss=3.50508 (lr=3.0e-03)\n",
      "[step 1651/accstep :  (1/1)]: loss=4.67949 (lr=3.0e-03)\n",
      "[step 1652/accstep :  (1/1)]: loss=1.63652 (lr=3.0e-03)\n",
      "[step 1653/accstep :  (1/1)]: loss=1.75745 (lr=3.0e-03)\n",
      "[step 1654/accstep :  (1/1)]: loss=1.75430 (lr=3.0e-03)\n",
      "[step 1655/accstep :  (1/1)]: loss=2.86231 (lr=3.0e-03)\n",
      "[step 1656/accstep :  (1/1)]: loss=2.38354 (lr=3.0e-03)\n",
      "[step 1657/accstep :  (1/1)]: loss=1.76648 (lr=3.0e-03)\n",
      "[step 1658/accstep :  (1/1)]: loss=2.00432 (lr=3.0e-03)\n",
      "[step 1659/accstep :  (1/1)]: loss=1.78566 (lr=3.0e-03)\n",
      "[step 1660/accstep :  (1/1)]: loss=2.30196 (lr=3.0e-03)\n",
      "[step 1661/accstep :  (1/1)]: loss=2.89350 (lr=3.0e-03)\n",
      "[step 1662/accstep :  (1/1)]: loss=1.59864 (lr=3.0e-03)\n",
      "[step 1663/accstep :  (1/1)]: loss=2.53849 (lr=3.0e-03)\n",
      "[step 1664/accstep :  (1/1)]: loss=1.76677 (lr=3.0e-03)\n",
      "[step 1665/accstep :  (1/1)]: loss=1.67889 (lr=3.0e-03)\n",
      "[step 1666/accstep :  (1/1)]: loss=1.63846 (lr=3.0e-03)\n",
      "[step 1667/accstep :  (1/1)]: loss=1.51304 (lr=3.0e-03)\n",
      "[step 1668/accstep :  (1/1)]: loss=1.56635 (lr=3.0e-03)\n",
      "[step 1669/accstep :  (1/1)]: loss=1.01975 (lr=3.0e-03)\n",
      "[step 1670/accstep :  (1/1)]: loss=1.09606 (lr=3.0e-03)\n",
      "[step 1671/accstep :  (1/1)]: loss=2.20998 (lr=3.0e-03)\n",
      "[step 1672/accstep :  (1/1)]: loss=1.63090 (lr=3.0e-03)\n",
      "[step 1673/accstep :  (1/1)]: loss=1.10136 (lr=3.0e-03)\n",
      "[step 1674/accstep :  (1/1)]: loss=1.64719 (lr=3.0e-03)\n",
      "[step 1675/accstep :  (1/1)]: loss=1.14525 (lr=3.0e-03)\n",
      "[step 1676/accstep :  (1/1)]: loss=1.19654 (lr=3.0e-03)\n",
      "[step 1677/accstep :  (1/1)]: loss=1.24516 (lr=3.0e-03)\n",
      "[step 1678/accstep :  (1/1)]: loss=2.13622 (lr=3.0e-03)\n",
      "[step 1679/accstep :  (1/1)]: loss=1.12034 (lr=3.0e-03)\n",
      "[step 1680/accstep :  (1/1)]: loss=1.49003 (lr=3.0e-03)\n",
      "[step 1681/accstep :  (1/1)]: loss=2.01905 (lr=3.0e-03)\n",
      "[step 1682/accstep :  (1/1)]: loss=3.27087 (lr=3.0e-03)\n",
      "[step 1683/accstep :  (1/1)]: loss=1.87940 (lr=3.0e-03)\n",
      "[step 1684/accstep :  (1/1)]: loss=4.45963 (lr=3.0e-03)\n",
      "[step 1685/accstep :  (1/1)]: loss=3.55735 (lr=3.0e-03)\n",
      "[step 1686/accstep :  (1/1)]: loss=1.02403 (lr=3.0e-03)\n",
      "[step 1687/accstep :  (1/1)]: loss=2.20424 (lr=3.0e-03)\n",
      "[step 1688/accstep :  (1/1)]: loss=0.80104 (lr=3.0e-03)\n",
      "[step 1689/accstep :  (1/1)]: loss=2.71404 (lr=3.0e-03)\n",
      "[step 1690/accstep :  (1/1)]: loss=1.96812 (lr=3.0e-03)\n",
      "[step 1691/accstep :  (1/1)]: loss=3.01329 (lr=3.0e-03)\n",
      "[step 1692/accstep :  (1/1)]: loss=1.18611 (lr=3.0e-03)\n",
      "[step 1693/accstep :  (1/1)]: loss=1.25535 (lr=3.0e-03)\n",
      "[step 1694/accstep :  (1/1)]: loss=1.52364 (lr=3.0e-03)\n",
      "[step 1695/accstep :  (1/1)]: loss=1.96642 (lr=3.0e-03)\n",
      "[step 1696/accstep :  (1/1)]: loss=2.65168 (lr=3.0e-03)\n",
      "[step 1697/accstep :  (1/1)]: loss=1.25698 (lr=3.0e-03)\n",
      "[step 1698/accstep :  (1/1)]: loss=3.65520 (lr=3.0e-03)\n",
      "[step 1699/accstep :  (1/1)]: loss=1.54778 (lr=3.0e-03)\n",
      "[step 1700/accstep :  (1/1)]: loss=1.52146 (lr=3.0e-03)\n",
      "[step 1701/accstep :  (1/1)]: loss=0.83430 (lr=3.0e-03)\n",
      "[step 1702/accstep :  (1/1)]: loss=1.78996 (lr=3.0e-03)\n",
      "[step 1703/accstep :  (1/1)]: loss=2.72173 (lr=3.0e-03)\n",
      "[step 1704/accstep :  (1/1)]: loss=0.91046 (lr=3.0e-03)\n",
      "[step 1705/accstep :  (1/1)]: loss=1.30332 (lr=3.0e-03)\n",
      "[step 1706/accstep :  (1/1)]: loss=2.05951 (lr=3.0e-03)\n",
      "[step 1707/accstep :  (1/1)]: loss=0.72531 (lr=3.0e-03)\n",
      "[step 1708/accstep :  (1/1)]: loss=1.35868 (lr=3.0e-03)\n",
      "[step 1709/accstep :  (1/1)]: loss=2.88745 (lr=3.0e-03)\n",
      "[step 1710/accstep :  (1/1)]: loss=3.14453 (lr=3.0e-03)\n",
      "[step 1711/accstep :  (1/1)]: loss=2.48695 (lr=3.0e-03)\n",
      "[step 1712/accstep :  (1/1)]: loss=4.05523 (lr=3.0e-03)\n",
      "[step 1713/accstep :  (1/1)]: loss=2.39882 (lr=3.0e-03)\n",
      "[step 1714/accstep :  (1/1)]: loss=1.29935 (lr=3.0e-03)\n",
      "[step 1715/accstep :  (1/1)]: loss=1.45242 (lr=3.0e-03)\n",
      "[step 1716/accstep :  (1/1)]: loss=1.86804 (lr=3.0e-03)\n",
      "[step 1717/accstep :  (1/1)]: loss=1.44247 (lr=3.0e-03)\n",
      "[step 1718/accstep :  (1/1)]: loss=3.90462 (lr=3.0e-03)\n",
      "[step 1719/accstep :  (1/1)]: loss=3.29280 (lr=3.0e-03)\n",
      "[step 1720/accstep :  (1/1)]: loss=1.68139 (lr=3.0e-03)\n",
      "[step 1721/accstep :  (1/1)]: loss=1.67304 (lr=3.0e-03)\n",
      "[step 1722/accstep :  (1/1)]: loss=1.39043 (lr=3.0e-03)\n",
      "[step 1723/accstep :  (1/1)]: loss=1.76510 (lr=3.0e-03)\n",
      "[step 1724/accstep :  (1/1)]: loss=1.64228 (lr=3.0e-03)\n",
      "[step 1725/accstep :  (1/1)]: loss=1.29298 (lr=3.0e-03)\n",
      "[step 1726/accstep :  (1/1)]: loss=1.53224 (lr=3.0e-03)\n",
      "[step 1727/accstep :  (1/1)]: loss=1.41831 (lr=3.0e-03)\n",
      "[step 1728/accstep :  (1/1)]: loss=1.57085 (lr=3.0e-03)\n",
      "[step 1729/accstep :  (1/1)]: loss=3.43272 (lr=3.0e-03)\n",
      "[step 1730/accstep :  (1/1)]: loss=1.26860 (lr=3.0e-03)\n",
      "[step 1731/accstep :  (1/1)]: loss=1.43960 (lr=3.0e-03)\n",
      "[step 1732/accstep :  (1/1)]: loss=1.15462 (lr=3.0e-03)\n",
      "[step 1733/accstep :  (1/1)]: loss=1.32425 (lr=3.0e-03)\n",
      "[step 1734/accstep :  (1/1)]: loss=1.30104 (lr=3.0e-03)\n",
      "[step 1735/accstep :  (1/1)]: loss=1.74168 (lr=3.0e-03)\n",
      "[step 1736/accstep :  (1/1)]: loss=0.91152 (lr=3.0e-03)\n",
      "[step 1737/accstep :  (1/1)]: loss=1.87095 (lr=3.0e-03)\n",
      "[step 1738/accstep :  (1/1)]: loss=1.48470 (lr=3.0e-03)\n",
      "[step 1739/accstep :  (1/1)]: loss=1.93645 (lr=3.0e-03)\n",
      "[step 1740/accstep :  (1/1)]: loss=3.73676 (lr=3.0e-03)\n",
      "[step 1741/accstep :  (1/1)]: loss=1.32363 (lr=3.0e-03)\n",
      "[step 1742/accstep :  (1/1)]: loss=3.07771 (lr=3.0e-03)\n",
      "[step 1743/accstep :  (1/1)]: loss=1.43102 (lr=3.0e-03)\n",
      "[step 1744/accstep :  (1/1)]: loss=0.92810 (lr=3.0e-03)\n",
      "[step 1745/accstep :  (1/1)]: loss=1.07953 (lr=3.0e-03)\n",
      "[step 1746/accstep :  (1/1)]: loss=1.55530 (lr=3.0e-03)\n",
      "[step 1747/accstep :  (1/1)]: loss=1.65979 (lr=3.0e-03)\n",
      "[step 1748/accstep :  (1/1)]: loss=1.83568 (lr=3.0e-03)\n",
      "[step 1749/accstep :  (1/1)]: loss=1.47288 (lr=3.0e-03)\n",
      "[step 1750/accstep :  (1/1)]: loss=3.82650 (lr=3.0e-03)\n",
      "[step 1751/accstep :  (1/1)]: loss=1.26385 (lr=3.0e-03)\n",
      "[step 1752/accstep :  (1/1)]: loss=3.04666 (lr=3.0e-03)\n",
      "[step 1753/accstep :  (1/1)]: loss=1.76423 (lr=3.0e-03)\n",
      "[step 1754/accstep :  (1/1)]: loss=1.57877 (lr=3.0e-03)\n",
      "[step 1755/accstep :  (1/1)]: loss=1.75405 (lr=3.0e-03)\n",
      "[step 1756/accstep :  (1/1)]: loss=1.44960 (lr=3.0e-03)\n",
      "[step 1757/accstep :  (1/1)]: loss=1.38676 (lr=3.0e-03)\n",
      "[step 1758/accstep :  (1/1)]: loss=1.21946 (lr=3.0e-03)\n",
      "[step 1759/accstep :  (1/1)]: loss=1.06260 (lr=3.0e-03)\n",
      "[step 1760/accstep :  (1/1)]: loss=1.61321 (lr=3.0e-03)\n",
      "[step 1761/accstep :  (1/1)]: loss=1.74278 (lr=3.0e-03)\n",
      "[step 1762/accstep :  (1/1)]: loss=1.16936 (lr=3.0e-03)\n",
      "[step 1763/accstep :  (1/1)]: loss=4.27217 (lr=3.0e-03)\n",
      "[step 1764/accstep :  (1/1)]: loss=2.50908 (lr=3.0e-03)\n",
      "[step 1765/accstep :  (1/1)]: loss=2.31901 (lr=3.0e-03)\n",
      "[step 1766/accstep :  (1/1)]: loss=1.15605 (lr=3.0e-03)\n",
      "[step 1767/accstep :  (1/1)]: loss=3.46384 (lr=3.0e-03)\n",
      "[step 1768/accstep :  (1/1)]: loss=3.60534 (lr=3.0e-03)\n",
      "[step 1769/accstep :  (1/1)]: loss=2.03474 (lr=3.0e-03)\n",
      "[step 1770/accstep :  (1/1)]: loss=1.97084 (lr=3.0e-03)\n",
      "[step 1771/accstep :  (1/1)]: loss=1.59614 (lr=3.0e-03)\n",
      "[step 1772/accstep :  (1/1)]: loss=1.09250 (lr=3.0e-03)\n",
      "[step 1773/accstep :  (1/1)]: loss=1.23896 (lr=3.0e-03)\n",
      "[step 1774/accstep :  (1/1)]: loss=1.36433 (lr=3.0e-03)\n",
      "[step 1775/accstep :  (1/1)]: loss=1.43765 (lr=3.0e-03)\n",
      "[step 1776/accstep :  (1/1)]: loss=1.53455 (lr=3.0e-03)\n",
      "[step 1777/accstep :  (1/1)]: loss=3.45970 (lr=3.0e-03)\n",
      "[step 1778/accstep :  (1/1)]: loss=0.98623 (lr=3.0e-03)\n",
      "[step 1779/accstep :  (1/1)]: loss=1.64811 (lr=3.0e-03)\n",
      "[step 1780/accstep :  (1/1)]: loss=1.13866 (lr=3.0e-03)\n",
      "[step 1781/accstep :  (1/1)]: loss=1.39251 (lr=3.0e-03)\n",
      "[step 1782/accstep :  (1/1)]: loss=3.03955 (lr=3.0e-03)\n",
      "[step 1783/accstep :  (1/1)]: loss=1.55590 (lr=3.0e-03)\n",
      "[step 1784/accstep :  (1/1)]: loss=3.17675 (lr=3.0e-03)\n",
      "[step 1785/accstep :  (1/1)]: loss=1.20178 (lr=3.0e-03)\n",
      "[step 1786/accstep :  (1/1)]: loss=1.52079 (lr=3.0e-03)\n",
      "[step 1787/accstep :  (1/1)]: loss=1.24274 (lr=3.0e-03)\n",
      "[step 1788/accstep :  (1/1)]: loss=0.83749 (lr=3.0e-03)\n",
      "[step 1789/accstep :  (1/1)]: loss=1.51532 (lr=3.0e-03)\n",
      "[step 1790/accstep :  (1/1)]: loss=2.90145 (lr=3.0e-03)\n",
      "[step 1791/accstep :  (1/1)]: loss=2.45855 (lr=3.0e-03)\n",
      "[step 1792/accstep :  (1/1)]: loss=1.53753 (lr=3.0e-03)\n",
      "[step 1793/accstep :  (1/1)]: loss=1.64390 (lr=3.0e-03)\n",
      "[step 1794/accstep :  (1/1)]: loss=1.36870 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1795/accstep :  (1/1)]: loss=1.28428 (lr=3.0e-03)\n",
      "[step 1796/accstep :  (1/1)]: loss=0.50469 (lr=3.0e-03)\n",
      "[step 1797/accstep :  (1/1)]: loss=1.18377 (lr=3.0e-03)\n",
      "[step 1798/accstep :  (1/1)]: loss=1.07570 (lr=3.0e-03)\n",
      "[step 1799/accstep :  (1/1)]: loss=1.34520 (lr=3.0e-03)\n",
      "[step 1800/accstep :  (1/1)]: loss=1.62287 (lr=3.0e-03)\n",
      "[step 1801/accstep :  (1/1)]: loss=1.41613 (lr=3.0e-03)\n",
      "[step 1802/accstep :  (1/1)]: loss=1.24278 (lr=3.0e-03)\n",
      "[step 1803/accstep :  (1/1)]: loss=2.07609 (lr=3.0e-03)\n",
      "[step 1804/accstep :  (1/1)]: loss=3.38634 (lr=3.0e-03)\n",
      "[step 1805/accstep :  (1/1)]: loss=0.78071 (lr=3.0e-03)\n",
      "[step 1806/accstep :  (1/1)]: loss=1.11626 (lr=3.0e-03)\n",
      "[step 1807/accstep :  (1/1)]: loss=1.22563 (lr=3.0e-03)\n",
      "[step 1808/accstep :  (1/1)]: loss=4.18168 (lr=3.0e-03)\n",
      "[step 1809/accstep :  (1/1)]: loss=1.58446 (lr=3.0e-03)\n",
      "[step 1810/accstep :  (1/1)]: loss=1.74646 (lr=3.0e-03)\n",
      "[step 1811/accstep :  (1/1)]: loss=1.50250 (lr=3.0e-03)\n",
      "[step 1812/accstep :  (1/1)]: loss=3.61119 (lr=3.0e-03)\n",
      "[step 1813/accstep :  (1/1)]: loss=1.58183 (lr=3.0e-03)\n",
      "[step 1814/accstep :  (1/1)]: loss=1.30104 (lr=3.0e-03)\n",
      "[step 1815/accstep :  (1/1)]: loss=0.78650 (lr=3.0e-03)\n",
      "[step 1816/accstep :  (1/1)]: loss=2.15482 (lr=3.0e-03)\n",
      "[step 1817/accstep :  (1/1)]: loss=1.13977 (lr=3.0e-03)\n",
      "[step 1818/accstep :  (1/1)]: loss=2.03555 (lr=3.0e-03)\n",
      "[step 1819/accstep :  (1/1)]: loss=2.00612 (lr=3.0e-03)\n",
      "[step 1820/accstep :  (1/1)]: loss=1.55246 (lr=3.0e-03)\n",
      "[step 1821/accstep :  (1/1)]: loss=2.63648 (lr=3.0e-03)\n",
      "[step 1822/accstep :  (1/1)]: loss=2.04278 (lr=3.0e-03)\n",
      "[step 1823/accstep :  (1/1)]: loss=1.44451 (lr=3.0e-03)\n",
      "[step 1824/accstep :  (1/1)]: loss=1.36343 (lr=3.0e-03)\n",
      "[step 1825/accstep :  (1/1)]: loss=3.04538 (lr=3.0e-03)\n",
      "[step 1826/accstep :  (1/1)]: loss=0.99903 (lr=3.0e-03)\n",
      "[step 1827/accstep :  (1/1)]: loss=2.01437 (lr=3.0e-03)\n",
      "[step 1828/accstep :  (1/1)]: loss=2.80856 (lr=3.0e-03)\n",
      "[step 1829/accstep :  (1/1)]: loss=2.13348 (lr=3.0e-03)\n",
      "[step 1830/accstep :  (1/1)]: loss=2.11027 (lr=3.0e-03)\n",
      "[step 1831/accstep :  (1/1)]: loss=1.16914 (lr=3.0e-03)\n",
      "[step 1832/accstep :  (1/1)]: loss=1.09603 (lr=3.0e-03)\n",
      "[step 1833/accstep :  (1/1)]: loss=1.90180 (lr=3.0e-03)\n",
      "[step 1834/accstep :  (1/1)]: loss=1.22634 (lr=3.0e-03)\n",
      "[step 1835/accstep :  (1/1)]: loss=1.73909 (lr=3.0e-03)\n",
      "[step 1836/accstep :  (1/1)]: loss=3.99535 (lr=3.0e-03)\n",
      "[step 1837/accstep :  (1/1)]: loss=1.44962 (lr=3.0e-03)\n",
      "[step 1838/accstep :  (1/1)]: loss=1.56384 (lr=3.0e-03)\n",
      "[step 1839/accstep :  (1/1)]: loss=1.35628 (lr=3.0e-03)\n",
      "[step 1840/accstep :  (1/1)]: loss=0.92913 (lr=3.0e-03)\n",
      "[step 1841/accstep :  (1/1)]: loss=1.78202 (lr=3.0e-03)\n",
      "[step 1842/accstep :  (1/1)]: loss=1.14052 (lr=3.0e-03)\n",
      "[step 1843/accstep :  (1/1)]: loss=3.13901 (lr=3.0e-03)\n",
      "[step 1844/accstep :  (1/1)]: loss=1.70619 (lr=3.0e-03)\n",
      "[step 1845/accstep :  (1/1)]: loss=1.66919 (lr=3.0e-03)\n",
      "[step 1846/accstep :  (1/1)]: loss=3.35456 (lr=3.0e-03)\n",
      "[step 1847/accstep :  (1/1)]: loss=2.00407 (lr=3.0e-03)\n",
      "[step 1848/accstep :  (1/1)]: loss=1.32034 (lr=3.0e-03)\n",
      "[step 1849/accstep :  (1/1)]: loss=1.56753 (lr=3.0e-03)\n",
      "[step 1850/accstep :  (1/1)]: loss=1.38409 (lr=3.0e-03)\n",
      "[step 1851/accstep :  (1/1)]: loss=1.75813 (lr=3.0e-03)\n",
      "[step 1852/accstep :  (1/1)]: loss=1.37707 (lr=3.0e-03)\n",
      "[step 1853/accstep :  (1/1)]: loss=0.87784 (lr=3.0e-03)\n",
      "[step 1854/accstep :  (1/1)]: loss=0.95818 (lr=3.0e-03)\n",
      "[step 1855/accstep :  (1/1)]: loss=0.98612 (lr=3.0e-03)\n",
      "[step 1856/accstep :  (1/1)]: loss=1.82202 (lr=3.0e-03)\n",
      "[step 1857/accstep :  (1/1)]: loss=1.46350 (lr=3.0e-03)\n",
      "[step 1858/accstep :  (1/1)]: loss=1.29193 (lr=3.0e-03)\n",
      "[step 1859/accstep :  (1/1)]: loss=2.53968 (lr=3.0e-03)\n",
      "[step 1860/accstep :  (1/1)]: loss=1.15489 (lr=3.0e-03)\n",
      "[step 1861/accstep :  (1/1)]: loss=1.56127 (lr=3.0e-03)\n",
      "[step 1862/accstep :  (1/1)]: loss=2.55747 (lr=3.0e-03)\n",
      "[step 1863/accstep :  (1/1)]: loss=2.39680 (lr=3.0e-03)\n",
      "[step 1864/accstep :  (1/1)]: loss=3.22168 (lr=3.0e-03)\n",
      "[step 1865/accstep :  (1/1)]: loss=1.39281 (lr=3.0e-03)\n",
      "[step 1866/accstep :  (1/1)]: loss=1.28664 (lr=3.0e-03)\n",
      "[step 1867/accstep :  (1/1)]: loss=2.24341 (lr=3.0e-03)\n",
      "[step 1868/accstep :  (1/1)]: loss=1.41159 (lr=3.0e-03)\n",
      "[step 1869/accstep :  (1/1)]: loss=3.83823 (lr=3.0e-03)\n",
      "[step 1870/accstep :  (1/1)]: loss=1.21012 (lr=3.0e-03)\n",
      "[step 1871/accstep :  (1/1)]: loss=1.86369 (lr=3.0e-03)\n",
      "[step 1872/accstep :  (1/1)]: loss=3.69846 (lr=3.0e-03)\n",
      "[step 1873/accstep :  (1/1)]: loss=1.52938 (lr=3.0e-03)\n",
      "[step 1874/accstep :  (1/1)]: loss=1.68523 (lr=3.0e-03)\n",
      "[step 1875/accstep :  (1/1)]: loss=2.30934 (lr=3.0e-03)\n",
      "[step 1876/accstep :  (1/1)]: loss=1.19722 (lr=3.0e-03)\n",
      "[step 1877/accstep :  (1/1)]: loss=1.15613 (lr=3.0e-03)\n",
      "[step 1878/accstep :  (1/1)]: loss=1.91663 (lr=3.0e-03)\n",
      "[step 1879/accstep :  (1/1)]: loss=1.49448 (lr=3.0e-03)\n",
      "[step 1880/accstep :  (1/1)]: loss=2.23916 (lr=3.0e-03)\n",
      "[step 1881/accstep :  (1/1)]: loss=1.82407 (lr=3.0e-03)\n",
      "[step 1882/accstep :  (1/1)]: loss=0.65567 (lr=3.0e-03)\n",
      "[step 1883/accstep :  (1/1)]: loss=1.52327 (lr=3.0e-03)\n",
      "[step 1884/accstep :  (1/1)]: loss=1.59064 (lr=3.0e-03)\n",
      "[step 1885/accstep :  (1/1)]: loss=1.57316 (lr=3.0e-03)\n",
      "[step 1886/accstep :  (1/1)]: loss=1.18927 (lr=3.0e-03)\n",
      "[step 1887/accstep :  (1/1)]: loss=1.43877 (lr=3.0e-03)\n",
      "[step 1888/accstep :  (1/1)]: loss=0.92619 (lr=3.0e-03)\n",
      "[step 1889/accstep :  (1/1)]: loss=1.92872 (lr=3.0e-03)\n",
      "[step 1890/accstep :  (1/1)]: loss=1.18171 (lr=3.0e-03)\n",
      "[step 1891/accstep :  (1/1)]: loss=0.65185 (lr=3.0e-03)\n",
      "[step 1892/accstep :  (1/1)]: loss=1.06705 (lr=3.0e-03)\n",
      "[step 1893/accstep :  (1/1)]: loss=0.91482 (lr=3.0e-03)\n",
      "[step 1894/accstep :  (1/1)]: loss=2.62031 (lr=3.0e-03)\n",
      "[step 1895/accstep :  (1/1)]: loss=3.24014 (lr=3.0e-03)\n",
      "[step 1896/accstep :  (1/1)]: loss=1.83838 (lr=3.0e-03)\n",
      "[step 1897/accstep :  (1/1)]: loss=1.77567 (lr=3.0e-03)\n",
      "[step 1898/accstep :  (1/1)]: loss=1.21033 (lr=3.0e-03)\n",
      "[step 1899/accstep :  (1/1)]: loss=2.32313 (lr=3.0e-03)\n",
      "[step 1900/accstep :  (1/1)]: loss=0.90774 (lr=3.0e-03)\n",
      "[step 1901/accstep :  (1/1)]: loss=1.93256 (lr=3.0e-03)\n",
      "[step 1902/accstep :  (1/1)]: loss=1.34438 (lr=3.0e-03)\n",
      "[step 1903/accstep :  (1/1)]: loss=1.46084 (lr=3.0e-03)\n",
      "[step 1904/accstep :  (1/1)]: loss=3.60371 (lr=3.0e-03)\n",
      "[step 1905/accstep :  (1/1)]: loss=1.39303 (lr=3.0e-03)\n",
      "[step 1906/accstep :  (1/1)]: loss=2.50880 (lr=3.0e-03)\n",
      "[step 1907/accstep :  (1/1)]: loss=0.68484 (lr=3.0e-03)\n",
      "[step 1908/accstep :  (1/1)]: loss=1.78469 (lr=3.0e-03)\n",
      "[step 1909/accstep :  (1/1)]: loss=1.93632 (lr=3.0e-03)\n",
      "[step 1910/accstep :  (1/1)]: loss=1.93944 (lr=3.0e-03)\n",
      "[step 1911/accstep :  (1/1)]: loss=1.55633 (lr=3.0e-03)\n",
      "[step 1912/accstep :  (1/1)]: loss=2.47556 (lr=3.0e-03)\n",
      "[step 1913/accstep :  (1/1)]: loss=0.90362 (lr=3.0e-03)\n",
      "[step 1914/accstep :  (1/1)]: loss=4.20434 (lr=3.0e-03)\n",
      "[step 1915/accstep :  (1/1)]: loss=2.03177 (lr=3.0e-03)\n",
      "[step 1916/accstep :  (1/1)]: loss=0.96345 (lr=3.0e-03)\n",
      "[step 1917/accstep :  (1/1)]: loss=0.70932 (lr=3.0e-03)\n",
      "[step 1918/accstep :  (1/1)]: loss=1.26529 (lr=3.0e-03)\n",
      "[step 1919/accstep :  (1/1)]: loss=0.80364 (lr=3.0e-03)\n",
      "[step 1920/accstep :  (1/1)]: loss=1.40145 (lr=3.0e-03)\n",
      "[step 1921/accstep :  (1/1)]: loss=1.83340 (lr=3.0e-03)\n",
      "[step 1922/accstep :  (1/1)]: loss=1.95309 (lr=3.0e-03)\n",
      "[step 1923/accstep :  (1/1)]: loss=0.81689 (lr=3.0e-03)\n",
      "[step 1924/accstep :  (1/1)]: loss=1.66540 (lr=3.0e-03)\n",
      "[step 1925/accstep :  (1/1)]: loss=2.80601 (lr=3.0e-03)\n",
      "[step 1926/accstep :  (1/1)]: loss=1.74170 (lr=3.0e-03)\n",
      "[step 1927/accstep :  (1/1)]: loss=1.33177 (lr=3.0e-03)\n",
      "[step 1928/accstep :  (1/1)]: loss=1.95567 (lr=3.0e-03)\n",
      "[step 1929/accstep :  (1/1)]: loss=1.03143 (lr=3.0e-03)\n",
      "[step 1930/accstep :  (1/1)]: loss=1.33594 (lr=3.0e-03)\n",
      "[step 1931/accstep :  (1/1)]: loss=1.59692 (lr=3.0e-03)\n",
      "[step 1932/accstep :  (1/1)]: loss=1.42371 (lr=3.0e-03)\n",
      "[step 1933/accstep :  (1/1)]: loss=1.21429 (lr=3.0e-03)\n",
      "[step 1934/accstep :  (1/1)]: loss=1.80597 (lr=3.0e-03)\n",
      "[step 1935/accstep :  (1/1)]: loss=2.27522 (lr=3.0e-03)\n",
      "[step 1936/accstep :  (1/1)]: loss=2.49229 (lr=3.0e-03)\n",
      "[step 1937/accstep :  (1/1)]: loss=3.87510 (lr=3.0e-03)\n",
      "[step 1938/accstep :  (1/1)]: loss=1.66097 (lr=3.0e-03)\n",
      "[step 1939/accstep :  (1/1)]: loss=1.31635 (lr=3.0e-03)\n",
      "[step 1940/accstep :  (1/1)]: loss=1.20022 (lr=3.0e-03)\n",
      "[step 1941/accstep :  (1/1)]: loss=0.82972 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1942/accstep :  (1/1)]: loss=1.77442 (lr=3.0e-03)\n",
      "[step 1943/accstep :  (1/1)]: loss=1.50272 (lr=3.0e-03)\n",
      "[step 1944/accstep :  (1/1)]: loss=3.40598 (lr=3.0e-03)\n",
      "[step 1945/accstep :  (1/1)]: loss=0.97586 (lr=3.0e-03)\n",
      "[step 1946/accstep :  (1/1)]: loss=1.90468 (lr=3.0e-03)\n",
      "[step 1947/accstep :  (1/1)]: loss=0.78273 (lr=3.0e-03)\n",
      "[step 1948/accstep :  (1/1)]: loss=1.24576 (lr=3.0e-03)\n",
      "[step 1949/accstep :  (1/1)]: loss=3.67530 (lr=3.0e-03)\n",
      "[step 1950/accstep :  (1/1)]: loss=1.26722 (lr=3.0e-03)\n",
      "[step 1951/accstep :  (1/1)]: loss=2.15248 (lr=3.0e-03)\n",
      "[step 1952/accstep :  (1/1)]: loss=1.19801 (lr=3.0e-03)\n",
      "[step 1953/accstep :  (1/1)]: loss=3.32256 (lr=3.0e-03)\n",
      "[step 1954/accstep :  (1/1)]: loss=2.93247 (lr=3.0e-03)\n",
      "[step 1955/accstep :  (1/1)]: loss=0.97913 (lr=3.0e-03)\n",
      "[step 1956/accstep :  (1/1)]: loss=0.73364 (lr=3.0e-03)\n",
      "[step 1957/accstep :  (1/1)]: loss=0.85020 (lr=3.0e-03)\n",
      "[step 1958/accstep :  (1/1)]: loss=1.80560 (lr=3.0e-03)\n",
      "[step 1959/accstep :  (1/1)]: loss=2.60596 (lr=3.0e-03)\n",
      "[step 1960/accstep :  (1/1)]: loss=4.14162 (lr=3.0e-03)\n",
      "[step 1961/accstep :  (1/1)]: loss=1.59665 (lr=3.0e-03)\n",
      "[step 1962/accstep :  (1/1)]: loss=2.86053 (lr=3.0e-03)\n",
      "[step 1963/accstep :  (1/1)]: loss=2.94539 (lr=3.0e-03)\n",
      "[step 1964/accstep :  (1/1)]: loss=1.39525 (lr=3.0e-03)\n",
      "[step 1965/accstep :  (1/1)]: loss=2.89108 (lr=3.0e-03)\n",
      "[step 1966/accstep :  (1/1)]: loss=1.85949 (lr=3.0e-03)\n",
      "[step 1967/accstep :  (1/1)]: loss=2.34238 (lr=3.0e-03)\n",
      "[step 1968/accstep :  (1/1)]: loss=3.34509 (lr=3.0e-03)\n",
      "[step 1969/accstep :  (1/1)]: loss=1.95130 (lr=3.0e-03)\n",
      "[step 1970/accstep :  (1/1)]: loss=1.54330 (lr=3.0e-03)\n",
      "[step 1971/accstep :  (1/1)]: loss=1.51838 (lr=3.0e-03)\n",
      "[step 1972/accstep :  (1/1)]: loss=2.20730 (lr=3.0e-03)\n",
      "[step 1973/accstep :  (1/1)]: loss=1.63889 (lr=3.0e-03)\n",
      "[step 1974/accstep :  (1/1)]: loss=1.32725 (lr=3.0e-03)\n",
      "[step 1975/accstep :  (1/1)]: loss=1.54205 (lr=3.0e-03)\n",
      "[step 1976/accstep :  (1/1)]: loss=1.84893 (lr=3.0e-03)\n",
      "[step 1977/accstep :  (1/1)]: loss=2.50053 (lr=3.0e-03)\n",
      "[step 1978/accstep :  (1/1)]: loss=2.04283 (lr=3.0e-03)\n",
      "[step 1979/accstep :  (1/1)]: loss=1.41144 (lr=3.0e-03)\n",
      "[step 1980/accstep :  (1/1)]: loss=1.35862 (lr=3.0e-03)\n",
      "[step 1981/accstep :  (1/1)]: loss=1.26669 (lr=3.0e-03)\n",
      "[step 1982/accstep :  (1/1)]: loss=1.09221 (lr=3.0e-03)\n",
      "[step 1983/accstep :  (1/1)]: loss=1.15712 (lr=3.0e-03)\n",
      "[step 1984/accstep :  (1/1)]: loss=1.37337 (lr=3.0e-03)\n",
      "[step 1985/accstep :  (1/1)]: loss=1.25649 (lr=3.0e-03)\n",
      "[step 1986/accstep :  (1/1)]: loss=2.31581 (lr=3.0e-03)\n",
      "[step 1987/accstep :  (1/1)]: loss=1.54783 (lr=3.0e-03)\n",
      "[step 1988/accstep :  (1/1)]: loss=1.64922 (lr=3.0e-03)\n",
      "[step 1989/accstep :  (1/1)]: loss=1.94162 (lr=3.0e-03)\n",
      "[step 1990/accstep :  (1/1)]: loss=0.57675 (lr=3.0e-03)\n",
      "[step 1991/accstep :  (1/1)]: loss=1.29994 (lr=3.0e-03)\n",
      "[step 1992/accstep :  (1/1)]: loss=1.69659 (lr=3.0e-03)\n",
      "[step 1993/accstep :  (1/1)]: loss=3.05421 (lr=3.0e-03)\n",
      "[step 1994/accstep :  (1/1)]: loss=1.98890 (lr=3.0e-03)\n",
      "[step 1995/accstep :  (1/1)]: loss=0.70244 (lr=3.0e-03)\n",
      "[step 1996/accstep :  (1/1)]: loss=2.14527 (lr=3.0e-03)\n",
      "[step 1997/accstep :  (1/1)]: loss=1.46083 (lr=3.0e-03)\n",
      "[step 1998/accstep :  (1/1)]: loss=1.30359 (lr=3.0e-03)\n",
      "[step 1999/accstep :  (1/1)]: loss=1.28119 (lr=3.0e-03)\n",
      "[step 2000/accstep :  (1/1)]: loss=0.99946 (lr=3.0e-03)\n",
      "[step 2001/accstep :  (1/1)]: loss=0.95793 (lr=3.0e-03)\n",
      "[step 2002/accstep :  (1/1)]: loss=2.16240 (lr=3.0e-03)\n",
      "[step 2003/accstep :  (1/1)]: loss=0.52966 (lr=3.0e-03)\n",
      "[step 2004/accstep :  (1/1)]: loss=1.23554 (lr=3.0e-03)\n",
      "[step 2005/accstep :  (1/1)]: loss=1.90781 (lr=3.0e-03)\n",
      "[step 2006/accstep :  (1/1)]: loss=1.76038 (lr=3.0e-03)\n",
      "[step 2007/accstep :  (1/1)]: loss=1.49403 (lr=3.0e-03)\n",
      "[step 2008/accstep :  (1/1)]: loss=1.22350 (lr=3.0e-03)\n",
      "[step 2009/accstep :  (1/1)]: loss=1.48747 (lr=3.0e-03)\n",
      "[step 2010/accstep :  (1/1)]: loss=1.45517 (lr=3.0e-03)\n",
      "[step 2011/accstep :  (1/1)]: loss=1.32464 (lr=3.0e-03)\n",
      "[step 2012/accstep :  (1/1)]: loss=1.36706 (lr=3.0e-03)\n",
      "[step 2013/accstep :  (1/1)]: loss=3.00848 (lr=3.0e-03)\n",
      "[step 2014/accstep :  (1/1)]: loss=0.98336 (lr=3.0e-03)\n",
      "[step 2015/accstep :  (1/1)]: loss=0.88061 (lr=3.0e-03)\n",
      "[step 2016/accstep :  (1/1)]: loss=0.57982 (lr=3.0e-03)\n",
      "[step 2017/accstep :  (1/1)]: loss=1.29018 (lr=3.0e-03)\n",
      "[step 2018/accstep :  (1/1)]: loss=1.03771 (lr=3.0e-03)\n",
      "[step 2019/accstep :  (1/1)]: loss=1.92789 (lr=3.0e-03)\n",
      "[step 2020/accstep :  (1/1)]: loss=2.18571 (lr=3.0e-03)\n",
      "[step 2021/accstep :  (1/1)]: loss=1.37877 (lr=3.0e-03)\n",
      "[step 2022/accstep :  (1/1)]: loss=2.64104 (lr=3.0e-03)\n",
      "[step 2023/accstep :  (1/1)]: loss=1.26853 (lr=3.0e-03)\n",
      "[step 2024/accstep :  (1/1)]: loss=0.87758 (lr=3.0e-03)\n",
      "[step 2025/accstep :  (1/1)]: loss=2.45216 (lr=3.0e-03)\n",
      "[step 2026/accstep :  (1/1)]: loss=3.92995 (lr=3.0e-03)\n",
      "[step 2027/accstep :  (1/1)]: loss=1.77308 (lr=3.0e-03)\n",
      "[step 2028/accstep :  (1/1)]: loss=1.18285 (lr=3.0e-03)\n",
      "[step 2029/accstep :  (1/1)]: loss=1.73786 (lr=3.0e-03)\n",
      "[step 2030/accstep :  (1/1)]: loss=2.63449 (lr=3.0e-03)\n",
      "[step 2031/accstep :  (1/1)]: loss=0.93740 (lr=3.0e-03)\n",
      "[step 2032/accstep :  (1/1)]: loss=1.17183 (lr=3.0e-03)\n",
      "[step 2033/accstep :  (1/1)]: loss=0.77399 (lr=3.0e-03)\n",
      "[step 2034/accstep :  (1/1)]: loss=1.16238 (lr=3.0e-03)\n",
      "[step 2035/accstep :  (1/1)]: loss=1.42933 (lr=3.0e-03)\n",
      "[step 2036/accstep :  (1/1)]: loss=1.36591 (lr=3.0e-03)\n",
      "[step 2037/accstep :  (1/1)]: loss=1.57903 (lr=3.0e-03)\n",
      "[step 2038/accstep :  (1/1)]: loss=1.62812 (lr=3.0e-03)\n",
      "[step 2039/accstep :  (1/1)]: loss=3.01504 (lr=3.0e-03)\n",
      "[step 2040/accstep :  (1/1)]: loss=1.32883 (lr=3.0e-03)\n",
      "[step 2041/accstep :  (1/1)]: loss=1.73183 (lr=3.0e-03)\n",
      "[step 2042/accstep :  (1/1)]: loss=1.38008 (lr=3.0e-03)\n",
      "[step 2043/accstep :  (1/1)]: loss=1.78177 (lr=3.0e-03)\n",
      "[step 2044/accstep :  (1/1)]: loss=0.97910 (lr=3.0e-03)\n",
      "[step 2045/accstep :  (1/1)]: loss=1.73968 (lr=3.0e-03)\n",
      "[step 2046/accstep :  (1/1)]: loss=1.09112 (lr=3.0e-03)\n",
      "[step 2047/accstep :  (1/1)]: loss=0.90379 (lr=3.0e-03)\n",
      "[step 2048/accstep :  (1/1)]: loss=1.09346 (lr=3.0e-03)\n",
      "[step 2049/accstep :  (1/1)]: loss=1.29488 (lr=3.0e-03)\n",
      "[step 2050/accstep :  (1/1)]: loss=1.51439 (lr=3.0e-03)\n",
      "[step 2051/accstep :  (1/1)]: loss=0.94582 (lr=3.0e-03)\n",
      "[step 2052/accstep :  (1/1)]: loss=1.66554 (lr=3.0e-03)\n",
      "[step 2053/accstep :  (1/1)]: loss=3.28306 (lr=3.0e-03)\n",
      "[step 2054/accstep :  (1/1)]: loss=1.05664 (lr=3.0e-03)\n",
      "[step 2055/accstep :  (1/1)]: loss=2.36222 (lr=3.0e-03)\n",
      "[step 2056/accstep :  (1/1)]: loss=1.20712 (lr=3.0e-03)\n",
      "[step 2057/accstep :  (1/1)]: loss=1.58489 (lr=3.0e-03)\n",
      "[step 2058/accstep :  (1/1)]: loss=1.16274 (lr=3.0e-03)\n",
      "[step 2059/accstep :  (1/1)]: loss=2.53434 (lr=3.0e-03)\n",
      "[step 2060/accstep :  (1/1)]: loss=1.99144 (lr=3.0e-03)\n",
      "[step 2061/accstep :  (1/1)]: loss=3.05858 (lr=3.0e-03)\n",
      "[step 2062/accstep :  (1/1)]: loss=0.68205 (lr=3.0e-03)\n",
      "[step 2063/accstep :  (1/1)]: loss=4.05038 (lr=3.0e-03)\n",
      "[step 2064/accstep :  (1/1)]: loss=1.55075 (lr=3.0e-03)\n",
      "[step 2065/accstep :  (1/1)]: loss=2.26397 (lr=3.0e-03)\n",
      "[step 2066/accstep :  (1/1)]: loss=1.99783 (lr=3.0e-03)\n",
      "[step 2067/accstep :  (1/1)]: loss=1.20782 (lr=3.0e-03)\n",
      "[step 2068/accstep :  (1/1)]: loss=1.15082 (lr=3.0e-03)\n",
      "[step 2069/accstep :  (1/1)]: loss=1.10892 (lr=3.0e-03)\n",
      "[step 2070/accstep :  (1/1)]: loss=1.68642 (lr=3.0e-03)\n",
      "[step 2071/accstep :  (1/1)]: loss=2.16260 (lr=3.0e-03)\n",
      "[step 2072/accstep :  (1/1)]: loss=1.66209 (lr=3.0e-03)\n",
      "[step 2073/accstep :  (1/1)]: loss=3.16122 (lr=3.0e-03)\n",
      "[step 2074/accstep :  (1/1)]: loss=1.86276 (lr=3.0e-03)\n",
      "[step 2075/accstep :  (1/1)]: loss=1.26050 (lr=3.0e-03)\n",
      "[step 2076/accstep :  (1/1)]: loss=1.08740 (lr=3.0e-03)\n",
      "[step 2077/accstep :  (1/1)]: loss=1.42931 (lr=3.0e-03)\n",
      "[step 2078/accstep :  (1/1)]: loss=3.20713 (lr=3.0e-03)\n",
      "[step 2079/accstep :  (1/1)]: loss=0.81096 (lr=3.0e-03)\n",
      "[step 2080/accstep :  (1/1)]: loss=2.34112 (lr=3.0e-03)\n",
      "[step 2081/accstep :  (1/1)]: loss=1.11654 (lr=3.0e-03)\n",
      "[step 2082/accstep :  (1/1)]: loss=0.82972 (lr=3.0e-03)\n",
      "[step 2083/accstep :  (1/1)]: loss=1.20266 (lr=3.0e-03)\n",
      "[step 2084/accstep :  (1/1)]: loss=1.64914 (lr=3.0e-03)\n",
      "[step 2085/accstep :  (1/1)]: loss=1.35846 (lr=3.0e-03)\n",
      "[step 2086/accstep :  (1/1)]: loss=2.59725 (lr=3.0e-03)\n",
      "[step 2087/accstep :  (1/1)]: loss=2.12281 (lr=3.0e-03)\n",
      "[step 2088/accstep :  (1/1)]: loss=0.92757 (lr=3.0e-03)\n",
      "[step 2089/accstep :  (1/1)]: loss=0.92191 (lr=3.0e-03)\n",
      "[step 2090/accstep :  (1/1)]: loss=3.18203 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 2091/accstep :  (1/1)]: loss=1.91169 (lr=3.0e-03)\n",
      "[step 2092/accstep :  (1/1)]: loss=3.48672 (lr=3.0e-03)\n",
      "[step 2093/accstep :  (1/1)]: loss=0.69597 (lr=3.0e-03)\n",
      "[step 2094/accstep :  (1/1)]: loss=4.12178 (lr=3.0e-03)\n",
      "[step 2095/accstep :  (1/1)]: loss=1.48143 (lr=3.0e-03)\n",
      "[step 2096/accstep :  (1/1)]: loss=3.48433 (lr=3.0e-03)\n",
      "[step 2097/accstep :  (1/1)]: loss=2.20089 (lr=3.0e-03)\n",
      "[step 2098/accstep :  (1/1)]: loss=1.59587 (lr=3.0e-03)\n",
      "[step 2099/accstep :  (1/1)]: loss=1.99224 (lr=3.0e-03)\n",
      "[step 2100/accstep :  (1/1)]: loss=2.14954 (lr=3.0e-03)\n",
      "[step 2101/accstep :  (1/1)]: loss=1.59435 (lr=3.0e-03)\n",
      "[step 2102/accstep :  (1/1)]: loss=2.45318 (lr=3.0e-03)\n",
      "[step 2103/accstep :  (1/1)]: loss=3.05461 (lr=3.0e-03)\n",
      "[step 2104/accstep :  (1/1)]: loss=3.26836 (lr=3.0e-03)\n",
      "[step 2105/accstep :  (1/1)]: loss=2.62210 (lr=3.0e-03)\n",
      "[step 2106/accstep :  (1/1)]: loss=1.84492 (lr=3.0e-03)\n",
      "[step 2107/accstep :  (1/1)]: loss=1.34767 (lr=3.0e-03)\n",
      "[step 2108/accstep :  (1/1)]: loss=1.30034 (lr=3.0e-03)\n",
      "[step 2109/accstep :  (1/1)]: loss=2.16417 (lr=3.0e-03)\n",
      "[step 2110/accstep :  (1/1)]: loss=1.02079 (lr=3.0e-03)\n",
      "[step 2111/accstep :  (1/1)]: loss=2.34636 (lr=3.0e-03)\n",
      "[step 2112/accstep :  (1/1)]: loss=1.24358 (lr=3.0e-03)\n",
      "[step 2113/accstep :  (1/1)]: loss=1.03213 (lr=3.0e-03)\n",
      "[step 2114/accstep :  (1/1)]: loss=1.45697 (lr=3.0e-03)\n",
      "[step 2115/accstep :  (1/1)]: loss=1.19124 (lr=3.0e-03)\n",
      "[step 2116/accstep :  (1/1)]: loss=1.01824 (lr=3.0e-03)\n",
      "[step 2117/accstep :  (1/1)]: loss=1.93286 (lr=3.0e-03)\n",
      "[step 2118/accstep :  (1/1)]: loss=0.82368 (lr=3.0e-03)\n",
      "[step 2119/accstep :  (1/1)]: loss=1.07454 (lr=3.0e-03)\n",
      "[step 2120/accstep :  (1/1)]: loss=1.21137 (lr=3.0e-03)\n",
      "[step 2121/accstep :  (1/1)]: loss=1.81278 (lr=3.0e-03)\n",
      "[step 2122/accstep :  (1/1)]: loss=1.56012 (lr=3.0e-03)\n",
      "[step 2123/accstep :  (1/1)]: loss=1.26866 (lr=3.0e-03)\n",
      "[step 2124/accstep :  (1/1)]: loss=3.93460 (lr=3.0e-03)\n",
      "[step 2125/accstep :  (1/1)]: loss=1.50617 (lr=3.0e-03)\n",
      "[step 2126/accstep :  (1/1)]: loss=1.26764 (lr=3.0e-03)\n",
      "[step 2127/accstep :  (1/1)]: loss=1.30617 (lr=3.0e-03)\n",
      "[step 2128/accstep :  (1/1)]: loss=0.64459 (lr=3.0e-03)\n",
      "[step 2129/accstep :  (1/1)]: loss=1.66375 (lr=3.0e-03)\n",
      "[step 2130/accstep :  (1/1)]: loss=1.01791 (lr=3.0e-03)\n",
      "[step 2131/accstep :  (1/1)]: loss=1.23582 (lr=3.0e-03)\n",
      "[step 2132/accstep :  (1/1)]: loss=2.02017 (lr=3.0e-03)\n",
      "[step 2133/accstep :  (1/1)]: loss=1.34534 (lr=3.0e-03)\n",
      "[step 2134/accstep :  (1/1)]: loss=1.04656 (lr=3.0e-03)\n",
      "[step 2135/accstep :  (1/1)]: loss=1.41088 (lr=3.0e-03)\n",
      "[step 2136/accstep :  (1/1)]: loss=1.51185 (lr=3.0e-03)\n",
      "[step 2137/accstep :  (1/1)]: loss=2.84854 (lr=3.0e-03)\n",
      "[step 2138/accstep :  (1/1)]: loss=3.67872 (lr=3.0e-03)\n",
      "[step 2139/accstep :  (1/1)]: loss=1.55041 (lr=3.0e-03)\n",
      "[step 2140/accstep :  (1/1)]: loss=1.33378 (lr=3.0e-03)\n",
      "[step 2141/accstep :  (1/1)]: loss=3.60238 (lr=3.0e-03)\n",
      "[step 2142/accstep :  (1/1)]: loss=1.93500 (lr=3.0e-03)\n",
      "[step 2143/accstep :  (1/1)]: loss=1.81038 (lr=3.0e-03)\n",
      "[step 2144/accstep :  (1/1)]: loss=1.52799 (lr=3.0e-03)\n",
      "[step 2145/accstep :  (1/1)]: loss=1.03059 (lr=3.0e-03)\n",
      "[step 2146/accstep :  (1/1)]: loss=1.11402 (lr=3.0e-03)\n",
      "[step 2147/accstep :  (1/1)]: loss=1.31911 (lr=3.0e-03)\n",
      "[step 2148/accstep :  (1/1)]: loss=2.66270 (lr=3.0e-03)\n",
      "[step 2149/accstep :  (1/1)]: loss=1.42063 (lr=3.0e-03)\n",
      "[step 2150/accstep :  (1/1)]: loss=1.00618 (lr=3.0e-03)\n",
      "[step 2151/accstep :  (1/1)]: loss=1.46218 (lr=3.0e-03)\n",
      "[step 2152/accstep :  (1/1)]: loss=1.39436 (lr=3.0e-03)\n",
      "[step 2153/accstep :  (1/1)]: loss=1.40323 (lr=3.0e-03)\n",
      "[step 2154/accstep :  (1/1)]: loss=2.56792 (lr=3.0e-03)\n",
      "[step 2155/accstep :  (1/1)]: loss=1.90931 (lr=3.0e-03)\n",
      "[step 2156/accstep :  (1/1)]: loss=1.10925 (lr=3.0e-03)\n",
      "[step 2157/accstep :  (1/1)]: loss=0.38326 (lr=3.0e-03)\n",
      "[step 2158/accstep :  (1/1)]: loss=1.48747 (lr=3.0e-03)\n",
      "[step 2159/accstep :  (1/1)]: loss=1.64031 (lr=3.0e-03)\n",
      "[step 2160/accstep :  (1/1)]: loss=1.99991 (lr=3.0e-03)\n",
      "[step 2161/accstep :  (1/1)]: loss=1.56813 (lr=3.0e-03)\n",
      "[step 2162/accstep :  (1/1)]: loss=1.53266 (lr=3.0e-03)\n",
      "[step 2163/accstep :  (1/1)]: loss=1.11065 (lr=3.0e-03)\n",
      "[step 2164/accstep :  (1/1)]: loss=0.92105 (lr=3.0e-03)\n",
      "[step 2165/accstep :  (1/1)]: loss=2.75890 (lr=3.0e-03)\n",
      "[step 2166/accstep :  (1/1)]: loss=3.40402 (lr=3.0e-03)\n",
      "[step 2167/accstep :  (1/1)]: loss=1.19025 (lr=3.0e-03)\n",
      "[step 2168/accstep :  (1/1)]: loss=3.15738 (lr=3.0e-03)\n",
      "[step 2169/accstep :  (1/1)]: loss=1.42933 (lr=3.0e-03)\n",
      "[step 2170/accstep :  (1/1)]: loss=1.10443 (lr=3.0e-03)\n",
      "[step 2171/accstep :  (1/1)]: loss=1.30508 (lr=3.0e-03)\n",
      "[step 2172/accstep :  (1/1)]: loss=0.36136 (lr=3.0e-03)\n",
      "[step 2173/accstep :  (1/1)]: loss=1.23411 (lr=3.0e-03)\n",
      "[step 2174/accstep :  (1/1)]: loss=1.44952 (lr=3.0e-03)\n",
      "[step 2175/accstep :  (1/1)]: loss=0.95704 (lr=3.0e-03)\n",
      "[step 2176/accstep :  (1/1)]: loss=2.60758 (lr=3.0e-03)\n",
      "[step 2177/accstep :  (1/1)]: loss=0.86419 (lr=3.0e-03)\n",
      "[step 2178/accstep :  (1/1)]: loss=0.70440 (lr=3.0e-03)\n",
      "[step 2179/accstep :  (1/1)]: loss=1.42769 (lr=3.0e-03)\n",
      "[step 2180/accstep :  (1/1)]: loss=1.11914 (lr=3.0e-03)\n",
      "[step 2181/accstep :  (1/1)]: loss=1.05350 (lr=3.0e-03)\n",
      "[step 2182/accstep :  (1/1)]: loss=1.11189 (lr=3.0e-03)\n",
      "[step 2183/accstep :  (1/1)]: loss=1.86687 (lr=3.0e-03)\n",
      "[step 2184/accstep :  (1/1)]: loss=2.39580 (lr=3.0e-03)\n",
      "[step 2185/accstep :  (1/1)]: loss=2.08656 (lr=3.0e-03)\n",
      "[step 2186/accstep :  (1/1)]: loss=1.83077 (lr=3.0e-03)\n",
      "[step 2187/accstep :  (1/1)]: loss=1.83405 (lr=3.0e-03)\n",
      "[step 2188/accstep :  (1/1)]: loss=1.41730 (lr=3.0e-03)\n",
      "[step 2189/accstep :  (1/1)]: loss=0.53438 (lr=3.0e-03)\n",
      "[step 2190/accstep :  (1/1)]: loss=1.41318 (lr=3.0e-03)\n",
      "[step 2191/accstep :  (1/1)]: loss=3.62722 (lr=3.0e-03)\n",
      "[step 2192/accstep :  (1/1)]: loss=1.43353 (lr=3.0e-03)\n",
      "[step 2193/accstep :  (1/1)]: loss=1.02310 (lr=3.0e-03)\n",
      "[step 2194/accstep :  (1/1)]: loss=1.59292 (lr=3.0e-03)\n",
      "[step 2195/accstep :  (1/1)]: loss=1.33286 (lr=3.0e-03)\n",
      "[step 2196/accstep :  (1/1)]: loss=0.94719 (lr=3.0e-03)\n",
      "[step 2197/accstep :  (1/1)]: loss=1.21134 (lr=3.0e-03)\n",
      "[step 2198/accstep :  (1/1)]: loss=1.40946 (lr=3.0e-03)\n",
      "[step 2199/accstep :  (1/1)]: loss=0.77328 (lr=3.0e-03)\n",
      "[step 2200/accstep :  (1/1)]: loss=1.82713 (lr=3.0e-03)\n",
      "[step 2201/accstep :  (1/1)]: loss=3.59167 (lr=3.0e-03)\n",
      "[step 2202/accstep :  (1/1)]: loss=0.72631 (lr=3.0e-03)\n",
      "[step 2203/accstep :  (1/1)]: loss=0.87216 (lr=3.0e-03)\n",
      "[step 2204/accstep :  (1/1)]: loss=0.63828 (lr=3.0e-03)\n",
      "[step 2205/accstep :  (1/1)]: loss=1.41269 (lr=3.0e-03)\n",
      "[step 2206/accstep :  (1/1)]: loss=1.43890 (lr=3.0e-03)\n",
      "[step 2207/accstep :  (1/1)]: loss=1.14503 (lr=3.0e-03)\n",
      "[step 2208/accstep :  (1/1)]: loss=1.82549 (lr=3.0e-03)\n",
      "[step 2209/accstep :  (1/1)]: loss=1.16301 (lr=3.0e-03)\n",
      "[step 2210/accstep :  (1/1)]: loss=0.85187 (lr=3.0e-03)\n",
      "[step 2211/accstep :  (1/1)]: loss=1.05059 (lr=3.0e-03)\n",
      "[step 2212/accstep :  (1/1)]: loss=1.93393 (lr=3.0e-03)\n",
      "[step 2213/accstep :  (1/1)]: loss=0.84232 (lr=3.0e-03)\n",
      "[step 2214/accstep :  (1/1)]: loss=1.21218 (lr=3.0e-03)\n",
      "[step 2215/accstep :  (1/1)]: loss=0.99911 (lr=3.0e-03)\n",
      "[step 2216/accstep :  (1/1)]: loss=1.66769 (lr=3.0e-03)\n",
      "[step 2217/accstep :  (1/1)]: loss=3.03981 (lr=3.0e-03)\n",
      "[step 2218/accstep :  (1/1)]: loss=0.62238 (lr=3.0e-03)\n",
      "[step 2219/accstep :  (1/1)]: loss=1.44966 (lr=3.0e-03)\n",
      "[step 2220/accstep :  (1/1)]: loss=1.15113 (lr=3.0e-03)\n",
      "[step 2221/accstep :  (1/1)]: loss=1.35526 (lr=3.0e-03)\n",
      "[step 2222/accstep :  (1/1)]: loss=2.79329 (lr=3.0e-03)\n",
      "[step 2223/accstep :  (1/1)]: loss=2.02460 (lr=3.0e-03)\n",
      "[step 2224/accstep :  (1/1)]: loss=1.26174 (lr=3.0e-03)\n",
      "[step 2225/accstep :  (1/1)]: loss=3.46273 (lr=3.0e-03)\n",
      "[step 2226/accstep :  (1/1)]: loss=1.10560 (lr=3.0e-03)\n",
      "[step 2227/accstep :  (1/1)]: loss=1.45477 (lr=3.0e-03)\n",
      "[step 2228/accstep :  (1/1)]: loss=1.56905 (lr=3.0e-03)\n",
      "[step 2229/accstep :  (1/1)]: loss=0.99343 (lr=3.0e-03)\n",
      "[step 2230/accstep :  (1/1)]: loss=2.64986 (lr=3.0e-03)\n",
      "[step 2231/accstep :  (1/1)]: loss=1.43440 (lr=3.0e-03)\n",
      "[step 2232/accstep :  (1/1)]: loss=1.06955 (lr=3.0e-03)\n",
      "[step 2233/accstep :  (1/1)]: loss=1.65682 (lr=3.0e-03)\n",
      "[step 2234/accstep :  (1/1)]: loss=1.28238 (lr=3.0e-03)\n",
      "[step 2235/accstep :  (1/1)]: loss=2.89353 (lr=3.0e-03)\n",
      "[step 2236/accstep :  (1/1)]: loss=1.28766 (lr=3.0e-03)\n",
      "[step 2237/accstep :  (1/1)]: loss=0.98141 (lr=3.0e-03)\n",
      "[step 2238/accstep :  (1/1)]: loss=1.67141 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 2239/accstep :  (1/1)]: loss=1.68349 (lr=3.0e-03)\n",
      "[step 2240/accstep :  (1/1)]: loss=1.57083 (lr=3.0e-03)\n",
      "[step 2241/accstep :  (1/1)]: loss=1.70781 (lr=3.0e-03)\n",
      "[step 2242/accstep :  (1/1)]: loss=1.65426 (lr=3.0e-03)\n",
      "[step 2243/accstep :  (1/1)]: loss=1.58610 (lr=3.0e-03)\n",
      "[step 2244/accstep :  (1/1)]: loss=3.05754 (lr=3.0e-03)\n",
      "[step 2245/accstep :  (1/1)]: loss=1.16120 (lr=3.0e-03)\n",
      "[step 2246/accstep :  (1/1)]: loss=1.36971 (lr=3.0e-03)\n",
      "[step 2247/accstep :  (1/1)]: loss=1.53077 (lr=3.0e-03)\n",
      "[step 2248/accstep :  (1/1)]: loss=1.48128 (lr=3.0e-03)\n",
      "[step 2249/accstep :  (1/1)]: loss=2.86999 (lr=3.0e-03)\n",
      "[step 2250/accstep :  (1/1)]: loss=1.56508 (lr=3.0e-03)\n",
      "[step 2251/accstep :  (1/1)]: loss=2.77701 (lr=3.0e-03)\n",
      "[step 2252/accstep :  (1/1)]: loss=3.42052 (lr=3.0e-03)\n",
      "[step 2253/accstep :  (1/1)]: loss=1.82771 (lr=3.0e-03)\n",
      "[step 2254/accstep :  (1/1)]: loss=1.33887 (lr=3.0e-03)\n",
      "[step 2255/accstep :  (1/1)]: loss=1.29788 (lr=3.0e-03)\n",
      "[step 2256/accstep :  (1/1)]: loss=1.65839 (lr=3.0e-03)\n",
      "[step 2257/accstep :  (1/1)]: loss=1.59246 (lr=3.0e-03)\n",
      "[step 2258/accstep :  (1/1)]: loss=1.35877 (lr=3.0e-03)\n",
      "[step 2259/accstep :  (1/1)]: loss=3.06527 (lr=3.0e-03)\n",
      "[step 2260/accstep :  (1/1)]: loss=1.63408 (lr=3.0e-03)\n",
      "[step 2261/accstep :  (1/1)]: loss=1.70102 (lr=3.0e-03)\n",
      "[step 2262/accstep :  (1/1)]: loss=0.94999 (lr=3.0e-03)\n",
      "[step 2263/accstep :  (1/1)]: loss=2.26481 (lr=3.0e-03)\n",
      "[step 2264/accstep :  (1/1)]: loss=2.06801 (lr=3.0e-03)\n",
      "[step 2265/accstep :  (1/1)]: loss=1.97463 (lr=3.0e-03)\n",
      "[step 2266/accstep :  (1/1)]: loss=0.89439 (lr=3.0e-03)\n",
      "[step 2267/accstep :  (1/1)]: loss=0.59043 (lr=3.0e-03)\n",
      "[step 2268/accstep :  (1/1)]: loss=1.46642 (lr=3.0e-03)\n",
      "[step 2269/accstep :  (1/1)]: loss=1.57693 (lr=3.0e-03)\n",
      "[step 2270/accstep :  (1/1)]: loss=0.99292 (lr=3.0e-03)\n",
      "[step 2271/accstep :  (1/1)]: loss=1.41553 (lr=3.0e-03)\n",
      "[step 2272/accstep :  (1/1)]: loss=1.58318 (lr=3.0e-03)\n",
      "[step 2273/accstep :  (1/1)]: loss=0.63386 (lr=3.0e-03)\n",
      "[step 2274/accstep :  (1/1)]: loss=1.63463 (lr=3.0e-03)\n",
      "[step 2275/accstep :  (1/1)]: loss=1.25066 (lr=3.0e-03)\n",
      "[step 2276/accstep :  (1/1)]: loss=1.35841 (lr=3.0e-03)\n",
      "[step 2277/accstep :  (1/1)]: loss=0.86623 (lr=3.0e-03)\n",
      "[step 2278/accstep :  (1/1)]: loss=1.70100 (lr=3.0e-03)\n",
      "[step 2279/accstep :  (1/1)]: loss=3.38404 (lr=3.0e-03)\n",
      "[step 2280/accstep :  (1/1)]: loss=3.93261 (lr=3.0e-03)\n",
      "[step 2281/accstep :  (1/1)]: loss=1.02897 (lr=3.0e-03)\n",
      "[step 2282/accstep :  (1/1)]: loss=1.07526 (lr=3.0e-03)\n",
      "[step 2283/accstep :  (1/1)]: loss=1.02826 (lr=3.0e-03)\n",
      "[step 2284/accstep :  (1/1)]: loss=1.03657 (lr=3.0e-03)\n",
      "[step 2285/accstep :  (1/1)]: loss=1.68195 (lr=3.0e-03)\n",
      "[step 2286/accstep :  (1/1)]: loss=1.72581 (lr=3.0e-03)\n",
      "[step 2287/accstep :  (1/1)]: loss=4.04531 (lr=3.0e-03)\n",
      "[step 2288/accstep :  (1/1)]: loss=3.43874 (lr=3.0e-03)\n",
      "[step 2289/accstep :  (1/1)]: loss=1.21053 (lr=3.0e-03)\n",
      "[step 2290/accstep :  (1/1)]: loss=1.37555 (lr=3.0e-03)\n",
      "[step 2291/accstep :  (1/1)]: loss=0.91178 (lr=3.0e-03)\n",
      "[step 2292/accstep :  (1/1)]: loss=1.21443 (lr=3.0e-03)\n",
      "[step 2293/accstep :  (1/1)]: loss=1.51028 (lr=3.0e-03)\n",
      "[step 2294/accstep :  (1/1)]: loss=2.30766 (lr=3.0e-03)\n",
      "[step 2295/accstep :  (1/1)]: loss=2.83359 (lr=3.0e-03)\n",
      "[step 2296/accstep :  (1/1)]: loss=1.40819 (lr=3.0e-03)\n",
      "[step 2297/accstep :  (1/1)]: loss=1.09525 (lr=3.0e-03)\n",
      "[step 2298/accstep :  (1/1)]: loss=0.93919 (lr=3.0e-03)\n",
      "[step 2299/accstep :  (1/1)]: loss=2.98490 (lr=3.0e-03)\n",
      "[step 2300/accstep :  (1/1)]: loss=3.82014 (lr=3.0e-03)\n",
      "[step 2301/accstep :  (1/1)]: loss=0.86989 (lr=3.0e-03)\n",
      "[step 2302/accstep :  (1/1)]: loss=1.89319 (lr=3.0e-03)\n",
      "[step 2303/accstep :  (1/1)]: loss=1.41868 (lr=3.0e-03)\n",
      "[step 2304/accstep :  (1/1)]: loss=1.21340 (lr=3.0e-03)\n",
      "[step 2305/accstep :  (1/1)]: loss=1.18168 (lr=3.0e-03)\n",
      "[step 2306/accstep :  (1/1)]: loss=2.05803 (lr=3.0e-03)\n",
      "[step 2307/accstep :  (1/1)]: loss=1.59601 (lr=3.0e-03)\n",
      "[step 2308/accstep :  (1/1)]: loss=0.96169 (lr=3.0e-03)\n",
      "[step 2309/accstep :  (1/1)]: loss=1.54431 (lr=3.0e-03)\n",
      "[step 2310/accstep :  (1/1)]: loss=0.89992 (lr=3.0e-03)\n",
      "[step 2311/accstep :  (1/1)]: loss=0.76382 (lr=3.0e-03)\n",
      "[step 2312/accstep :  (1/1)]: loss=1.87149 (lr=3.0e-03)\n",
      "[step 2313/accstep :  (1/1)]: loss=0.85759 (lr=3.0e-03)\n",
      "[step 2314/accstep :  (1/1)]: loss=1.06606 (lr=3.0e-03)\n",
      "[step 2315/accstep :  (1/1)]: loss=1.65998 (lr=3.0e-03)\n",
      "[step 2316/accstep :  (1/1)]: loss=1.04368 (lr=3.0e-03)\n",
      "[step 2317/accstep :  (1/1)]: loss=1.33258 (lr=3.0e-03)\n",
      "[step 2318/accstep :  (1/1)]: loss=1.56250 (lr=3.0e-03)\n",
      "[step 2319/accstep :  (1/1)]: loss=1.90945 (lr=3.0e-03)\n",
      "[step 2320/accstep :  (1/1)]: loss=1.47880 (lr=3.0e-03)\n",
      "[step 2321/accstep :  (1/1)]: loss=3.75844 (lr=3.0e-03)\n",
      "[step 2322/accstep :  (1/1)]: loss=1.54037 (lr=3.0e-03)\n",
      "[step 2323/accstep :  (1/1)]: loss=1.26673 (lr=3.0e-03)\n",
      "[step 2324/accstep :  (1/1)]: loss=1.13722 (lr=3.0e-03)\n",
      "[step 2325/accstep :  (1/1)]: loss=2.14585 (lr=3.0e-03)\n",
      "[step 2326/accstep :  (1/1)]: loss=1.29499 (lr=3.0e-03)\n",
      "[step 2327/accstep :  (1/1)]: loss=1.80861 (lr=3.0e-03)\n",
      "[step 2328/accstep :  (1/1)]: loss=1.08348 (lr=3.0e-03)\n",
      "[step 2329/accstep :  (1/1)]: loss=1.35482 (lr=3.0e-03)\n",
      "[step 2330/accstep :  (1/1)]: loss=1.80822 (lr=3.0e-03)\n",
      "[step 2331/accstep :  (1/1)]: loss=1.21324 (lr=3.0e-03)\n",
      "[step 2332/accstep :  (1/1)]: loss=4.15077 (lr=3.0e-03)\n",
      "[step 2333/accstep :  (1/1)]: loss=1.03523 (lr=3.0e-03)\n",
      "[step 2334/accstep :  (1/1)]: loss=1.28139 (lr=3.0e-03)\n",
      "[step 2335/accstep :  (1/1)]: loss=4.09576 (lr=3.0e-03)\n",
      "[step 2336/accstep :  (1/1)]: loss=1.15651 (lr=3.0e-03)\n",
      "[step 2337/accstep :  (1/1)]: loss=0.90483 (lr=3.0e-03)\n",
      "[step 2338/accstep :  (1/1)]: loss=2.73559 (lr=3.0e-03)\n",
      "[step 2339/accstep :  (1/1)]: loss=1.25926 (lr=3.0e-03)\n",
      "[step 2340/accstep :  (1/1)]: loss=2.17346 (lr=3.0e-03)\n",
      "[step 2341/accstep :  (1/1)]: loss=1.18952 (lr=3.0e-03)\n",
      "[step 2342/accstep :  (1/1)]: loss=1.51065 (lr=3.0e-03)\n",
      "[step 2343/accstep :  (1/1)]: loss=1.29059 (lr=3.0e-03)\n",
      "[step 2344/accstep :  (1/1)]: loss=2.37162 (lr=3.0e-03)\n",
      "[step 2345/accstep :  (1/1)]: loss=0.92004 (lr=3.0e-03)\n",
      "[step 2346/accstep :  (1/1)]: loss=1.76406 (lr=3.0e-03)\n",
      "[step 2347/accstep :  (1/1)]: loss=1.06542 (lr=3.0e-03)\n",
      "[step 2348/accstep :  (1/1)]: loss=1.69492 (lr=3.0e-03)\n",
      "[step 2349/accstep :  (1/1)]: loss=1.13360 (lr=3.0e-03)\n",
      "[step 2350/accstep :  (1/1)]: loss=3.16653 (lr=3.0e-03)\n",
      "[step 2351/accstep :  (1/1)]: loss=2.30806 (lr=3.0e-03)\n",
      "[step 2352/accstep :  (1/1)]: loss=1.30298 (lr=3.0e-03)\n",
      "[step 2353/accstep :  (1/1)]: loss=2.21973 (lr=3.0e-03)\n",
      "[step 2354/accstep :  (1/1)]: loss=1.37816 (lr=3.0e-03)\n",
      "[step 2355/accstep :  (1/1)]: loss=0.87568 (lr=3.0e-03)\n",
      "[step 2356/accstep :  (1/1)]: loss=1.83623 (lr=3.0e-03)\n",
      "[step 2357/accstep :  (1/1)]: loss=1.30430 (lr=3.0e-03)\n",
      "[step 2358/accstep :  (1/1)]: loss=1.34529 (lr=3.0e-03)\n",
      "[step 2359/accstep :  (1/1)]: loss=2.02705 (lr=3.0e-03)\n",
      "[step 2360/accstep :  (1/1)]: loss=1.11302 (lr=3.0e-03)\n",
      "[step 2361/accstep :  (1/1)]: loss=1.39569 (lr=3.0e-03)\n",
      "[step 2362/accstep :  (1/1)]: loss=1.26196 (lr=3.0e-03)\n",
      "[step 2363/accstep :  (1/1)]: loss=1.00776 (lr=3.0e-03)\n",
      "[step 2364/accstep :  (1/1)]: loss=1.97122 (lr=3.0e-03)\n",
      "[step 2365/accstep :  (1/1)]: loss=1.32898 (lr=3.0e-03)\n",
      "[step 2366/accstep :  (1/1)]: loss=4.33070 (lr=3.0e-03)\n",
      "[step 2367/accstep :  (1/1)]: loss=0.71293 (lr=3.0e-03)\n",
      "[step 2368/accstep :  (1/1)]: loss=2.61052 (lr=3.0e-03)\n",
      "[step 2369/accstep :  (1/1)]: loss=1.03308 (lr=3.0e-03)\n",
      "[step 2370/accstep :  (1/1)]: loss=1.52105 (lr=3.0e-03)\n",
      "[step 2371/accstep :  (1/1)]: loss=2.36717 (lr=3.0e-03)\n",
      "[step 2372/accstep :  (1/1)]: loss=1.19495 (lr=3.0e-03)\n",
      "[step 2373/accstep :  (1/1)]: loss=1.00140 (lr=3.0e-03)\n",
      "[step 2374/accstep :  (1/1)]: loss=0.77824 (lr=3.0e-03)\n",
      "[step 2375/accstep :  (1/1)]: loss=0.75982 (lr=3.0e-03)\n",
      "[step 2376/accstep :  (1/1)]: loss=0.83929 (lr=3.0e-03)\n",
      "[step 2377/accstep :  (1/1)]: loss=1.14738 (lr=3.0e-03)\n",
      "[step 2378/accstep :  (1/1)]: loss=1.82110 (lr=3.0e-03)\n",
      "[step 2379/accstep :  (1/1)]: loss=1.72974 (lr=3.0e-03)\n",
      "[step 2380/accstep :  (1/1)]: loss=0.81088 (lr=3.0e-03)\n",
      "[step 2381/accstep :  (1/1)]: loss=0.72930 (lr=3.0e-03)\n",
      "[step 2382/accstep :  (1/1)]: loss=3.77629 (lr=3.0e-03)\n",
      "[step 2383/accstep :  (1/1)]: loss=1.78814 (lr=3.0e-03)\n",
      "[step 2384/accstep :  (1/1)]: loss=2.86453 (lr=3.0e-03)\n",
      "[step 2385/accstep :  (1/1)]: loss=1.50997 (lr=3.0e-03)\n",
      "[step 2386/accstep :  (1/1)]: loss=1.27596 (lr=3.0e-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 2387/accstep :  (1/1)]: loss=1.21105 (lr=3.0e-03)\n",
      "[step 2388/accstep :  (1/1)]: loss=1.83746 (lr=3.0e-03)\n",
      "[step 2389/accstep :  (1/1)]: loss=0.72455 (lr=3.0e-03)\n",
      "[step 2390/accstep :  (1/1)]: loss=1.19292 (lr=3.0e-03)\n",
      "[step 2391/accstep :  (1/1)]: loss=1.35115 (lr=3.0e-03)\n",
      "[step 2392/accstep :  (1/1)]: loss=1.01328 (lr=3.0e-03)\n",
      "[step 2393/accstep :  (1/1)]: loss=0.57754 (lr=3.0e-03)\n",
      "[step 2394/accstep :  (1/1)]: loss=4.19527 (lr=3.0e-03)\n",
      "[step 2395/accstep :  (1/1)]: loss=1.99391 (lr=3.0e-03)\n",
      "[step 2396/accstep :  (1/1)]: loss=0.81982 (lr=3.0e-03)\n",
      "[step 2397/accstep :  (1/1)]: loss=3.34830 (lr=3.0e-03)\n",
      "[step 2398/accstep :  (1/1)]: loss=1.80918 (lr=3.0e-03)\n",
      "[step 2399/accstep :  (1/1)]: loss=1.34247 (lr=3.0e-03)\n",
      "[step 2400/accstep :  (1/1)]: loss=1.25317 (lr=3.0e-03)\n",
      "[step 2401/accstep :  (1/1)]: loss=1.31735 (lr=3.0e-03)\n",
      "[step 2402/accstep :  (1/1)]: loss=0.67932 (lr=3.0e-03)\n",
      "[step 2403/accstep :  (1/1)]: loss=1.36641 (lr=3.0e-03)\n",
      "[step 2404/accstep :  (1/1)]: loss=1.86541 (lr=3.0e-03)\n",
      "[step 2405/accstep :  (1/1)]: loss=2.62567 (lr=3.0e-03)\n",
      "[step 2406/accstep :  (1/1)]: loss=3.49429 (lr=3.0e-03)\n",
      "[step 2407/accstep :  (1/1)]: loss=1.14763 (lr=3.0e-03)\n",
      "[step 2408/accstep :  (1/1)]: loss=3.38706 (lr=3.0e-03)\n",
      "[step 2409/accstep :  (1/1)]: loss=0.93013 (lr=3.0e-03)\n",
      "[step 2410/accstep :  (1/1)]: loss=1.25752 (lr=3.0e-03)\n",
      "[step 2411/accstep :  (1/1)]: loss=0.82721 (lr=3.0e-03)\n",
      "[step 2412/accstep :  (1/1)]: loss=1.66356 (lr=3.0e-03)\n",
      "[step 2413/accstep :  (1/1)]: loss=1.60452 (lr=3.0e-03)\n",
      "[step 2414/accstep :  (1/1)]: loss=3.17951 (lr=3.0e-03)\n",
      "[step 2415/accstep :  (1/1)]: loss=3.84513 (lr=3.0e-03)\n",
      "[step 2416/accstep :  (1/1)]: loss=0.93689 (lr=3.0e-03)\n",
      "[step 2417/accstep :  (1/1)]: loss=0.80386 (lr=3.0e-03)\n",
      "[step 2418/accstep :  (1/1)]: loss=1.88502 (lr=3.0e-03)\n",
      "[step 2419/accstep :  (1/1)]: loss=1.02958 (lr=3.0e-03)\n",
      "[step 2420/accstep :  (1/1)]: loss=1.55815 (lr=3.0e-03)\n",
      "[step 2421/accstep :  (1/1)]: loss=1.97839 (lr=3.0e-03)\n",
      "[step 2422/accstep :  (1/1)]: loss=2.20258 (lr=3.0e-03)\n",
      "[step 2423/accstep :  (1/1)]: loss=1.03647 (lr=3.0e-03)\n",
      "[step 2424/accstep :  (1/1)]: loss=3.47520 (lr=3.0e-03)\n",
      "[step 2425/accstep :  (1/1)]: loss=2.14336 (lr=3.0e-03)\n",
      "[step 2426/accstep :  (1/1)]: loss=2.35049 (lr=3.0e-03)\n",
      "[step 2427/accstep :  (1/1)]: loss=1.46096 (lr=3.0e-03)\n",
      "[step 2428/accstep :  (1/1)]: loss=1.56601 (lr=3.0e-03)\n",
      "[step 2429/accstep :  (1/1)]: loss=3.26450 (lr=3.0e-03)\n",
      "[step 2430/accstep :  (1/1)]: loss=1.05651 (lr=3.0e-03)\n",
      "[step 2431/accstep :  (1/1)]: loss=1.41943 (lr=3.0e-03)\n",
      "[step 2432/accstep :  (1/1)]: loss=1.39447 (lr=3.0e-03)\n",
      "[step 2433/accstep :  (1/1)]: loss=1.11307 (lr=3.0e-03)\n",
      "[step 2434/accstep :  (1/1)]: loss=1.64078 (lr=3.0e-03)\n",
      "[step 2435/accstep :  (1/1)]: loss=1.80628 (lr=3.0e-03)\n",
      "[step 2436/accstep :  (1/1)]: loss=1.84528 (lr=3.0e-03)\n",
      "[step 2437/accstep :  (1/1)]: loss=1.50981 (lr=3.0e-03)\n",
      "[step 2438/accstep :  (1/1)]: loss=1.25004 (lr=3.0e-03)\n",
      "[step 2439/accstep :  (1/1)]: loss=0.82170 (lr=3.0e-03)\n",
      "[step 2440/accstep :  (1/1)]: loss=2.05794 (lr=3.0e-03)\n",
      "[step 2441/accstep :  (1/1)]: loss=0.76522 (lr=3.0e-03)\n",
      "[step 2442/accstep :  (1/1)]: loss=2.22574 (lr=3.0e-03)\n",
      "[step 2443/accstep :  (1/1)]: loss=3.21344 (lr=3.0e-03)\n",
      "[step 2444/accstep :  (1/1)]: loss=1.45944 (lr=3.0e-03)\n",
      "[step 2445/accstep :  (1/1)]: loss=2.92320 (lr=3.0e-03)\n",
      "[step 2446/accstep :  (1/1)]: loss=1.08004 (lr=3.0e-03)\n",
      "[step 2447/accstep :  (1/1)]: loss=1.51276 (lr=3.0e-03)\n",
      "[step 2448/accstep :  (1/1)]: loss=1.37036 (lr=3.0e-03)\n",
      "[step 2449/accstep :  (1/1)]: loss=1.83639 (lr=3.0e-03)\n",
      "[step 2450/accstep :  (1/1)]: loss=1.17739 (lr=3.0e-03)\n",
      "[step 2451/accstep :  (1/1)]: loss=1.02511 (lr=3.0e-03)\n",
      "[step 2452/accstep :  (1/1)]: loss=1.42364 (lr=3.0e-03)\n",
      "[step 2453/accstep :  (1/1)]: loss=1.20528 (lr=3.0e-03)\n",
      "[step 2454/accstep :  (1/1)]: loss=0.51106 (lr=3.0e-03)\n",
      "[step 2455/accstep :  (1/1)]: loss=1.24712 (lr=3.0e-03)\n",
      "[step 2456/accstep :  (1/1)]: loss=1.19940 (lr=3.0e-03)\n",
      "[step 2457/accstep :  (1/1)]: loss=0.75568 (lr=3.0e-03)\n",
      "[step 2458/accstep :  (1/1)]: loss=0.66762 (lr=3.0e-03)\n",
      "[step 2459/accstep :  (1/1)]: loss=1.17742 (lr=3.0e-03)\n",
      "[step 2460/accstep :  (1/1)]: loss=1.27519 (lr=3.0e-03)\n",
      "[step 2461/accstep :  (1/1)]: loss=1.37077 (lr=3.0e-03)\n",
      "[step 2462/accstep :  (1/1)]: loss=1.15619 (lr=3.0e-03)\n",
      "[step 2463/accstep :  (1/1)]: loss=1.18079 (lr=3.0e-03)\n",
      "[step 2464/accstep :  (1/1)]: loss=4.17628 (lr=3.0e-03)\n",
      "[step 2465/accstep :  (1/1)]: loss=2.36955 (lr=3.0e-03)\n",
      "[step 2466/accstep :  (1/1)]: loss=1.37268 (lr=3.0e-03)\n",
      "[step 2467/accstep :  (1/1)]: loss=1.06470 (lr=3.0e-03)\n",
      "[step 2468/accstep :  (1/1)]: loss=1.30747 (lr=3.0e-03)\n",
      "[step 2469/accstep :  (1/1)]: loss=1.04236 (lr=3.0e-03)\n",
      "[step 2470/accstep :  (1/1)]: loss=1.23920 (lr=3.0e-03)\n",
      "[step 2471/accstep :  (1/1)]: loss=1.41083 (lr=3.0e-03)\n",
      "[step 2472/accstep :  (1/1)]: loss=0.61201 (lr=3.0e-03)\n",
      "[step 2473/accstep :  (1/1)]: loss=1.01253 (lr=3.0e-03)\n",
      "[step 2474/accstep :  (1/1)]: loss=0.87083 (lr=3.0e-03)\n",
      "[step 2475/accstep :  (1/1)]: loss=2.06931 (lr=3.0e-03)\n",
      "[step 2476/accstep :  (1/1)]: loss=1.18390 (lr=3.0e-03)\n",
      "[step 2477/accstep :  (1/1)]: loss=2.81733 (lr=3.0e-03)\n",
      "[step 2478/accstep :  (1/1)]: loss=1.80405 (lr=3.0e-03)\n",
      "[step 2479/accstep :  (1/1)]: loss=0.98466 (lr=3.0e-03)\n",
      "[step 2480/accstep :  (1/1)]: loss=1.04256 (lr=3.0e-03)\n",
      "[step 2481/accstep :  (1/1)]: loss=1.07036 (lr=3.0e-03)\n",
      "[step 2482/accstep :  (1/1)]: loss=1.31100 (lr=3.0e-03)\n",
      "[step 2483/accstep :  (1/1)]: loss=1.43001 (lr=3.0e-03)\n",
      "[step 2484/accstep :  (1/1)]: loss=1.66897 (lr=3.0e-03)\n",
      "[step 2485/accstep :  (1/1)]: loss=1.13636 (lr=3.0e-03)\n",
      "[step 2486/accstep :  (1/1)]: loss=1.62755 (lr=3.0e-03)\n",
      "[step 2487/accstep :  (1/1)]: loss=0.48553 (lr=3.0e-03)\n",
      "[step 2488/accstep :  (1/1)]: loss=0.94743 (lr=3.0e-03)\n",
      "[step 2489/accstep :  (1/1)]: loss=1.25030 (lr=3.0e-03)\n",
      "[step 2490/accstep :  (1/1)]: loss=1.03847 (lr=3.0e-03)\n",
      "[step 2491/accstep :  (1/1)]: loss=4.08587 (lr=3.0e-03)\n",
      "[step 2492/accstep :  (1/1)]: loss=1.07362 (lr=3.0e-03)\n",
      "[step 2493/accstep :  (1/1)]: loss=1.23517 (lr=3.0e-03)\n",
      "[step 2494/accstep :  (1/1)]: loss=1.58187 (lr=3.0e-03)\n",
      "[step 2495/accstep :  (1/1)]: loss=1.16374 (lr=3.0e-03)\n",
      "[step 2496/accstep :  (1/1)]: loss=1.64841 (lr=3.0e-03)\n",
      "[step 2497/accstep :  (1/1)]: loss=1.00766 (lr=3.0e-03)\n",
      "[step 2498/accstep :  (1/1)]: loss=1.02504 (lr=3.0e-03)\n",
      "[step 2499/accstep :  (1/1)]: loss=1.65418 (lr=3.0e-03)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running validation...\n",
      "Validation@end loss 2.28353, top1 50.00%, top5 87.50%\n",
      "Timings:\n",
      "grads     : 0.03045s\n",
      "eval fprop: 0.01997s\n",
      "fprop     : 0.01647s\n",
      "load      : 0.01222s\n",
      "eval load : 0.00998s\n",
      "update    : 0.00556s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a2969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9432a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
